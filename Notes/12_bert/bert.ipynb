{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "For most machine learning tasks, you're better off **fine-tuning** a **pre-trained** neural network than training from scratch.\n",
    "What this means is you can take a model that was trained using a large training set that is not exactly what you need (usually a self-supervised task) and then continue training it (fine-tuning it) on a small data set that is precisely what you need.\n",
    "Doing this will allow the model to start off with being able to extract useful information from the small training set and so will not need to learn the basics of how language works.\n",
    "This technique, called **transfer learning**, let's you get much better results using a small data set.\n",
    "So useful was this concept that people started training models with the intention of publishing them as a **source model** for others to perform transfer learning.\n",
    "\n",
    "One of the most popular pre-trained models is [BERT](https://aclanthology.org/N19-1423/) (Bidirectional Encoder Representations from Transformers) which was trained by self-supervised learning on a large corpus of text from Wikipedia and the Book Corpus.\n",
    "The self-supervised tasks it was trained on are two: **masked language modelling** (MLM) and **next sentence prediction** (NSP).\n",
    "\n",
    "MLM is a task for predicting what was the missing token from a sentence.\n",
    "This is done by randomly replacing some tokens in a sentence with the special token `[MASK]` and training the model to predict every token in the sentence, including the masked ones.\n",
    "Masked language modelling is contrasted with **causal language modelling**, which is what the language models we've used up to now did.\n",
    "\n",
    "NSP is a task for predicting if two sentences were found next to each other or not.\n",
    "This is done by using two special tokens: `[SEP]` (separator) and `[CLS]` (class).\n",
    "The separator token is placed between the two sentences which are concatenated together into a single sequence.\n",
    "The class token is used such that the word-in-context vector produced by it is passed into a binary classifier to determine if the two sentences follow each other or if they were picked randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace\n",
    "\n",
    "The easiest way to use BERT is to use [HuggingFace](https://huggingface.co/models), a library of pre-trained transformers that are readily usable and downloadable.\n",
    "To use HuggingFace you just need to install the `transformers` Python library using pip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokeniser\n",
    "\n",
    "Since it's pre-trained, BERT has its own vocabulary that you need to use.\n",
    "It also comes with its own tokeniser that you are expected to use.\n",
    "\n",
    "The vocabulary consists of tokens called **word pieces**, which makes it possible to avoid using unknown tokens (although not completely).\n",
    "At its extreme, the tokeniser can break down a word into its individual characters and treat these characters as separate tokens.\n",
    "This would make the vocabulary be all the characters in the alphabet, digits, punctuation, and so on.\n",
    "While avoiding most unknown tokens, such a tokeniser would turn sentences into token sequences that are too long and which require a lot of memory to work with.\n",
    "So in addition to the individual characters, the vocabulary also includes commonly occuring substrings that are found in words.\n",
    "BERT's tokeniser automatically identifies these substrings and makes them a single token.\n",
    "Any unknown substrings are broken into smaller substrings and treated as separate tokens.\n",
    "\n",
    "The algorithm for extracting the vocabulary is called the **byte pair encoding** algorithm (BPE) and it works as follows:\n",
    "\n",
    "1. Start with a corpus of text from which to extract a vocabulary and tokenise it into whole words.\n",
    "1. Collect all the individual characters from all the words, treat these as tokens, and add them to the vocabulary.\n",
    "1. Look for the most frequent pair of adjacent vocabulary tokens in the corpus.\n",
    "1. Concatenate this pair, replace all occurrences of it in the corpus with a single token, and add it to the vocabulary.\n",
    "1. Repeat the previous two steps until the vocabulary is a certain size (something like 50 000 words).\n",
    "\n",
    "Let's take the word 'banana' as our example corpus.\n",
    "We treat the characters of the word as individual tokens and add them to our vocabulary:\n",
    "\n",
    "    Corpus: b a n a n a\n",
    "    Vocabulary: {a, b, n}\n",
    "\n",
    "The frequencies of the pairs of adjacent tokens in the corpus are as follows:\n",
    "\n",
    "    ba: 1\n",
    "    an: 2\n",
    "    na: 2\n",
    "\n",
    "Let's say that the most frequent pair is 'an'.\n",
    "We replace all instances of this in the corpus with a single token and add it to the vocabulary:\n",
    "\n",
    "    Corpus: b an an a\n",
    "    Vocabulary: {a, b, n, an}\n",
    "\n",
    "The frequencies of the pairs of new adjacent tokens is now:\n",
    "\n",
    "    ban: 1\n",
    "    anan: 1\n",
    "    ana: 1\n",
    "\n",
    "Keep replacing the most frequent pair of tokens with a single token until you reach a desired vocabulary size.\n",
    "\n",
    "Unlike other tokenisers, such as [the one used by GPT](https://gpt-tokenizer.dev/), the BERT tokeniser does not include the space between words as a token.\n",
    "Instead, each token has two versions: tokens that can only be used at the beginning of a word and tokens that can only appear anywhere else in the word.\n",
    "This allows BERT to know where multi-token words begin and end in a sentence.\n",
    "In fact, BERT repesents tokens that can be used after the first token by putting a '##' in front of the token.\n",
    "For example, the vocabulary shown above would actually be:\n",
    "\n",
    "    Vocabulary: {b, ##a, ##n, ##an}\n",
    "\n",
    "And the word 'banana' would be represented as:\n",
    "\n",
    "    b ##an ##an ##a\n",
    "\n",
    "In Huggingface, the BERT tokenizer can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = transformers.BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how you tokenise your text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'The dog bit the cat.',\n",
    "    'It was unbelievable.',\n",
    "]\n",
    "\n",
    "# No need to tokenise the text into a list of words first!\n",
    "\n",
    "token_indexes = tokeniser(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device) # BERT cannot process texts longer than 512 tokens (because of the position embeddings).\n",
    "\n",
    "# Token indexes.\n",
    "print('indexes:')\n",
    "print(token_indexes['input_ids'])\n",
    "print()\n",
    "\n",
    "# Pad mask.\n",
    "print('mask:')\n",
    "print(token_indexes['attention_mask'])\n",
    "print()\n",
    "\n",
    "# Readable tokens.\n",
    "print('tokens:')\n",
    "print(tokeniser.convert_ids_to_tokens(token_indexes['input_ids'][0]))\n",
    "print(tokeniser.convert_ids_to_tokens(token_indexes['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked language modelling\n",
    "\n",
    "This is how you load the BERT model for masked language modelling (it will spend time downloading the model from HuggingFace the first time you run this but then will use the cached model for future calls):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = transformers.BertForMaskedLM.from_pretrained('bert-base-cased').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can replace a token in the tokenised sentences with the mask token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indexes['input_ids'][0, 3] = tokeniser.mask_token_id\n",
    "token_indexes['input_ids'][1, 4] = tokeniser.mask_token_id\n",
    "\n",
    "print(tokeniser.convert_ids_to_tokens(token_indexes['input_ids'][0]))\n",
    "print(tokeniser.convert_ids_to_tokens(token_indexes['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can get the predictions made by BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = bert(token_indexes['input_ids'], attention_mask=token_indexes['attention_mask']).logits\n",
    "probs = torch.softmax(logits, dim=2)\n",
    "vocab = tokeniser.get_vocab()\n",
    "\n",
    "print(tokeniser.convert_ids_to_tokens(token_indexes['input_ids'][0]))\n",
    "for (prob, token) in sorted(zip(probs[0, 3, :].tolist(), vocab), reverse=True)[:5]:\n",
    "    print(token, prob)\n",
    "print()\n",
    "\n",
    "print(tokeniser.convert_ids_to_tokens(token_indexes['input_ids'][1]))\n",
    "for (prob, token) in sorted(zip(probs[1, 4, :].tolist(), vocab), reverse=True)[:5]:\n",
    "    print(token, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that trying to get predictions of a non-mask token will just predict the token itself with almost 100% probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokeniser.convert_ids_to_tokens(token_indexes['input_ids'][0]))\n",
    "print(tokeniser.convert_ids_to_tokens([token_indexes['input_ids'][0][2]]))\n",
    "for (prob, token) in sorted(zip(probs[0, 2, :].tolist(), vocab), reverse=True)[:5]:\n",
    "    print(token, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't need to limit yourself to only one mask.\n",
    "You can use multiple masks to perform **text in-filling**, that is, filling a span of text rather than a single token.\n",
    "Keep in mind that you can't just predict tokens for all the masks at once because the predictions made for each mask do not take into account what the other masks will be replaced with.\n",
    "The way you fill in multiple masks is exactly the same as when we performed text generation: you predict a token for a single mask, put it in place of said mask, and then perform a completely new prediction for the next mask with the previous mask filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'The [MASK] [MASK] bit the cat.' # The tokeniser will turn '[MASK]' into the special token.\n",
    "]\n",
    "\n",
    "token_indexes = tokeniser(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "print(tokeniser.convert_ids_to_tokens(token_indexes['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the indexes of all the mask tokens like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_mask = token_indexes['input_ids'][0] == tokeniser.mask_token_id\n",
    "print('mask_token_mask:')\n",
    "print(mask_token_mask)\n",
    "print()\n",
    "\n",
    "token_positions = torch.arange(token_indexes['input_ids'].shape[1], dtype=torch.int64, device=device)\n",
    "print('token_positions:')\n",
    "print(token_positions)\n",
    "print()\n",
    "\n",
    "mask_token_positions = token_positions[mask_token_mask]\n",
    "print('mask_token_positions:')\n",
    "print(mask_token_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can predict the token that can replace one mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = bert(token_indexes['input_ids'], attention_mask=token_indexes['attention_mask']).logits\n",
    "probs = torch.softmax(logits, dim=2)\n",
    "print('mask 1:')\n",
    "for (prob, token) in sorted(zip(probs[0, mask_token_positions[0], :].tolist(), vocab), reverse=True)[:5]:\n",
    "    print(token, prob)\n",
    "print()\n",
    "\n",
    "logits = bert(token_indexes['input_ids'], attention_mask=token_indexes['attention_mask']).logits\n",
    "probs = torch.softmax(logits, dim=2)\n",
    "print('mask 2:')\n",
    "for (prob, token) in sorted(zip(probs[0, mask_token_positions[1], :].tolist(), vocab), reverse=True)[:5]:\n",
    "    print(token, prob)\n",
    "print()\n",
    "\n",
    "print('replacing mask 1 with most probable token:')\n",
    "token_indexes['input_ids'][0, 2] = probs[0, 2, :].argmax()\n",
    "print(tokeniser.convert_ids_to_tokens(token_indexes['input_ids'][0]))\n",
    "print()\n",
    "\n",
    "logits = bert(token_indexes['input_ids'], attention_mask=token_indexes['attention_mask']).logits\n",
    "probs = torch.softmax(logits, dim=2)\n",
    "print('mask 2 now:')\n",
    "for (prob, token) in sorted(zip(probs[0, 3, :].tolist(), vocab), reverse=True)[:5]:\n",
    "    print(token, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you enforce the two masks to form a single word (that is made of two tokens)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pseudo-log-likelihood score\n",
    "\n",
    "It's not possible to get the probability of a sentence using a masked language model.\n",
    "Remember how we decomposed the probability of a sentence into the product of multiple probabilities made from prefixes of the sentence?\n",
    "Masked language models do not use prefixes so the token probabilities do not combine into the mathematical definition of the probability of a sentence.\n",
    "But this did not stop people from doing it anyway, with good results.\n",
    "It's called a [**pseudo-log-likelihood score**](https://aclanthology.org/2020.acl-main.240/) (that is, not the true log-likelihood) and it's done as follows:\n",
    "\n",
    "1. Replace the first token in the sentence with a mask.\n",
    "1. Get the log probability of the original token in the mask's place.\n",
    "1. Put the original token back.\n",
    "1. Repeat the previous three steps for every token.\n",
    "1. Add up all the log probabilities into a single score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'The dog bit the cat.'\n",
    "]\n",
    "token_indexes = tokeniser(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "print(tokeniser.convert_ids_to_tokens(token_indexes['input_ids'][0]))\n",
    "print()\n",
    "\n",
    "score = 0.0\n",
    "for (i, token_index) in enumerate(token_indexes['input_ids'][0, 1:-1].tolist(), start=1):\n",
    "    token_indexes['input_ids'][0, i] = tokeniser.mask_token_id\n",
    "    \n",
    "    logits = bert(token_indexes['input_ids'], attention_mask=token_indexes['attention_mask']).logits\n",
    "    pseudo_logprobs = torch.log_softmax(logits, dim=2)\n",
    "    token_pseudo_logprob = pseudo_logprobs[0, i, token_index].tolist()\n",
    "    \n",
    "    print(tokeniser.convert_ids_to_tokens(token_indexes['input_ids'][0]), token_pseudo_logprob)\n",
    "    \n",
    "    score += token_pseudo_logprob\n",
    "    \n",
    "    token_indexes['input_ids'][0, i] = token_index\n",
    "print()\n",
    "\n",
    "print('score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that this is not a real probability, so there is not point in converting the log probability to a probability.\n",
    "It should only be treated as a score for ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning BERT\n",
    "\n",
    "Masked language modelling is pretty cool, but not as cool as incorporating BERT into your model and fine-tuning it to do your bidding.\n",
    "What we'll do is to crack open BERT and use the output of a hidden layer inside BERT as a part of our model.\n",
    "We can then train our model together with BERT.\n",
    "We'll use it to perform sentiment analysis and part of speech tagging.\n",
    "\n",
    "Here is how you access a hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT has 13 hidden layers, the first being the embedding layer.\n",
    "hidden_layers = bert(\n",
    "    token_indexes['input_ids'],\n",
    "    attention_mask=token_indexes['attention_mask'],\n",
    "    output_hidden_states=True\n",
    ").hidden_states\n",
    "\n",
    "hidden_layer = hidden_layers[7] # The middle layer is usually the most transferrable.\n",
    "print(hidden_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important thing about these pre-trained transformers is that they are normal PyTorch modules, and so you can treat them as a layer in your own module.\n",
    "\n",
    "There are certain things you need to keep in mind when fine-tuning:\n",
    "\n",
    "* BERT makes use of dropout internally so you have to use `model.train()` and `model.eval()` to say whether the calls you're making on the model are for optimisation or to get predictions.\n",
    "* You should use a smaller learning rate for BERT than for your own parameters.\n",
    "    This is to avoid **catastrophic forgetting**, which is when the pre-trained model overfits on your data and forgets what it was pre-trained on.\n",
    "    We usually use a learning rate of `2E-5` on BERT.\n",
    "    You can do this easily in PyTorch, as shown below.\n",
    "* Due to the dropout inside BERT, it will have slightly unstable learning progress, which is normal, provided that the error tends to go down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [\n",
    "    'I like it .',\n",
    "    'I hate it .',\n",
    "    'I don\\'t hate it .',\n",
    "    'I don\\'t like it .',\n",
    "]\n",
    "train_y = torch.tensor([\n",
    "    [1],\n",
    "    [0],\n",
    "    [1],\n",
    "    [0],\n",
    "], dtype=torch.float32, device=device)\n",
    "\n",
    "token_indexes = tokeniser(train_x, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "train_x_indexed = token_indexes['input_ids']\n",
    "train_x_mask = token_indexes['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = transformers.BertForMaskedLM.from_pretrained('bert-base-cased') # Make sure you're using a fresh copy of the original BERT and not a fine-tuned one.\n",
    "        self.output_layer = torch.nn.Linear(768, 1)\n",
    "    \n",
    "    def forward(self, x_indexed, pad_mask):\n",
    "        word_in_context_vecs = self.bert(x_indexed, attention_mask=pad_mask, output_hidden_states=True).hidden_states[7]\n",
    "        cls_vec = word_in_context_vecs[:, 0, :] # Use the CLS token to represent the entire text.\n",
    "        return self.output_layer(cls_vec)\n",
    "\n",
    "model = Model()\n",
    "model.to(device)\n",
    "\n",
    "# Use a normal learning rate for the parameters we created and a tiny learning rate for BERT.\n",
    "optimiser = torch.optim.Adam([\n",
    "    {'params': model.output_layer.parameters(), 'lr': 0.1},\n",
    "    {'params': model.bert.parameters(), 'lr': 2E-5}\n",
    "])\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 40+1):\n",
    "    optimiser.zero_grad()\n",
    "    model.train()\n",
    "    logits = model(train_x_indexed, train_x_mask)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_errors.append(train_error.cpu().detach().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "    model.eval()\n",
    "\n",
    "    if epoch%1 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('text', 'output')\n",
    "    outputs = torch.sigmoid(model(train_x_indexed, train_x_mask))\n",
    "    for (text, y) in zip(train_x, outputs):\n",
    "        print(text, y.round(decimals=1).cpu().tolist())\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text tagging\n",
    "\n",
    "Given that BERT works on subword tokens rather than at the word level, using BERT to tag full words is a bit of a challenge.\n",
    "For example, \"I don't like it.\" becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokeniser.convert_ids_to_tokens(token_indexes['input_ids'][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 5 word sentence will result in 9 tokens which will result in 9 predictions.\n",
    "How can you make predictions for whole words?\n",
    "\n",
    "What is usually done is that only the first token of every word is considered, with all other subword tokens being masked out.\n",
    "Here's a function that converts a list of words with tags into a list of tokens, mask, and aligned tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aligned_tokens_and_tags(tokeniser, text_tokens, text_tags, no_tag=0):\n",
    "    '''\n",
    "    Convert a list of word-tokenised texts with a tag for each word into BERT tokenised texts with aligned tags.\n",
    "    '''\n",
    "    text_indexes = []\n",
    "    text_aligned_tags = []\n",
    "    text_tag_masks = []\n",
    "    \n",
    "    for (tokens, tags) in zip(text_tokens, text_tags):\n",
    "        # Replicate the process of tokenising a sentence by concatenating the tokenised words.\n",
    "        \n",
    "        # Start with the CLS token.\n",
    "        indexes = [tokeniser.cls_token_id]\n",
    "        tag_mask = [False]\n",
    "        aligned_tags = [no_tag]\n",
    "        \n",
    "        # Add the tokens of each word.\n",
    "        for (tag, word) in zip(tags, tokens):\n",
    "            for (i, index) in enumerate(tokeniser(word)['input_ids'][1:-1]): # For every subword token in the current word:\n",
    "                indexes.append(index)\n",
    "                if i == 0: # Only the first token of the word gets a true tag.\n",
    "                    tag_mask.append(True)\n",
    "                    aligned_tags.append(tag)\n",
    "                else:\n",
    "                    tag_mask.append(False)\n",
    "                    aligned_tags.append(no_tag)\n",
    "        \n",
    "        # End with the SEP token.\n",
    "        indexes.append(tokeniser.sep_token_id)\n",
    "        tag_mask.append(False)\n",
    "        aligned_tags.append(no_tag)\n",
    "        \n",
    "        # Add the new tokenised sentence to the list.\n",
    "        text_indexes.append(indexes)\n",
    "        text_aligned_tags.append(aligned_tags)\n",
    "        text_tag_masks.append(tag_mask)\n",
    "    \n",
    "    # Pad the tokens.\n",
    "    max_len = max(len(indexes) for indexes in text_indexes)\n",
    "    text_token_masks = []\n",
    "    for (i, (indexes, tag_mask, aligned_tags)) in enumerate(zip(text_indexes, text_tag_masks, text_aligned_tags)):\n",
    "        num_tokens = len(indexes)\n",
    "        text_token_masks.append([1]*num_tokens + [0]*(max_len - num_tokens))\n",
    "        text_indexes[i].extend([tokeniser.pad_token_id]*(max_len - num_tokens))\n",
    "        text_tag_masks[i].extend([False]*(max_len - num_tokens))\n",
    "        text_aligned_tags[i].extend([no_tag]*(max_len - num_tokens))\n",
    "    \n",
    "    return (\n",
    "        torch.tensor(text_indexes, dtype=torch.int64, device=device),\n",
    "        torch.tensor(text_token_masks, dtype=torch.bool, device=device),\n",
    "        torch.tensor(text_aligned_tags, dtype=torch.int64, device=device),\n",
    "        torch.tensor(text_tag_masks, dtype=torch.bool, device=device),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'I like it .'.split(' '),\n",
    "    'I don\\'t like it .'.split(' '),\n",
    "]\n",
    "tags = [\n",
    "    'PRON VERB PROP .'.split(' '),\n",
    "    'PRON VERB ADP PROP .'.split(' '),\n",
    "]\n",
    "\n",
    "tag_set = ['PAD', 'PRON', 'VERB', 'ADP', 'PROP', '.']\n",
    "tag_indexes = [[tag_set.index(tag) for tag in text] for text in tags]\n",
    "\n",
    "(token_indexes, token_mask, aligned_tags, tag_mask) = get_aligned_tokens_and_tags(tokeniser, texts, tag_indexes)\n",
    "token_indexes.to(device)\n",
    "token_mask.to(device)\n",
    "\n",
    "print('token_indexes:')\n",
    "print(token_indexes)\n",
    "print()\n",
    "print('readable token_indexes:')\n",
    "for text in token_indexes:\n",
    "    print(tokeniser.convert_ids_to_tokens(text))\n",
    "print()\n",
    "print('token_mask:')\n",
    "print(token_mask)\n",
    "print()\n",
    "print('aligned_tags:')\n",
    "print(aligned_tags)\n",
    "print()\n",
    "print('readable aligned_tags:')\n",
    "for text in aligned_tags:\n",
    "    print([tag_set[index] for index in text])\n",
    "print()\n",
    "print('tag_mask:')\n",
    "print(tag_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can preprocess our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [\n",
    "    'I like it .'.split(' '),\n",
    "    'I hate it .'.split(' '),\n",
    "    'I don\\'t hate it .'.split(' '),\n",
    "    'I don\\'t like it .'.split(' '),\n",
    "]\n",
    "train_y = [\n",
    "    ['PRON', 'VERB', 'PROP', '.'],\n",
    "    ['PRON', 'VERB', 'PROP', '.'],\n",
    "    ['PRON', 'VERB', 'ADP', 'PROP', '.'],\n",
    "    ['PRON', 'VERB', 'ADP', 'PROP', '.'],\n",
    "]\n",
    "\n",
    "tag_set = ['PAD'] + sorted({tag for text in train_y for tag in text})\n",
    "train_y_indexed = [[tag_set.index(tag) for tag in text] for text in train_y]\n",
    "\n",
    "(token_indexes, token_mask, aligned_tags, tag_mask) = get_aligned_tokens_and_tags(tokeniser, train_x, train_y_indexed)\n",
    "token_indexes.to(device)\n",
    "token_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_tags):\n",
    "        super().__init__()\n",
    "        self.bert = transformers.BertForMaskedLM.from_pretrained('bert-base-cased')\n",
    "        self.output_layer = torch.nn.Linear(768, num_tags)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        vecs = self.bert(x, attention_mask=mask, output_hidden_states=True).hidden_states[7]\n",
    "        return self.output_layer(vecs)\n",
    "\n",
    "model = Model(len(tag_set))\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam([\n",
    "    {'params': model.output_layer.parameters(), 'lr': 0.1},\n",
    "    {'params': model.bert.parameters(), 'lr': 2E-5}\n",
    "])\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 10+1):\n",
    "    optimiser.zero_grad()\n",
    "    model.train()\n",
    "    logits = model(token_indexes, token_mask)\n",
    "    train_token_errors = torch.nn.functional.cross_entropy(logits.transpose(1, 2), aligned_tags, reduction='none')\n",
    "    train_token_errors = train_token_errors.masked_fill(~tag_mask, 0.0)\n",
    "    train_error = train_token_errors.sum()/tag_mask.sum()\n",
    "    train_errors.append(train_error.cpu().detach().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "    model.eval()\n",
    "\n",
    "    if epoch%1 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('text', 'output')\n",
    "    output = torch.softmax(model(token_indexes, token_mask), dim=2)\n",
    "    for (text, indexes, mask, y) in zip(train_x, token_indexes, tag_mask, output):\n",
    "        tokens = tokeniser.convert_ids_to_tokens(indexes)\n",
    "        tags = [tag_set[probs.argmax()] if m else '-' for (probs, m) in zip(y, mask)]\n",
    "        print('text  :', ' '.join(text))\n",
    "        print('tokens:', *[f'{token: <6s}' for token in tokens])\n",
    "        print('tags  :', *[f'{tag: <6s}' for tag in tags])\n",
    "        print()\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie reviews with BERT\n",
    "\n",
    "Redo the movie review sentiment classification task we were doing in earlier topics but this time use BERT.\n",
    "Note that we don't need to tokenise, pad, or replace out-of-vocabulary tokens.\n",
    "Also note that your batch size might need to be tiny to fit BERT into you VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data_set/sentiment/train.csv')\n",
    "test_df = pd.read_csv('../data_set/sentiment/test.csv')\n",
    "\n",
    "train_x = train_df['text']\n",
    "train_y = train_df['class']\n",
    "test_x = test_df['text']\n",
    "test_y = test_df['class']\n",
    "categories = ['neg', 'pos']\n",
    "cat2idx = {cat: i for (i, cat) in enumerate(categories)}\n",
    "\n",
    "train_y_indexed = torch.tensor(\n",
    "    train_y.map(cat2idx.get).to_numpy()[:, None],\n",
    "    dtype=torch.float32, device=device\n",
    ")\n",
    "test_y_indexed = test_y.map(cat2idx.get).to_numpy()[:, None]\n",
    "\n",
    "tokeniser = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "token_indexes = tokeniser(train_x.tolist(), return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "train_x_indexed = token_indexes['input_ids']\n",
    "train_x_mask = token_indexes['attention_mask']\n",
    "\n",
    "token_indexes = tokeniser(test_x.tolist(), return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "test_x_indexed = token_indexes['input_ids']\n",
    "test_x_mask = token_indexes['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75ee2b71ad44bf9ef4e9bee896f68ffbc764a6a2c6d1f57c86c48f99ffc25ca8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
