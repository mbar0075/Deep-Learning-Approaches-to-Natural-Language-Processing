{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural language models\n",
    "\n",
    "Up to now we've always been classifying given sequences of texts.\n",
    "In this topic, we'll see how to generate a sequence ourselves that will contain acceptable English text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next token prediction\n",
    "\n",
    "The basis of neural network sequence generation is the **next token prediction** task, that is, given a prefix of a text, determine which token can be the next token in the text.\n",
    "For example, how likely is 'it' to come after 'I like'?\n",
    "This is a basic classification task where the input is the prefix and the output is a softmax over the tokens in the vocabulary.\n",
    "\n",
    "In order to be able to predict the first token, we'll add a new special token to our vocabulary called the **start token**.\n",
    "The start token is added to the front of every text so that we can predict the first token in a text by asking what comes after the the start token.\n",
    "Another important feature in next token prediction is the ability to predict that no token should come after a given prefix because the prefix makes a complete text.\n",
    "To handle this case we'll add another control token to our vocabulary called the **end token**, which, when predicted, signals that the end of the text has been reached.\n",
    "The end token is added to the end of every text.\n",
    "In practice there is no reason for these two to be two separate tokens and we usually use one token for both called the **edge token**.\n",
    "\n",
    "One way to predict the next token in a prefix is by having a data set that looks like this:\n",
    "\n",
    "| prefix                        | next token|\n",
    "|-------------------------------|-----------|\n",
    "| `EDGE PAD PAD   PAD  PAD PAD` | `I`       |\n",
    "| `EDGE I   PAD   PAD  PAD PAD` | `like`    |\n",
    "| `EDGE I   like  PAD  PAD PAD` | `it`      |\n",
    "| `EDGE I   like  it   PAD PAD` | `.`       |\n",
    "| `EDGE I   like  it   .   PAD` | `EDGE`    |\n",
    "| `EDGE PAD PAD   PAD  PAD PAD` | `I`       |\n",
    "| `EDGE I   PAD   PAD  PAD PAD` | `don't`   |\n",
    "| `EDGE I   don't PAD  PAD PAD` | `like`    |\n",
    "| `EDGE I   don't like PAD PAD` | `it`      |\n",
    "| `EDGE I   don't like it  PAD` | `.`       |\n",
    "| `EDGE I   don't like it    .` | `EDGE`    |\n",
    "\n",
    "But this will result in a lot of pad tokens which waste space and we don't actually need to learn to predict just one token in every row.\n",
    "We can learn this task efficiently by taking advantage of the intermediate RNN states.\n",
    "In the previous topic, we learned to make the same prediction for every prefix in a text.\n",
    "Now we will predict the next token for every prefix in a text.\n",
    "In this case, our data set will look like this:\n",
    "\n",
    "| x                          | y                           |\n",
    "|----------------------------|-----------------------------|\n",
    "| `EDGE I like  it   .  PAD` | `I like  it   .  EDGE  PAD` |\n",
    "| `EDGE I don't like it   .` | `I don't like it .    EDGE` |\n",
    "\n",
    "What will happen here is that the RNN will generate a prefix vector for each prefix in the sequence in 'x' followed by a softmax layer applied to each prefix vector.\n",
    "The target class for each prefix corresponds to the sequence in 'y'.\n",
    "The 'y' column is saying what the next token should be for every corresponding token in 'x'.\n",
    "For the first row, 'EDGE' corresponds to 'I', 'I' corresponds to 'like', and so on.\n",
    "So the prefix vector generated from 'EDGE' will have the target class 'I', the prefix vector generated from 'EDGE I' will have the target class 'like', and so on.\n",
    "This is much more efficient than the 'prefix - next token' data set, both in terms of space and training time.\n",
    "\n",
    "Here is how to make this data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len: 6\n",
      "vocab: ['<PAD>', '<EDGE>', '.', 'I', \"don't\", 'hate', 'it', 'like']\n",
      "edge index: 1\n",
      "\n",
      "train_text_x_indexed (edge token at the beginning):\n",
      "tensor([[1, 3, 7, 6, 2, 0],\n",
      "        [1, 3, 5, 6, 2, 0],\n",
      "        [1, 3, 4, 5, 6, 2],\n",
      "        [1, 3, 4, 7, 6, 2]], device='cuda:0')\n",
      "\n",
      "train_text_y_indexed (edge token at the end):\n",
      "tensor([[3, 7, 6, 2, 1, 0],\n",
      "        [3, 5, 6, 2, 1, 0],\n",
      "        [3, 4, 5, 6, 2, 1],\n",
      "        [3, 4, 7, 6, 2, 1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train_text_tokens = [\n",
    "    'I like it .'.split(' '),\n",
    "    'I hate it .'.split(' '),\n",
    "    'I don\\'t hate it .'.split(' '),\n",
    "    'I don\\'t like it .'.split(' '),\n",
    "]\n",
    "\n",
    "max_len = max(len(text) for text in train_text_tokens) + 1 # Including the edge token.\n",
    "print('max_len:', max_len)\n",
    "\n",
    "vocab = ['<PAD>', '<EDGE>'] + sorted({token for text in train_text_tokens for token in text})\n",
    "token2index = {t: i for (i, t) in enumerate(vocab)}\n",
    "pad_index = token2index['<PAD>']\n",
    "edge_index = token2index['<EDGE>']\n",
    "print('vocab:', vocab)\n",
    "print('edge index:', edge_index)\n",
    "print()\n",
    "\n",
    "train_text_x_indexed_np = np.full((len(train_text_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_text_tokens)):\n",
    "    train_text_x_indexed_np[i, 0] = edge_index # Add the edge token at the beginning.\n",
    "    for j in range(len(train_text_tokens[i])):\n",
    "        train_text_x_indexed_np[i, j + 1] = token2index[train_text_tokens[i][j]]\n",
    "train_text_x_indexed = torch.tensor(train_text_x_indexed_np, device=device)\n",
    "print('train_text_x_indexed (edge token at the beginning):')\n",
    "print(train_text_x_indexed)\n",
    "print()\n",
    "\n",
    "train_text_y_indexed_np = np.full((len(train_text_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_text_tokens)):\n",
    "    for j in range(len(train_text_tokens[i])):\n",
    "        train_text_y_indexed_np[i, j] = token2index[train_text_tokens[i][j]]\n",
    "    train_text_y_indexed_np[i, len(train_text_tokens[i])] = edge_index # Add the edge token at the end.\n",
    "train_text_y_indexed = torch.tensor(train_text_y_indexed_np, device=device)\n",
    "print('train_text_y_indexed (edge token at the end):')\n",
    "print(train_text_y_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now train a neural network to predict the next token from every prefix.\n",
    "Afterwards, we'll go through each unique prefix in the training set and show the softmax output of each possible token in the vocabulary being the next token in the prefix.\n",
    "\n",
    "Note that when using `torch.nn.functional.cross_entropy` on 3D logits, as is the case here, the function expects the dimensions to be `(batch size, num classes, time steps)`.\n",
    "Since our logits are `(batch size, time steps, num classes)`, we need to transpose dimensions 1 and 2 before passing them to the cross-entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch error\n",
      "200 0.31368598341941833\n",
      "400 0.2596908509731293\n",
      "600 0.2559770941734314\n",
      "800 0.25450924038887024\n",
      "1000 0.2537882328033447\n",
      "1200 0.2533169090747833\n",
      "1400 0.25302961468696594\n",
      "1600 0.2528192400932312\n",
      "1800 0.2526983916759491\n",
      "2000 0.25256654620170593\n",
      "2200 0.252660870552063\n",
      "2400 0.25241896510124207\n",
      "2600 0.25236520171165466\n",
      "2800 0.2523222267627716\n",
      "3000 0.25228703022003174\n",
      "3200 0.2522587478160858\n",
      "3400 0.25224220752716064\n",
      "3600 0.2529582977294922\n",
      "3800 0.25219258666038513\n",
      "4000 0.25217878818511963\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, state_size):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_s0 = torch.nn.Parameter(torch.zeros((state_size,), dtype=torch.float32))\n",
    "        self.rnn_c0 = torch.nn.Parameter(torch.zeros((state_size,), dtype=torch.float32))\n",
    "        self.rnn_cell = torch.nn.LSTMCell(embedding_size, state_size)\n",
    "        self.output_layer = torch.nn.Linear(state_size, vocab_size) # Output size is vocabulary size.\n",
    "\n",
    "    def forward(self, text_x_indexed):\n",
    "        batch_size = text_x_indexed.shape[0]\n",
    "        time_steps = text_x_indexed.shape[1]\n",
    "\n",
    "        embedded = self.embedding(text_x_indexed)\n",
    "        state = self.rnn_s0[None, :].tile((batch_size, 1))\n",
    "        c = self.rnn_c0[None, :].tile((batch_size, 1))\n",
    "        interm_states_list = []\n",
    "        for t in range(time_steps):\n",
    "            (state, c) = self.rnn_cell(embedded[:, t, :], (state, c))\n",
    "            interm_states_list.append(state)\n",
    "        interm_states = torch.stack(interm_states_list, dim=1)\n",
    "        return self.output_layer(interm_states)\n",
    "\n",
    "model = Model(len(vocab), embedding_size=2, state_size=2)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 4000+1):\n",
    "    pad_mask = train_text_x_indexed == pad_index\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_text_x_indexed)\n",
    "    train_token_errors = torch.nn.functional.cross_entropy(logits.transpose(1, 2), train_text_y_indexed, reduction='none') # Transpose.\n",
    "    train_token_errors = train_token_errors.masked_fill(pad_mask, 0.0)\n",
    "    train_error = train_token_errors.sum()/(~pad_mask).sum()\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%200 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to extract all the unique prefixed from the texts and indexify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = sorted({tuple(text[:i]) for text in train_text_tokens for i in range(len(text) + 1)}) # Prefix must be a tuple so we can put it in a set.\n",
    "prefix_max_len = max(len(prefix) for prefix in prefixes) + 1\n",
    "prefix_text_x_indexed_np = np.full((len(prefixes), prefix_max_len), pad_index, np.int64)\n",
    "for i in range(len(prefixes)):\n",
    "    prefix_text_x_indexed_np[i, 0] = edge_index\n",
    "    for j in range(len(prefixes[i])):\n",
    "        prefix_text_x_indexed_np[i, j+1] = token2index[prefixes[i][j]]\n",
    "prefix_text_x_indexed = torch.tensor(prefix_text_x_indexed_np, device=device)\n",
    "with torch.no_grad():\n",
    "    output = torch.softmax(model(prefix_text_x_indexed), dim=2).cpu().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`output` is a 3D tensor with a prediction over the whole vocabulary, for every token in every text.\n",
    "We only want the predictions on the last token so that we can predict the next token after it.\n",
    "Since the prefix starts with an edge token, the index of the last token is `len(prefix)` rather than `len(prefix)-1`.\n",
    "Here we'll print the top 5 next tokens for every prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<EDGE>']\n",
      "   I     : 0.99990225\n",
      "   it    : 0.00005834\n",
      "   hate  : 0.00002253\n",
      "   like  : 0.00001682\n",
      "   don't : 0.00000001\n",
      "\n",
      "['<EDGE>', 'I']\n",
      "   don't : 0.49980861\n",
      "   like  : 0.25005248\n",
      "   hate  : 0.25004482\n",
      "   .     : 0.00006885\n",
      "   it    : 0.00002454\n",
      "\n",
      "['<EDGE>', 'I', \"don't\"]\n",
      "   like  : 0.49991548\n",
      "   hate  : 0.49985737\n",
      "   don't : 0.00018412\n",
      "   I     : 0.00004304\n",
      "   it    : 0.00000000\n",
      "\n",
      "['<EDGE>', 'I', \"don't\", 'hate']\n",
      "   it    : 0.99982423\n",
      "   <EDGE>: 0.00008438\n",
      "   I     : 0.00007792\n",
      "   don't : 0.00001099\n",
      "   <PAD> : 0.00000101\n",
      "\n",
      "['<EDGE>', 'I', \"don't\", 'hate', 'it']\n",
      "   .     : 0.99989533\n",
      "   don't : 0.00008289\n",
      "   <EDGE>: 0.00002171\n",
      "   it    : 0.00000007\n",
      "   <PAD> : 0.00000000\n",
      "\n",
      "['<EDGE>', 'I', \"don't\", 'hate', 'it', '.']\n",
      "   <EDGE>: 0.99989903\n",
      "   it    : 0.00006367\n",
      "   .     : 0.00003729\n",
      "   <PAD> : 0.00000000\n",
      "   don't : 0.00000000\n",
      "\n",
      "['<EDGE>', 'I', \"don't\", 'like']\n",
      "   it    : 0.99982041\n",
      "   <EDGE>: 0.00010519\n",
      "   I     : 0.00006176\n",
      "   don't : 0.00001040\n",
      "   <PAD> : 0.00000101\n",
      "\n",
      "['<EDGE>', 'I', \"don't\", 'like', 'it']\n",
      "   .     : 0.99989522\n",
      "   don't : 0.00008301\n",
      "   <EDGE>: 0.00002171\n",
      "   it    : 0.00000007\n",
      "   <PAD> : 0.00000000\n",
      "\n",
      "['<EDGE>', 'I', \"don't\", 'like', 'it', '.']\n",
      "   <EDGE>: 0.99989903\n",
      "   it    : 0.00006368\n",
      "   .     : 0.00003729\n",
      "   <PAD> : 0.00000000\n",
      "   don't : 0.00000000\n",
      "\n",
      "['<EDGE>', 'I', 'hate']\n",
      "   it    : 0.99982351\n",
      "   I     : 0.00008453\n",
      "   <EDGE>: 0.00007817\n",
      "   don't : 0.00001129\n",
      "   <PAD> : 0.00000101\n",
      "\n",
      "['<EDGE>', 'I', 'hate', 'it']\n",
      "   .     : 0.99989533\n",
      "   don't : 0.00008289\n",
      "   <EDGE>: 0.00002172\n",
      "   it    : 0.00000007\n",
      "   <PAD> : 0.00000000\n",
      "\n",
      "['<EDGE>', 'I', 'hate', 'it', '.']\n",
      "   <EDGE>: 0.99989903\n",
      "   it    : 0.00006367\n",
      "   .     : 0.00003729\n",
      "   <PAD> : 0.00000000\n",
      "   don't : 0.00000000\n",
      "\n",
      "['<EDGE>', 'I', 'like']\n",
      "   it    : 0.99982268\n",
      "   I     : 0.00009110\n",
      "   <EDGE>: 0.00007257\n",
      "   don't : 0.00001107\n",
      "   <PAD> : 0.00000099\n",
      "\n",
      "['<EDGE>', 'I', 'like', 'it']\n",
      "   .     : 0.99989522\n",
      "   don't : 0.00008294\n",
      "   <EDGE>: 0.00002173\n",
      "   it    : 0.00000007\n",
      "   <PAD> : 0.00000000\n",
      "\n",
      "['<EDGE>', 'I', 'like', 'it', '.']\n",
      "   <EDGE>: 0.99989903\n",
      "   it    : 0.00006367\n",
      "   .     : 0.00003729\n",
      "   <PAD> : 0.00000000\n",
      "   don't : 0.00000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (prefix, y) in zip(prefixes, output):\n",
    "    last_token_y = y[len(prefix)]\n",
    "    top_preds = sorted(zip(last_token_y, vocab), reverse=True)[:5]\n",
    "    print(['<EDGE>'] + list(prefix))\n",
    "    for (x, token) in top_preds:\n",
    "        print(f'   {token:6s}: {x:.8f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the softmax output of the next token after '\\<EDGE> I' is evenly split between 'like' and 'hate'.\n",
    "This is for the same reason we saw in the previous topic when we were classifying prefixes according to the sentiment of the whole text they are in.\n",
    "The prefix 'I' was half of the time in a positive text and half of the time in a negative text, so the neural network learned to output 0.5 instead of 1 or 0.\n",
    "Here the neural network has also learned to split the output between all the right answers.\n",
    "In fact, we can stop thinking of the outputs as classifying the next token and start thinking of them as the probability of each token being the next token.\n",
    "These probabilities will be the basis for our language models and for generating text.\n",
    "\n",
    "Note also that we're doing something called **self-supervised learning** here, that is, we're training the neural network to predict the input itself rather than some label about the input.\n",
    "Self-supervised learning tasks have the advantage that you only need to collect free text as data, without needing to label it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modelling\n",
    "\n",
    "The next word prediction model is also known as a **neural language model**.\n",
    "A language model is a model that calculates the probability of a text in a certain language domain (the kind of language used in the training set).\n",
    "The probability returned by the language model should estimate how plausible a text is.\n",
    "For example, we'd expect that the sentence \"The dog bit the cat.\" to have a higher probability than \"The chair bit the cat.\" which would in turn have a higher probability than \"The the the the.\".\n",
    "\n",
    "To understand how to calculate the probability of a text, we need to understand some conditional probability theory.\n",
    "Say we have the Venn diagram below consisting of a set of even numbers and a set of square numbers, with some numbers intersecting both.\n",
    "\n",
    "![](conditional_prob.png)\n",
    "\n",
    "There are 10 different numbers in all, 4 in the square set, 8 in the even set, and 2 shared between both.\n",
    "Therefore, the probability of randomly selecting a square number out of all 10 numbers is $\\frac{4}{10}$, the probability of selecting an even number is $\\frac{8}{10}$, and the probability of selecting a number that is both square and even is $\\frac{2}{10}$.\n",
    "These are **unconditional probabilities**, because they are based on the entire set of numbers.\n",
    "They are expressed as $P(square)$, $P(even)$, and $P(square, even)$.\n",
    "This last probability is called a **joint probability**.\n",
    "\n",
    "An example of a **conditional probability** would be the probability of selecting a square number, given that it is also an even number.\n",
    "This is giving extra information that narrows down the possible choices, resulting in a larger probability.\n",
    "In this case, the probability of selecting a square number given that it is also even is $\\frac{2}{8}$.\n",
    "This probability is expressed as $P(square|even)$, that is, \"the probability of square, given even\".\n",
    "\n",
    "Mathematically, a conditional probability is defined as:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A, B)}{P(B)}$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$P(A, B) = P(A|B) P(B)$$\n",
    "\n",
    "This gives us a way to **decompose** complex joint probabilities, which we'll be using for language models.\n",
    "\n",
    "Back to text and language models.\n",
    "\n",
    "The probability of a text is expressed as:\n",
    "\n",
    "$$P(t_1, t_2, \\dots, t_n)$$\n",
    "\n",
    "This is read as the joint probability of token 1, token 2, and so on up to the last token.\n",
    "Now the problem with this is that language models don't calculate this directly.\n",
    "Instead, they calculate a conditional probability: the probability of a token given a prefix of tokens preceeding it.\n",
    "\n",
    "$$P(t_n | t_1, t_2, \\dots, t_{n-1})$$\n",
    "\n",
    "We can convert the joint probability into many conditional probabilities like this by repeatedly decomposing it.\n",
    "The joint probability can be decomposed using conditional probabilities:\n",
    "\n",
    "$P(t_1, t_2, \\dots, t_n) = P(t_1) \\times P(t_2, t_3, \\dots, t_n | t_1)$\n",
    "\n",
    "which is saying that the joint probability of all the tokens is equal to the probability of the first token multiplied by the probability of the rest of the tokens given the first token (as a prefix).\n",
    "We can then decompose the second part again:\n",
    "\n",
    "$P(t_2, t_3, \\dots, t_n | t_1) = P(t_2 | t_1) \\times P(t_3, t_4 \\dots, t_n | t_1, t_2)$\n",
    "\n",
    "The probability $P(t_2 | t_1)$ means the probability that the next token is $t_2$ given that the prefix is $t_1$, which is what our next token prediction neural network computes!\n",
    "We can keep decomposing until we get the following:\n",
    "\n",
    "$P(t_1, t_2, \\dots, t_n) = P(t_1) \\times P(t_2 | t_1) \\times P(t_3 | t_1, t_2) \\times \\dots \\times P(t_n | t_1, t_2, \\dots, t_{n-1})$\n",
    "\n",
    "So we'll end up with the following probabilities that need to be multiplied together:\n",
    "\n",
    "* the probability of the first token by\n",
    "* the probability of the second token following the first token by\n",
    "* the probability of the third token following the first and second tokens by\n",
    "* ...\n",
    "* the probability of the last token following the first up to the second to last token.\n",
    "\n",
    "We can use the neural network to calculate all of these probabilities except for the first one.\n",
    "But the first token is the edge token, which we know has a probability of 100% of being the first token, because we make sure that it is always at the beginning of every text.\n",
    "So we can ignore this probability because we'd just be multiplying by 1.\n",
    "\n",
    "Therefore, the probability of the sentence \"I like it.\" is calculated as:\n",
    "\n",
    "$P(\\text{I} | \\text{EDGE}) \\times P(\\text{like} | \\text{EDGE I}) \\times P(\\text{it} | \\text{EDGE I like}) \\times P(\\text{.} | \\text{EDGE I like it}) \\times P(\\text{EDGE} | \\text{EDGE I like it .})$\n",
    "\n",
    "So we just need to get the probabilities of each token in the text given its prefix, which can be easily calculated using the next token prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixes:\n",
      "tensor([[1, 3, 7, 6, 2, 0],\n",
      "        [1, 3, 5, 6, 2, 0],\n",
      "        [1, 3, 4, 5, 6, 2],\n",
      "        [1, 3, 4, 7, 6, 2]], device='cuda:0')\n",
      "\n",
      "next tokens:\n",
      "tensor([[3, 7, 6, 2, 1, 0],\n",
      "        [3, 5, 6, 2, 1, 0],\n",
      "        [3, 4, 5, 6, 2, 1],\n",
      "        [3, 4, 7, 6, 2, 1]], device='cuda:0')\n",
      "\n",
      "probability of every token in the vocabulary being the next token in every prefix\n",
      "tensor([[[1.5700e-11, 5.2787e-17, 8.0005e-20, 9.9990e-01, 1.2721e-08,\n",
      "          2.2534e-05, 5.8340e-05, 1.6823e-05],\n",
      "         [4.8782e-07, 2.7149e-10, 6.8849e-05, 1.1196e-07, 4.9981e-01,\n",
      "          2.5004e-01, 2.4536e-05, 2.5005e-01],\n",
      "         [9.8629e-07, 7.2571e-05, 1.8169e-08, 9.1097e-05, 1.1067e-05,\n",
      "          9.1128e-07, 9.9982e-01, 6.8019e-07],\n",
      "         [4.6111e-09, 2.1730e-05, 9.9990e-01, 4.8511e-18, 8.2945e-05,\n",
      "          3.3849e-09, 6.8333e-08, 3.3841e-09],\n",
      "         [2.4652e-10, 9.9990e-01, 3.7292e-05, 1.0911e-17, 3.0251e-11,\n",
      "          6.7203e-17, 6.3673e-05, 5.0148e-17],\n",
      "         [1.7285e-08, 6.7545e-01, 3.2450e-01, 1.8744e-17, 3.3416e-07,\n",
      "          1.0707e-12, 4.3820e-05, 9.1412e-13]],\n",
      "\n",
      "        [[1.5700e-11, 5.2787e-17, 8.0005e-20, 9.9990e-01, 1.2721e-08,\n",
      "          2.2534e-05, 5.8340e-05, 1.6823e-05],\n",
      "         [4.8782e-07, 2.7149e-10, 6.8849e-05, 1.1196e-07, 4.9981e-01,\n",
      "          2.5004e-01, 2.4536e-05, 2.5005e-01],\n",
      "         [1.0067e-06, 7.8169e-05, 2.0038e-08, 8.4528e-05, 1.1285e-05,\n",
      "          8.9320e-07, 9.9982e-01, 6.6700e-07],\n",
      "         [4.6082e-09, 2.1725e-05, 9.9990e-01, 4.8442e-18, 8.2893e-05,\n",
      "          3.3818e-09, 6.8277e-08, 3.3809e-09],\n",
      "         [2.4649e-10, 9.9990e-01, 3.7292e-05, 1.0908e-17, 3.0248e-11,\n",
      "          6.7191e-17, 6.3666e-05, 5.0140e-17],\n",
      "         [1.7298e-08, 6.7438e-01, 3.2558e-01, 1.8720e-17, 3.3528e-07,\n",
      "          1.0744e-12, 4.3747e-05, 9.1739e-13]],\n",
      "\n",
      "        [[1.5700e-11, 5.2787e-17, 8.0005e-20, 9.9990e-01, 1.2721e-08,\n",
      "          2.2534e-05, 5.8340e-05, 1.6823e-05],\n",
      "         [4.8782e-07, 2.7149e-10, 6.8849e-05, 1.1196e-07, 4.9981e-01,\n",
      "          2.5004e-01, 2.4536e-05, 2.5005e-01],\n",
      "         [4.5052e-12, 1.1842e-21, 1.4105e-15, 4.3040e-05, 1.8412e-04,\n",
      "          4.9986e-01, 6.9603e-10, 4.9992e-01],\n",
      "         [1.0054e-06, 8.4384e-05, 2.1175e-08, 7.7921e-05, 1.0989e-05,\n",
      "          8.3362e-07, 9.9982e-01, 6.2238e-07],\n",
      "         [4.6069e-09, 2.1713e-05, 9.9990e-01, 4.8419e-18, 8.2894e-05,\n",
      "          3.3819e-09, 6.8241e-08, 3.3810e-09],\n",
      "         [2.4650e-10, 9.9990e-01, 3.7292e-05, 1.0908e-17, 3.0248e-11,\n",
      "          6.7192e-17, 6.3666e-05, 5.0140e-17]],\n",
      "\n",
      "        [[1.5700e-11, 5.2787e-17, 8.0005e-20, 9.9990e-01, 1.2721e-08,\n",
      "          2.2534e-05, 5.8340e-05, 1.6823e-05],\n",
      "         [4.8782e-07, 2.7149e-10, 6.8849e-05, 1.1196e-07, 4.9981e-01,\n",
      "          2.5004e-01, 2.4536e-05, 2.5005e-01],\n",
      "         [4.5052e-12, 1.1842e-21, 1.4105e-15, 4.3040e-05, 1.8412e-04,\n",
      "          4.9986e-01, 6.9603e-10, 4.9992e-01],\n",
      "         [1.0122e-06, 1.0519e-04, 2.5355e-08, 6.1761e-05, 1.0402e-05,\n",
      "          6.9881e-07, 9.9982e-01, 5.2159e-07],\n",
      "         [4.6118e-09, 2.1710e-05, 9.9990e-01, 4.8541e-18, 8.3009e-05,\n",
      "          3.3890e-09, 6.8319e-08, 3.3881e-09],\n",
      "         [2.4655e-10, 9.9990e-01, 3.7293e-05, 1.0913e-17, 3.0255e-11,\n",
      "          6.7217e-17, 6.3680e-05, 5.0159e-17]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('prefixes:')\n",
    "print(train_text_x_indexed)\n",
    "print()\n",
    "\n",
    "print('next tokens:')\n",
    "print(train_text_y_indexed)\n",
    "print()\n",
    "\n",
    "output = torch.softmax(model(train_text_x_indexed), dim=2)\n",
    "print('probability of every token in the vocabulary being the next token in every prefix')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're not interested in the probability of every possible token in the vocabulary for every token position in `train_text_x_indexed`.\n",
    "What we want are the probabilities of the correct next token according to `train_text_y_indexed` only.\n",
    "So we somehow need to select the needed number in every vector of `output` for every vector of probabilities it contains.\n",
    "We need something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first vector of outputs:\n",
      "tensor([1.5700e-11, 5.2787e-17, 8.0005e-20, 9.9990e-01, 1.2721e-08, 2.2534e-05,\n",
      "        5.8340e-05, 1.6823e-05], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "index to extract from here:\n",
      "tensor(3, device='cuda:0')\n",
      "\n",
      "selected probability:\n",
      "tensor(0.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "\n",
      "second vector of outputs:\n",
      "tensor([4.8782e-07, 2.7149e-10, 6.8849e-05, 1.1196e-07, 4.9981e-01, 2.5004e-01,\n",
      "        2.4536e-05, 2.5005e-01], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "index to extract from here:\n",
      "tensor(7, device='cuda:0')\n",
      "\n",
      "selected probability:\n",
      "tensor(0.2501, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('first vector of outputs:')\n",
    "print(output[0, 0, :])\n",
    "print()\n",
    "print('index to extract from here:')\n",
    "print(train_text_y_indexed[0, 0])\n",
    "print()\n",
    "print('selected probability:')\n",
    "print(output[0, 0, train_text_y_indexed[0, 0]])\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('second vector of outputs:')\n",
    "print(output[0, 1, :])\n",
    "print()\n",
    "print('index to extract from here:')\n",
    "print(train_text_y_indexed[0, 1])\n",
    "print()\n",
    "print('selected probability:')\n",
    "print(output[0, 1, train_text_y_indexed[0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to extract a different index from every vector in the 3D tensor `outputs`.\n",
    "This is a complex selection mode that cannot be done using normal square bracket indexing.\n",
    "We need to use the `gather` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [3],\n",
      "        [4]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tensor  = torch.tensor([[0, 1], [2, 3], [4, 5]], dtype=torch.int64, device=device)\n",
    "indexes = torch.tensor([[ 0  ], [ 1  ], [ 0  ]], dtype=torch.int64, device=device)\n",
    "print(tensor.gather(1, indexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we're selecting index 0 from the first vector, index 1 from the second vector, and index 0 from the third vector.\n",
    "\n",
    "We'll explain this function in the way that is relevant to our needs, as `gather` can actually do more than what is explained below.\n",
    "`gather` takes a dimension and a tensor of indexes.\n",
    "The dimension says where to find the vectors to work on in the tensor.\n",
    "Since we want to select from the row vectors of the matrix, we specify dimension 1 (the rows).\n",
    "The indexes are a row-by-row specification of which element to extract from each vector.\n",
    "It must have the same shape as the tensor so that the indexes can be matched with the vectors.\n",
    "\n",
    "In our case, we have a 3D tensor (`outputs`) and 2D indexes (`train_text_y_indexed`).\n",
    "The vectors in `outputs` are found in dimension 2 and we need to add a singleton dimension at the end of `train_text_y_indexed` to make it compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of the tokens in y being the next token in every prefix\n",
      "tensor([[9.9990e-01, 2.5005e-01, 9.9982e-01, 9.9990e-01, 9.9990e-01, 1.7285e-08],\n",
      "        [9.9990e-01, 2.5004e-01, 9.9982e-01, 9.9990e-01, 9.9990e-01, 1.7298e-08],\n",
      "        [9.9990e-01, 4.9981e-01, 4.9986e-01, 9.9982e-01, 9.9990e-01, 9.9990e-01],\n",
      "        [9.9990e-01, 4.9981e-01, 4.9992e-01, 9.9982e-01, 9.9990e-01, 9.9990e-01]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "token_probs = output.gather(2, train_text_y_indexed[:, :, None])[:, :, 0]\n",
    "print('probability of the tokens in y being the next token in every prefix')\n",
    "print(token_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these probabilities are each of those $P(t_n | t_1, t_2, \\dots, t_{n-1})$ factors that need to be multiplied together.\n",
    "Of course there's a matrix of these because each row belongs to a different text, so we should only multiply together the probabilities in the rows.\n",
    "We still need to get rid of the pad tokens, which we need to replace with 1, since they're being multiplied together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked token probabilities:\n",
      "tensor([[0.9999, 0.2501, 0.9998, 0.9999, 0.9999, 1.0000],\n",
      "        [0.9999, 0.2500, 0.9998, 0.9999, 0.9999, 1.0000],\n",
      "        [0.9999, 0.4998, 0.4999, 0.9998, 0.9999, 0.9999],\n",
      "        [0.9999, 0.4998, 0.4999, 0.9998, 0.9999, 0.9999]], device='cuda:0',\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "\n",
      "probability of whole texts:\n",
      "tensor([0.2499, 0.2499, 0.2497, 0.2497], device='cuda:0',\n",
      "       grad_fn=<ProdBackward1>)\n"
     ]
    }
   ],
   "source": [
    "token_probs = token_probs.masked_fill(pad_mask, 1.0)\n",
    "print('masked token probabilities:')\n",
    "print(token_probs)\n",
    "print()\n",
    "\n",
    "text_probs = token_probs.prod(dim=1)\n",
    "print('probability of whole texts:')\n",
    "print(text_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how each text has a probability of approximately 25%, which is because we overfitted the language model on the training set and it now thinks that those four texts are the only texts that could ever exist.\n",
    "\n",
    "Multiplying a bunch of probabilities together can result in a number that is too small to do anything with (or it gets rounded down to zero).\n",
    "This problem is usually avoided by using logarithmic probabilities.\n",
    "Logarithms of probabilities are larger and more easily usable than the probabilities themselves.\n",
    "\n",
    "An important change with logarithmic probabilities is that multiplying probabilities together is replaced with adding logarithmic probabilities together:\n",
    "\n",
    "$\\log(p_1 \\times p_2 \\times \\dots \\times p_n) = \\log(p_1) + \\log(p_2) + \\dots + \\log(p_n)$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$\\log P(\\text{I like it . EDGE}) = \\log P(\\text{I} | \\text{EDGE}) + \\log P(\\text{like} | \\text{EDGE I}) + \\log P(\\text{it} | \\text{EDGE I like}) + \\log P(\\text{.} | \\text{EDGE I like it}) + \\log P(\\text{EDGE} | \\text{EDGE I like it .})$\n",
    "\n",
    "You can then remove the logarithm by exponentiating it:\n",
    "\n",
    "$P(\\text{I like it .}) = e^{\\log P(\\text{I like it .})}$\n",
    "\n",
    "If you just want to sort your sentences by probability, you can just work with the logarithmic probabilities directly without converting them into actual probabilities because the largest logarithmic probabilities is also the largest probability.\n",
    "\n",
    "The use of logarithmic probabilities is such a common practice that PyTorch provides a log softmax function which immediately calculates the log of probabilities from logits: `torch.log_softmax`.\n",
    "\n",
    "Here is the complete code for getting both the logarithmic probabilities and the probabilities of texts using a language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'it', '.']: 0.24993 (-1.38657)\n",
      "['I', 'hate', 'it', '.']: 0.24992 (-1.38660)\n",
      "['I', \"don't\", 'hate', 'it', '.']: 0.24971 (-1.38744)\n",
      "['I', \"don't\", 'like', 'it', '.']: 0.24974 (-1.38733)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = torch.log_softmax(model(train_text_x_indexed), dim=2)\n",
    "    token_log_probs = output.gather(2, train_text_y_indexed[:, :, None])[:, :, 0]\n",
    "    token_log_probs = token_log_probs.masked_fill(pad_mask, 0.0)\n",
    "    text_log_probs = token_log_probs.sum(dim=1).cpu().tolist()\n",
    "\n",
    "for (text, log_prob) in zip(train_text_tokens, text_log_probs):\n",
    "    print(f'{text}: {np.exp(log_prob):.5f} ({log_prob:.5f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating language models\n",
    "\n",
    "This brings us to how to evaluate a language model.\n",
    "A good language model is one that always gives a high probability to the correct next token.\n",
    "The standard way this is measured is using the **perplexity measure** (**pplx**):\n",
    "\n",
    "$\\text{pplx}(s) = e^{-\\frac{1}{|T|} \\log{P(T)}}$\n",
    "\n",
    "where $T$ is a text and $|T|$ is the number of tokens in the sentence.\n",
    "The text is usually not from the training set in order to check for overfitting.\n",
    "What this function measures is the average perplexity (confusion) that the model experiences when trying to guess the next word in every prefix in the sentence.\n",
    "If you get a perplexity of 50, that means that, on average, the model was as perplexed as having to pick the correct item out of 50.\n",
    "The smaller the perplexity, the better the language model, because it could confidently guess the words in the sentence.\n",
    "A perfect language model (which is impossible on a non-trivial test set) would have a perplexity of 1.\n",
    "\n",
    "It is also just $e$ to the power of the cross-entropy of a language model on a text.\n",
    "In fact we can calculate it very easily by just using `torch.nn.functional.cross_entropy`, without having to use `gather` or `log_softmax`.\n",
    "Just be sure to mask out the pad tokens as usual.\n",
    "\n",
    "Below we're finding the perplexity of the language model with respect to the whole training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pplx: 1.2868258953094482\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(train_text_x_indexed)\n",
    "    train_token_xes = torch.nn.functional.cross_entropy(logits.transpose(1, 2), train_text_y_indexed, reduction='none')\n",
    "    train_token_xes = train_token_xes.masked_fill(pad_mask, 0.0)\n",
    "    train_xe = train_token_xes.sum()/(~pad_mask).sum()\n",
    "    pplx = torch.exp(train_xe).cpu().tolist()\n",
    "print('pplx:', pplx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token sampling\n",
    "\n",
    "We have seen how to guess the next item in a sequence and how to compute the probability of a sequence, but you were promised that we'd be generating sequences in this topic.\n",
    "This can be easily made using the probabilities of all tokens being the next token in a prefix.\n",
    "\n",
    "The idea is to start from a prefix consisting of just the edge token and iteratively add more tokens to the prefix until we have a complete text as a prefix.\n",
    "The tokens chosen to be the next token in the prefix will be chosen randomly based on their probabilities.\n",
    "This is called **sampling** tokens.\n",
    "So the more probable a token is to be the next token, the more likely it will be chosen as the next token.\n",
    "This can easily be done using the `random.choices` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n",
      "20\n",
      "30\n",
      "30\n",
      "30\n",
      "10\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "10\n",
      "30\n",
      "30\n",
      "30\n",
      "10\n",
      "20\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    print(random.choices([10, 20, 30], [0.1, 0.1, 0.8])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep on adding tokens to our prefix until the edge token is chosen, or until some maximum number of tokens is reached.\n",
    "\n",
    "We'll create a fixed tensor for the prefix that is as long as the maximum number of tokens to generate.\n",
    "It will start containing only the edge token and the rest is filled with pad tokens.\n",
    "We'll then iteratively predict the next token from the non-pad tokens in the tensor and add it to the prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<EDGE>', 'I', 'like', 'it', '.', '<EDGE>'] (p=0.250)\n"
     ]
    }
   ],
   "source": [
    "def sample_generate(model, vocab, edge_index, pad_index, max_len):\n",
    "    prefix_indexed = torch.tensor([[edge_index] + [pad_index]*max_len], dtype=torch.int64, device=device) # Initialise the prefix to just the edge token followed by pad tokens. It's faster to update a single element in a tensor than to recreate a whole new one.\n",
    "    prefix_prob = 1.0\n",
    "    prefix_len = 1\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_len):\n",
    "            outputs = torch.softmax(model(prefix_indexed[:, :i+1]), dim=2)\n",
    "            token_probs = outputs[0, -1, :].cpu().tolist() # Get the probabilities of each token in the vocabulary being the next token in the prefix.\n",
    "            next_token_index = random.choices(range(len(vocab)), token_probs)[0] # Randomly choose a token according to its probability.\n",
    "            prefix_indexed[0, i+1] = next_token_index # Put this token in the prefix.\n",
    "            prefix_prob *= token_probs[next_token_index] # Update the probability of the prefix.\n",
    "            prefix_len += 1\n",
    "            if next_token_index == edge_index: # Stop selecting tokens when the edge token is selected.\n",
    "                break\n",
    "    text = [vocab[index] for index in prefix_indexed[0, :prefix_len].cpu().tolist()] # Convert the indexes in the prefix to string tokens.\n",
    "    return (text, prefix_prob)\n",
    "\n",
    "(text, text_prob) = sample_generate(model, vocab, edge_index, pad_index, max_len=10)\n",
    "print(text, f'(p={text_prob:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than recreating a new prefix tensor every time a new token is selected, a maximum length prefix tensor is created at the start and is then updated with new tokens one token position at a time.\n",
    "This is faster.\n",
    "\n",
    "Note that we usually don't include the unknown token with the generated tokens as it is not a real word.\n",
    "Thankfully `random.choices` does not expect the probabilities to add up to 1 as it normalises the numbers so that they add up to 1 prior to using them.\n",
    "With this in mind, we can avoid generating unknown tokens by replacing the probability of the unknown token with 0.0 so that it would never be chosen, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generate_no_unk(model, vocab, edge_index, pad_index, unk_index, max_len):\n",
    "    prefix_indexed = torch.tensor([[edge_index] + [pad_index]*max_len], dtype=torch.int64, device=device)\n",
    "    prefix_prob = 1.0\n",
    "    prefix_len = 1\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_len):\n",
    "            outputs = torch.softmax(model(prefix_indexed[:, :i+1]), dim=2)\n",
    "            token_probs = outputs[0, -1, :].cpu().tolist()\n",
    "            token_probs[unk_index] = 0.0 # Replace the probability of the unknown token with 0.0.\n",
    "            next_token_index = random.choices(range(len(vocab)), token_probs)[0]\n",
    "            prefix_indexed[0, i+1] = next_token_index\n",
    "            prefix_prob *= token_probs[next_token_index]\n",
    "            prefix_len += 1\n",
    "            if next_token_index == edge_index:\n",
    "                break\n",
    "    text = [vocab[index] for index in prefix_indexed[0, :prefix_len].cpu().tolist()]\n",
    "    return (text, prefix_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Write a movie review\n",
    "\n",
    "Train a language model on the texts in the movie review data set.\n",
    "Preprocessing has been done for you.\n",
    "\n",
    "Measure the perplexity of the language model on the test set.\n",
    "Also generate a 3 random reviews from the trained language model.\n",
    "\n",
    "Note that language model training is considerably slow compared to classification, mostly due to the huge softmax.\n",
    "Note also that if you haven't been using batching up to now, you might need to use it now as language models use a considerable amount of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "min_freq = 3\n",
    "\n",
    "train_df = pd.read_csv('../data_set/sentiment/train.csv')\n",
    "test_df = pd.read_csv('../data_set/sentiment/test.csv')\n",
    "\n",
    "train_text = train_df['text']\n",
    "test_text = test_df['text']\n",
    "\n",
    "nltk.download('punkt')\n",
    "train_text_tokens = [nltk.word_tokenize(text) for text in train_text]\n",
    "test_text_tokens = [nltk.word_tokenize(text) for text in test_text]\n",
    "max_len = max(max(len(text) for text in train_text_tokens), max(len(text) for text in test_text_tokens)) + 1\n",
    "\n",
    "frequencies = collections.Counter(token for text in train_text_tokens for token in text)\n",
    "vocabulary = sorted(frequencies.keys(), key=frequencies.get, reverse=True)\n",
    "while frequencies[vocabulary[-1]] < min_freq:\n",
    "    vocabulary.pop()\n",
    "vocab = ['<PAD>', '<EDGE>', '<UNK>'] + vocabulary\n",
    "token2index = {token: i for (i, token) in enumerate(vocab)}\n",
    "pad_index = token2index['<PAD>']\n",
    "edge_index = token2index['<EDGE>']\n",
    "unk_index = token2index['<UNK>']\n",
    "\n",
    "train_text_x_indexed_np = np.full((len(train_text_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_text_tokens)):\n",
    "    train_text_x_indexed_np[i, 0] = edge_index\n",
    "    for j in range(len(train_text_tokens[i])):\n",
    "        train_text_x_indexed_np[i, j + 1] = token2index.get(train_text_tokens[i][j], unk_index)\n",
    "train_text_x_indexed = torch.tensor(train_text_x_indexed_np, device=device)\n",
    "\n",
    "train_text_y_indexed_np = np.full((len(train_text_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_text_tokens)):\n",
    "    for j in range(len(train_text_tokens[i])):\n",
    "        train_text_y_indexed_np[i, j] = token2index.get(train_text_tokens[i][j], unk_index)\n",
    "    train_text_y_indexed_np[i, len(train_text_tokens[i])] = edge_index\n",
    "train_text_y_indexed = torch.tensor(train_text_y_indexed_np, device=device)\n",
    "\n",
    "test_text_x_indexed_np = np.full((len(test_text_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(test_text_tokens)):\n",
    "    test_text_x_indexed_np[i, 0] = edge_index\n",
    "    for j in range(len(test_text_tokens[i])):\n",
    "        test_text_x_indexed_np[i, j + 1] = token2index.get(test_text_tokens[i][j], unk_index)\n",
    "test_text_x_indexed = torch.tensor(test_text_x_indexed_np, device=device)\n",
    "\n",
    "test_text_y_indexed_np = np.full((len(test_text_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(test_text_tokens)):\n",
    "    for j in range(len(test_text_tokens[i])):\n",
    "        test_text_y_indexed_np[i, j] = token2index.get(test_text_tokens[i][j], unk_index)\n",
    "    test_text_y_indexed_np[i, len(test_text_tokens[i])] = edge_index\n",
    "test_text_y_indexed = torch.tensor(test_text_y_indexed_np, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75ee2b71ad44bf9ef4e9bee896f68ffbc764a6a2c6d1f57c86c48f99ffc25ca8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
