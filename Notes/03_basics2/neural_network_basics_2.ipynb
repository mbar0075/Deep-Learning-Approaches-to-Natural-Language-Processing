{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network basics 2\n",
    "\n",
    "In the previous chapter we saw some history about the perceptron and said that people were thinking that this simple model could do very complex things.\n",
    "Minsky and Papert changed all that when they published their mathematical proof in 1969 showing that perceptrons were not even able to correctly classify whether two inputs are equal or not.\n",
    "Logistic regression classifiers have this same problem.\n",
    "Let's see what this means and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The XOR problem\n",
    "\n",
    "The training set below is meant to make a neural network learn to output a 0 if both inputs are the same and a 1 if both inputs are different, which is known as an exclusive-or (XOR):\n",
    "\n",
    "| $x_0$ | $x_1$ | $y$ |\n",
    "|-------|-------|-----|\n",
    "| 0     | 0     | 0   |\n",
    "| 0     | 1     | 1   |\n",
    "| 1     | 0     | 1   |\n",
    "| 1     | 1     | 0   |\n",
    "\n",
    "Let's try making a logistic regression classifier learn the above training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32, device=device)\n",
    "train_y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32, device=device)\n",
    "\n",
    "class Logistic(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) # Just the logits.\n",
    "\n",
    "model = Logistic()\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "print('step', 'error')\n",
    "train_errors = []\n",
    "for step in range(1, 100 + 1):\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if step%10 == 0:\n",
    "        print(step, train_errors[-1])\n",
    "print()\n",
    "\n",
    "print('x0', 'x1', 'y')\n",
    "output = torch.sigmoid(model(train_x))[:, 0].detach().cpu().tolist()\n",
    "for (x, y) in zip(train_x.cpu().tolist(), output):\n",
    "    print(x[0], x[1], y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('step')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best the model could learn was to make everything 0.5.\n",
    "There are no parameters that will make the logistic regression classifier give you the right output for all inputs.\n",
    "We can see why this with a 3D input-output plot of the classifier for different parameters.\n",
    "To avoid having a difficult-to-see 3D graph, we'll instead be using a heat map that uses colour to show where the output is high (red) and low (blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x:1/(1 + np.exp(-x))\n",
    "\n",
    "(fig, axs) = plt.subplots(1, 8, figsize=(20, 2))\n",
    "for i in range(8):\n",
    "    axs[i].set_xlabel('$x_0$')\n",
    "    axs[i].set_ylabel('$x_1$')\n",
    "\n",
    "    w1 = 4*(2*np.random.random() - 1)\n",
    "    w2 = 4*(2*np.random.random() - 1)\n",
    "    b = 4*(2*np.random.random() - 1)\n",
    "\n",
    "    (xs1, xs0) = np.mgrid[0:1.01:0.01, 0:1.01:0.01]\n",
    "    ys = sigmoid(w1*xs0 + w2*xs1 + b)\n",
    "    axs[i].matshow(ys, cmap='bwr', extent=(0, 1, 0, 1))\n",
    "\n",
    "    print('w1: {:6.3f}, w2: {:6.3f}, b: {:6.3f}'.format(w1, w2, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training set is concerned with the four corners of these heat maps.\n",
    "Each corner corresponds to the input $(0,0)$, $(0,1)$, $(1,0)$, and $(1,1)$.\n",
    "    In the previous topic, we trained our classifier to make an AND gate where $(0,0)$ output $0$ (blue), $(0,1)$ output $0$ (blue), $(1,0)$ output $0$ (blue), and $(1,1)$ output $1$ (red).\n",
    "So basically three corners had to be blue whilst the top right corner had to be red.\n",
    "\n",
    "If you look at how the heat maps change for different parameters, you'd notice that there's always a straight white line separating the red region from the blue region.\n",
    "Note how the only thing that the parameters change is where the white line is and how it's rotated.\n",
    "It never curves and there is never more than one of it.\n",
    "It's easy to imagine how the line should be positioned to get a red top right corner with all the other corners being blue.\n",
    "Unfortunately, in the case of the XOR training set, there is no position for the white line that works, because we'd need to position the line in such a way to get the coloured corners below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('$x_0$')\n",
    "ax.set_ylabel('$x_1$')\n",
    "\n",
    "ys = np.full((100, 100), 0.5)\n",
    "ys[:25, :25] = 1\n",
    "ys[:25, -25:] = 0\n",
    "ys[-25:, -25:] = 1\n",
    "ys[-25:, :25] = 0\n",
    "ax.matshow(ys, cmap='bwr', extent=(0, 1, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single straight line cannot separate reds and blues as shown above, so, given that different parameters can only work with a straight line, this proves that the logistic regression classifier cannot learn the given training set.\n",
    "We call such problems, that can be solved by a straight line, **linearly separable problems**.\n",
    "The XOR problem is a **non-linearly separable problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hidden layer\n",
    "\n",
    "When Minsky and Papert published this proof about perceptrons (which was slightly more mathematical than what was shown here), they also gave a solution to it: pass the output of several perceptrons into another perceptron.\n",
    "A set of perceptrons connected in this way could in fact solve the XOR problem, provided you had the right parameters.\n",
    "This is what the graphical diagram of such a model looks like:\n",
    "\n",
    "![](neuron_diagram.png)\n",
    "\n",
    "The neural units aren't connected randomly, they have to be organised into **layers**.\n",
    "This means that every unit in one layer is connected to all the units in the next layer, and no where else.\n",
    "Note how we now need to add a third number to the subscript of the weights and a second number to the subscript of the biases in order to specify in which layer they belong to.\n",
    "\n",
    "The layers have names: the first layer of neural units is called the **hidden layer**, because you don't get to see the outputs of this layer, and the second layer is called the **output layer**.\n",
    "The output numbers by each neural unit are called activations.\n",
    "We can also visualise the architecture as a simplified block diagram:\n",
    "\n",
    "![](block_diagram.png)\n",
    "\n",
    "Let's an input-output heat map two-layer neural network trained to solve the XOR problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('$x_0$')\n",
    "ax.set_ylabel('$x_1$')\n",
    "\n",
    "sigmoid = lambda x:1/(1 + np.exp(-x))\n",
    "(xs1, xs0) = np.mgrid[0:1.01:0.01, 0:1.01:0.01]\n",
    "h1 = sigmoid(-10.0*xs0 + -10.0*xs1 + 15.0)\n",
    "h2 = sigmoid(-10.0*xs0 + -10.0*xs1 + 5.0)\n",
    "ys = sigmoid(10.0*h1 + -10.0*h2 + -15.0)\n",
    "ax.matshow(ys, cmap='bwr', extent=(0, 1, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we see an example of a non-linear model.\n",
    "We can now have two slopes in our 3D graph, which allows us to solve the XOR problem.\n",
    "\n",
    "It's important to keep in mind that each one of the hidden neural units is still performing a linearly separable task.\n",
    "What's happening is that we are combining two lines together into a single model.\n",
    "Let's see the heat map of each hidden neural unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, axs) = plt.subplots(1, 3)\n",
    "axs[0].set_xlabel('$x_0$')\n",
    "axs[0].set_ylabel('$x_1$')\n",
    "axs[1].set_xlabel('$x_0$')\n",
    "axs[1].set_ylabel('$x_1$')\n",
    "axs[2].set_xlabel('$x_0$')\n",
    "axs[2].set_ylabel('$x_1$')\n",
    "\n",
    "sigmoid = lambda x:1/(1 + np.exp(-x))\n",
    "(xs1, xs0) = np.mgrid[0:1.01:0.01, 0:1.01:0.01]\n",
    "h0 = sigmoid(-10.0*xs0 + -10.0*xs1 + 15.0)\n",
    "h1 = sigmoid(-10.0*xs0 + -10.0*xs1 + 5.0)\n",
    "ys = sigmoid(10.0*h0 + -10.0*h1 + -15.0)\n",
    "\n",
    "axs[0].matshow(h0, cmap='bwr', extent=(0, 1, 0, 1))\n",
    "axs[0].text(0.4, 0.4, 'h0')\n",
    "axs[1].matshow(h1, cmap='bwr', extent=(0, 1, 0, 1))\n",
    "axs[1].text(0.4, 0.4, 'h1')\n",
    "axs[2].matshow(ys, cmap='bwr', extent=(0, 1, 0, 1))\n",
    "axs[2].text(0.4, 0.4, 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it was known that this was possible in 1969, the problem was that everyone was trying to use perceptrons in the hidden layer, that is, use the sign function, which makes the whole neural network undifferentiable.\n",
    "This is because the sign function has no gradient (it's always 0 as it has no slope steepness to measure).\n",
    "\n",
    "Leaving out the sign function also doesn't work as it makes the two layers behave exactly like a single layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, axs) = plt.subplots(1, 8, figsize=(20, 2))\n",
    "for i in range(8):\n",
    "    axs[i].set_xlabel('$x_0$')\n",
    "    axs[i].set_ylabel('$x_1$')\n",
    "\n",
    "    w000 = 4*(2*np.random.random() - 1)\n",
    "    w010 = 4*(2*np.random.random() - 1)\n",
    "    w001 = 4*(2*np.random.random() - 1)\n",
    "    w011 = 4*(2*np.random.random() - 1)\n",
    "    b00 = 4*(2*np.random.random() - 1)\n",
    "    b01 = 4*(2*np.random.random() - 1)\n",
    "    w100 = 4*(2*np.random.random() - 1)\n",
    "    w110 = 4*(2*np.random.random() - 1)\n",
    "    b10 = 4*(2*np.random.random() - 1)\n",
    "    \n",
    "    (xs1, xs0) = np.mgrid[0:1.01:0.01, 0:1.01:0.01]\n",
    "    hs0 = w000*xs0 + w010*xs1 + b00\n",
    "    hs1 = w001*xs0 + w011*xs1 + b01\n",
    "    ys = hs0*w100 + hs1*w110 + b10\n",
    "    axs[i].matshow(ys, cmap='bwr', extent=(0, 1, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need some kind of non-linear function, called an **activation function**, in the hidden layer or you'll still be working with something that is effectively a single-layer neural network.\n",
    "\n",
    "Replacing the sign function with sigmoid function, on the other hand, solves both problems as it is differentiable and non-linear, which is how the XOR problem was solved by Rumelhart, Hinton, and Williams in 1986: feeding the output of a logistic regression classifier into another logistic regression classifier.\n",
    "\n",
    "Let's use this two-layer neural network in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32, device=device)\n",
    "train_y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32, device=device)\n",
    "\n",
    "class TwoLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(2, 2)\n",
    "        self.layer2 = torch.nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.sigmoid(self.layer1(x))\n",
    "        return self.layer2(hidden) # Just the logits.\n",
    "\n",
    "model = TwoLayer()\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=10.0)\n",
    "\n",
    "print('step', 'error')\n",
    "train_errors = []\n",
    "for step in range(1, 1000+1):\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if step%100 == 0:\n",
    "        print(step, train_errors[-1])\n",
    "print()\n",
    "\n",
    "print('x0', 'x1', 'y')\n",
    "output = torch.sigmoid(model(train_x))[:, 0].detach().cpu().tolist()\n",
    "for (x, y) in zip(train_x.cpu().tolist(), output):\n",
    "    print(x[0], x[1], y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('step')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more neural units in the hidden layer allows us to create more complex heat map shapes when plotting the model's input-output plot.\n",
    "Below is a plot of a three unit hidden layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('$x_0$')\n",
    "ax.set_ylabel('$x_1$')\n",
    "\n",
    "sigmoid = lambda x:1/(1 + np.exp(-x))\n",
    "(xs1, xs0) = np.mgrid[0:1.01:0.01, 0:1.01:0.01]\n",
    "h0 = sigmoid(10.0*xs0 + 5.0*xs1 + -10.0)\n",
    "h1 = sigmoid(-10.0*xs0 + 10.0*xs1 + 0.0)\n",
    "h2 = sigmoid(5.0*xs0 + -10.0*xs1 + 0.0)\n",
    "ys = sigmoid(-10.0*h0 + -10.0*h1 + -10.0*h2 + 5.0)\n",
    "\n",
    "ax.matshow(ys, cmap='bwr', extent=(0, 1, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A larger hidden layer will allow for more complexity, but is there enough complexity?\n",
    "Will adding a second hidden layer allow the network to perform even more complex behaviour?\n",
    "\n",
    "The answer is no: a large enough single hidden layer can perform any task, provided that you find the right parameters.\n",
    "In fact there is a [mathematical proof](https://www.sciencedirect.com/science/article/abs/pii/0893608089900208) that shows that single hidden layer neural networks (with enough neural units) are **universal approximators**, that is, can have any finite region of their input-output plot be made into approximately any shape.\n",
    "'Approximately any shape' means that you can get as close as you want but, in many cases, never reach a perfect match.\n",
    "Note that polynomials are also univeral approximators, given enough terms in the polynomial.\n",
    "\n",
    "On the other hand, there is also [another mathematical proof](http://proceedings.mlr.press/v49/eldan16.html) that shows that, whilst having two hidden layers does not add more expressiveness than that of a single hidden layer, it does enable the model to require less neural units in total to perform the same task.\n",
    "So adding more hidden layers results in more efficient networks that can be more complex with less parameters, which is always good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training better\n",
    "\n",
    "Whilst it is definitely possible to optimise our two layer model to learn many complex behaviour, it is also more difficult to do so than the normal logistic regression classifier.\n",
    "This is because the error surface is no longer convex.\n",
    "\n",
    "We can't visualise the parameter-error plot of all 9 parameters as that would require us to plot a 10-dimensional graph.\n",
    "What we'll do instead is show a 3D slice through the 10-dimensional graph by only varying two of the ten parameters whilst keeping all the other parameters fixed to a random value.\n",
    "We'll be plotting the error on the XOR training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(1, 1, subplot_kw={'projection': '3d'})\n",
    "ax.set_xlabel('$b00$')\n",
    "ax.set_ylabel('$w000$')\n",
    "ax.set_zlabel('$E$')\n",
    "\n",
    "w001 = 20*(2*np.random.random() - 1)\n",
    "w010 = 20*(2*np.random.random() - 1)\n",
    "w011 = 20*(2*np.random.random() - 1)\n",
    "b01  = 20*(2*np.random.random() - 1)\n",
    "w100 = 20*(2*np.random.random() - 1)\n",
    "w110 = 20*(2*np.random.random() - 1)\n",
    "b10  = 20*(2*np.random.random() - 1)\n",
    "\n",
    "sig = lambda x:1/(1 + np.exp(-x))\n",
    "nn = lambda x0, x1, w000, w001, w010, w011, b00, b01, w100, w110, b10: \\\n",
    "    sig(w100*sig(x0*w000 + x1*w010 + b00) + w110*sig(x0*w001 + x1*w011 + b01) + b10)\n",
    "f = lambda w000,b00:0.25*(\n",
    "    -np.log(1 - nn(0, 0, w000, w001, w010, w011, b00, b01, w100, w110, b10)) # target 0\n",
    "    + -np.log(nn(0, 1, w000, w001, w010, w011, b00, b01, w100, w110, b10)) # target 1\n",
    "    + -np.log(nn(1, 0, w000, w001, w010, w011, b00, b01, w100, w110, b10)) # target 1\n",
    "    + -np.log(1 - nn(1, 1, w000, w001, w010, w011, b00, b01, w100, w110, b10)) # target 0\n",
    ")\n",
    "(ws, bs) = np.mgrid[-20:20.1:0.1, -20:20.1:0.1]\n",
    "es = f(ws, bs)\n",
    "ax.plot_surface(bs, ws, es, color='red', linewidth=0, antialiased=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine how complex the entire 10-dimensional graph is.\n",
    "Due to this complexity, several techniques were developed in order to help with training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "Since the error surface is no longer convex, we now run the risk of getting stuck in a **local minimum**, which is a point in the error surface where any small change in parameters will result in a larger error, so training cannot continue, but which is not the smallest error in the entire error surface.\n",
    "Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('$p$')\n",
    "ax.set_ylabel('$E$')\n",
    "\n",
    "f = lambda p:0.052*p**4 - 0.69*p**3 + 3.09*p**2 - 5.63*p + 5.02\n",
    "ps = np.mgrid[0:7.1:0.01]\n",
    "es = f(ps)\n",
    "ax.plot(ps, es, color='red', marker='', linestyle='-', linewidth=1)\n",
    "\n",
    "ax.plot([1.8, 5.3], [1.45, 0.3], color='black', marker='o', markersize=5, linestyle='')\n",
    "ax.text(0.8, 2.0, 'local minimum', fontdict={'fontsize': 12})\n",
    "ax.text(3.5, 1.0, 'local and global minimum', fontdict={'fontsize': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see an error surface with a single parameter ($p$) which has two local minima, only one of which is a **global minimum**.\n",
    "If gradient descent gets you in the first local minimum, it will not be able to escape it (unless the learning rate is large, in which case you'll have bigger problems).\n",
    "\n",
    "To make gradient descent be able to avoid this problem, we do not calculate the error and its gradient on the entire training set.\n",
    "Instead we take a random sample of training set rows and calculate the error on that.\n",
    "This sample is called a **batch**.\n",
    "After completing an update step on this batch, a different batch is sampled which doesn't share any training items with the first, and another update step is performed based on the new batch, and so on until all the items in the training set have been sampled.\n",
    "When all the training set has been visited, you have performed an **epoch**.\n",
    "From now on, we'll refer to steps of training on the entire training set as an epoch and to training on a single batch as a step.\n",
    "\n",
    "The first advantage of using batches is that you do not need to pass in the entire training set to the neural network, which will likely result in the GPU running out of memory for large data sets.\n",
    "The second advantage is that, since the error surface depends not only on the model but also the training set, the error surface will be changing with each update, meaning that getting stuck in a local minimum will probably only last for one update step.\n",
    "After changing the batch you will have a new error surface which will have differently positioned local minima, and, on average, you would be able to continue progressing.\n",
    "\n",
    "Of course this makes training less stable and the error will tend to fluctuate during training, which is a disadvantage.\n",
    "To make the process more stable, we use a smaller learning rate in order to avoid going too far off in random directions.\n",
    "In general, the process tends to improve training and is the standard way to train neural networks.\n",
    "The optimisation algorithm of using gradient descent with minibatches is called **stochastic gradient descent** (**SGD**) because of the randomness (**stochasticity**) introduced from the random training samples.\n",
    "\n",
    "The full SGD algorithm works as follows:\n",
    "\n",
    "1. Randomly shuffle the order of the rows in the training set.\n",
    "1. Take the first $b$ items, where $b$ is a batch size that you choose.\n",
    "    This will be the first batch.\n",
    "1. Now perform an update step on this batch and then take the next $b$ items from the training set.\n",
    "1. Repeat this process until all the items in the training set have been visited.\n",
    "1. Having finished an epoch, reshuffle the training set and repeat for a number of epochs.\n",
    "\n",
    "This guarantees that all the items in the training set are visited equally and that it is unlikely that the same batch of items is used more than once.\n",
    "\n",
    "Below is how you'd do this in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by having your training set defined as a NumPy array instead of a PyTorch tensor.\n",
    "The batch that will be passed through the model will be placed in a PyTorch tensor but the whole training set does not need to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], np.float32)\n",
    "train_y = np.array([[0], [1], [1], [0]], np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the training set consists of several arrays (`train_x` and `train_y`), we need to shuffle each array correspondingly, otherwise the first item in a 'x' batch will not belong to the first item in the 'y' batch.\n",
    "The neatest way to do this is to take advantage of NumPy's array indexing features which allow you to pass a list of indexes to an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([10, 20, 30, 40, 50], np.int64)\n",
    "indexes = [0, 2, 3]\n",
    "print(array[indexes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'll do is shuffle an array of indexes and then use subarrays of the shuffled indexes to load batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training set x')\n",
    "print(train_x)\n",
    "print('training set y')\n",
    "print(train_y)\n",
    "print()\n",
    "\n",
    "row_indexes = np.arange(len(train_x))\n",
    "print('row_indexes')\n",
    "print(row_indexes)\n",
    "print()\n",
    "\n",
    "np.random.shuffle(row_indexes)\n",
    "print('shuffled row_indexes')\n",
    "print(row_indexes)\n",
    "print()\n",
    "\n",
    "batch_row_indexes = row_indexes[0:2]\n",
    "print('batch_row_indexes')\n",
    "print(batch_row_indexes)\n",
    "print()\n",
    "\n",
    "print('batch x')\n",
    "print(train_x[batch_row_indexes])\n",
    "print('batch y')\n",
    "print(train_y[batch_row_indexes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you go through all the different batch row indexes in an epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_indexes = np.arange(len(train_x))\n",
    "np.random.shuffle(row_indexes)\n",
    "\n",
    "batch_size = 2\n",
    "for i in range(0, len(train_x), batch_size):\n",
    "    print('i:', i)\n",
    "    batch_row_indexes = row_indexes[i:i+batch_size]\n",
    "    print('batch_row_indexes:', batch_row_indexes.tolist())\n",
    "    print('train_x:', train_x[batch_row_indexes].tolist())\n",
    "    print('train_y:', train_y[batch_row_indexes].tolist())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now apply a step of gradient descent on each batch extracted from the training set.\n",
    "\n",
    "Here is the full SGD training process in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], np.float32)\n",
    "train_y = np.array([[0], [1], [1], [0]], np.float32)\n",
    "\n",
    "class TwoLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(2, 2)\n",
    "        self.layer2 = torch.nn.Linear(2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = torch.sigmoid(self.layer1(x))\n",
    "        return self.layer2(hidden)\n",
    "\n",
    "model = TwoLayer()\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "print('epoch', 'step', 'error')\n",
    "batch_size = 2\n",
    "train_errors = []\n",
    "step = 0\n",
    "for epoch in range(1, 1000+1):\n",
    "    row_indexes = np.arange(len(train_x))\n",
    "    np.random.shuffle(row_indexes)\n",
    "\n",
    "    for i in range(0, len(train_x), batch_size):\n",
    "        batch_row_indexes = row_indexes[i:i+batch_size]\n",
    "        batch_x = torch.tensor(train_x[batch_row_indexes], dtype=torch.float32, device=device)\n",
    "        batch_y = torch.tensor(train_y[batch_row_indexes], dtype=torch.float32, device=device)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        logits = model(batch_x)\n",
    "        train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, batch_y)\n",
    "        train_errors.append(train_error.detach().cpu().tolist())\n",
    "        train_error.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        step += 1\n",
    "        if step%100 == 0:\n",
    "            print(epoch, step, train_errors[-1])\n",
    "print()\n",
    "\n",
    "# We can also use batching after training.\n",
    "print('x0', 'x1', 'y')\n",
    "batch_size = 2\n",
    "for i in range(0, len(train_x), batch_size):\n",
    "    batch_x = torch.tensor(train_x[i:i+batch_size], dtype=torch.float32, device=device)\n",
    "    output = torch.sigmoid(model(batch_x))[:, 0].detach().cpu().tolist()\n",
    "    for (x, y) in zip(batch_x.cpu().tolist(), output):\n",
    "        print(x[0], x[1], y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('step')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with batching is that you don't get a nice training error per epoch you can show, because that would require going through the training set again after every epoch to get the average error over the whole training set, which would be more time that it's worth.\n",
    "You can instead show the training error per step, but that would be more noisy.\n",
    "Note how you can also use batching when simply using the model to get predictions after training (you won't stop running out of memory just because you stopped training).\n",
    "\n",
    "So what should the batch size be?\n",
    "The GPU is like a dish washer.\n",
    "A dish washer only saves time compared to washing dishes by hand when it is loaded with many dishes.\n",
    "If you only have a single plate to wash, it would be a complete waste of time using the dish washer.\n",
    "It takes as long for a dish washer to wash one dish as it does to wash twenty, so the more dishes are loaded in the same wash, the more time saved.\n",
    "GPUs also only make sense to use when they are filled with data since it takes almost the same time to process one training set item as it does to process a hundred.\n",
    "Furthermore, the time it takes to move data between the GPU and main memory (CPU) and back is a significant fraction of the time it takes to process that data.\n",
    "Let's quantify these times.\n",
    "\n",
    "This is how the time it takes to load a batch into GPU varies for different batch sizes, compared to CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = {\n",
    "    'cpu': [],\n",
    "    'cuda': [],\n",
    "}\n",
    "batch_sizes = list(range(250, 1000+1, 250))\n",
    "for batch_size in batch_sizes:\n",
    "    for device_ in ['cpu', 'cuda']:\n",
    "        start_time = timeit.default_timer()\n",
    "        for _ in range(100):\n",
    "            x = torch.tensor(np.random.rand(batch_size, 128, 128), dtype=torch.float32, device=device_)\n",
    "        duration = timeit.default_timer() - start_time\n",
    "        durations[device_].append(duration)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_ylabel('duration (seconds)')\n",
    "ax.set_xlabel('batch size')\n",
    "for device_ in durations:\n",
    "    ax.plot(batch_sizes, durations[device_], marker='o', linestyle='-', label=device_)\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the GPU always takes longer to load a batch, which is why we should minimise the number of times we move data from CPU to GPU.\n",
    "\n",
    "This is how the time it takes to load a batch into GPU *and then process it* varies for different batch sizes, compared to CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = {\n",
    "    'cpu': [],\n",
    "    'cuda': [],\n",
    "}\n",
    "batch_sizes = list(range(250, 1000+1, 250))\n",
    "for batch_size in batch_sizes:\n",
    "    for device_ in ['cpu', 'cuda']:\n",
    "        start_time = timeit.default_timer()\n",
    "        for _ in range(100):\n",
    "            x = torch.tensor(np.random.rand(batch_size, 128, 128), dtype=torch.float32, device=device_)\n",
    "            for _ in range(10):\n",
    "                x = x@x\n",
    "        duration = timeit.default_timer() - start_time\n",
    "        durations[device_].append(duration)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_ylabel('duration (seconds)')\n",
    "ax.set_xlabel('batch size')\n",
    "for device_ in durations:\n",
    "    ax.plot(batch_sizes, durations[device_], marker='o', linestyle='-', label=device_)\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the larger the batch size, the faster the training will be on GPU compared to CPU; so we should use the largest batches possible in order to save time.\n",
    "\n",
    "But given what we said about avoiding local minima, would smaller batches result in better training since the error surface would change more often?\n",
    "[The answer is no](https://jmlr.csail.mit.edu/papers/v20/18-789.html).\n",
    "As long as there is some batching, larger batches do not hurt training progress, provided that the right hidden layer size and learning rate are used, which would be different for different batch sizes.\n",
    "So find the largest batch you can fit on your GPU and use that, since you have nothing to lose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "\n",
    "Sometimes the error surface is shaped like a valley with a very gentle slope as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(1, 1, subplot_kw={'projection': '3d'})\n",
    "ax.set_xlabel('$p_0$')\n",
    "ax.set_ylabel('$p_1$')\n",
    "ax.set_zlabel('$E$')\n",
    "\n",
    "f = lambda p0,p1:(p0-5)**2 + 20/(p1+1.0)\n",
    "(p0s, p1s) = np.mgrid[0:10.1:0.1, 0:10.1:0.1]\n",
    "es = f(p0s, p1s)\n",
    "ax.plot_surface(p0s, p1s, es, color='red', linewidth=0, antialiased=True)\n",
    "ax.view_init(30, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, gradient descent will likely end up moving very slowly down the gentle slope whilst jumping back and forth along the walls of the valley.\n",
    "This is a problem, because training progress will be very slow and increasing the learning rate will make the parameters shoot out of the valley due to the steep sides.\n",
    "There is a simple solution to this: **momentum**.\n",
    "\n",
    "The word momentum is a reference to an object 'picking up momentum' as it rolls down a hill faster and faster, making it harder to slow down or change direction.\n",
    "In gradient descent we can do something like that by adding a fraction of the previous update to the new update.\n",
    "Consider the gradient descent update equation:\n",
    "\n",
    "$$p_{t+1} = p_t - \\alpha \\nabla_{p_t}$$\n",
    "\n",
    "where $p_t$ is the parameter to update, $p_{t+1}$ is the updated parameter, $\\alpha$ is the learning rate, and $\\nabla_{p_t}$ is the gradient with respect to the parameter.\n",
    "We can think of $\\nabla_{p_t}$ as the velocity with which we are moving along the parameter's direction.\n",
    "We shall now modify it so that a fraction of the previous velocity is added to the current velocity:\n",
    "\n",
    "$$v_{t+1} = \\mu v_t + \\nabla_{p_t}$$\n",
    "$$p_{t+1} = p_t - \\alpha v_{t+1}$$\n",
    "\n",
    "where $\\mu$ is the fraction of the previous velocity to apply to the update step, called the momentum.\n",
    "The bigger $\\mu$ is, the less gradient descent will be affected by the current gradient and the more it will be affected by the total of the previous gradients during training.\n",
    "In the case of the above valley situation, this will make the extent of jumping back and forth across the walls of the valley become smaller and smaller as the gradient in that direction will constantly oscillate from positive to negative, making the total of the gradients in that direction approach zero.\n",
    "It will also accelerate the movement along the gentle slope as the total gradient is constantly increasing.\n",
    "\n",
    "Note that normal gradient descent is just momentum with $\\mu = 0$.\n",
    "\n",
    "To use it in PyTorch is very easy, just add a `momentum` parameter to `SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32, device=device)\n",
    "train_y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32, device=device)\n",
    "\n",
    "class TwoLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(2, 2)\n",
    "        self.layer2 = torch.nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.sigmoid(self.layer1(x))\n",
    "        return self.layer2(hidden)\n",
    "\n",
    "model = TwoLayer()\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1.0, momentum=0.9)\n",
    "\n",
    "print('step', 'error')\n",
    "train_errors = []\n",
    "for step in range(1, 1000+1):\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if step%100 == 0:\n",
    "        print(step, train_errors[-1])\n",
    "print()\n",
    "\n",
    "print('x0', 'x1', 'y')\n",
    "output = torch.sigmoid(model(train_x))[:, 0].detach().cpu().tolist()\n",
    "for (x, y) in zip(train_x.cpu().tolist(), output):\n",
    "    print(x[0], x[1], y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('step')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also other more advanced gradient descent optimisers, such as Adam, which adapts the learning rate during training based on the previous gradients measured.\n",
    "It also adapts a different learning rate for each parameter rather than use one learning rate for all parameters, which is useful.\n",
    "\n",
    "To use the Adam optimiser in PyTorch, use the following:\n",
    "\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstable gradients\n",
    "\n",
    "Adding more layers often results in a problem called **the unstable gradient problem**.\n",
    "This is when the gradients of layers closer to the input will get either smaller and smaller to the point of becoming neglible, known as **the vanishing gradients problem**, or else it becomes larger and larger to the point of creating invalid values like `inf` and `nan`, known as **the exploding gradients problem**.\n",
    "The first is much more common.\n",
    "This happens because the gradient at a particular layer is the product of the activations of all the layers in front of it.\n",
    "\n",
    "One common focus of research in this problem has to do with the use of sigmoid as an activation function, whose gradient can be very small (or very large, but this is less common).\n",
    "Below, we're taking a bunch of randomly set two-layer networks and calculating the average magnitude of the gradients with respect to the weights of each layer.\n",
    "We then plot the gradients to see how the gradients of the hidden layer are different from the graidents of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32, device=device)\n",
    "train_y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32, device=device)\n",
    "\n",
    "class TwoLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = torch.nn.Parameter(20*(2*torch.rand((2, 2), dtype=torch.float32) - 1))\n",
    "        self.b1 = torch.nn.Parameter(20*(2*torch.rand((2,), dtype=torch.float32) - 1))\n",
    "        self.w2 = torch.nn.Parameter(20*(2*torch.rand((2, 1), dtype=torch.float32) - 1))\n",
    "        self.b2 = torch.nn.Parameter(20*(2*torch.rand((1,), dtype=torch.float32) - 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.sigmoid(x@self.w1 + self.b1)\n",
    "        return hidden@self.w2 + self.b2\n",
    "\n",
    "w1_grads = []\n",
    "w2_grads = []\n",
    "for _ in range(100):\n",
    "    model = TwoLayer()\n",
    "    model.to(device)\n",
    "\n",
    "    logits = model(train_x)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_error.backward()\n",
    "\n",
    "    w1_grads.append(model.w1.grad.abs().mean().cpu().tolist())\n",
    "    w2_grads.append(model.w2.grad.abs().mean().cpu().tolist())\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.grad.zero_()\n",
    "\n",
    "(fig, axs) = plt.subplots(1, 2)\n",
    "axs[0].set_title('gradients of $w_1$')\n",
    "axs[0].bar(np.arange(len(w1_grads)), w1_grads)\n",
    "axs[0].grid()\n",
    "axs[1].set_title('gradients of $w_2$')\n",
    "axs[1].bar(np.arange(len(w2_grads)), w2_grads)\n",
    "axs[1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the majority of the gradients of $w_1$, the weights of the hidden layer, are tend to be much smaller than those of $w_2$, the weights of the output layer.\n",
    "The more layers you add, the smaller the gradients of the early layers get, which makes training them by gradient descent difficult or even impossible (their weights just don't change during training).\n",
    "\n",
    "Using a hidden layer activation function that allows for larger gradients will reduce this problem, although it won't eliminate it.\n",
    "A popular activation function for this purpose is the **rectified linear unit** (**ReLU**), which is defined as\n",
    "\n",
    "$$y = \\max(0, x)$$\n",
    "\n",
    "that is, leave the input as-is when it is positive and replace it with zero when it is negative.\n",
    "In PyTorch, this is defined as:\n",
    "\n",
    "    torch.nn.functional.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "xs = np.mgrid[-10:10.1:0.01]\n",
    "ys = np.maximum(0, xs)\n",
    "ax.plot(xs, ys, color='red', marker='', linestyle='-', linewidth=1)\n",
    "ax.text(-5, 6, '$y = \\max(0, x)$', color='red', fontdict={'fontsize': 12})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the gradients of the two layers vary now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32, device=device)\n",
    "train_y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32, device=device)\n",
    "\n",
    "class TwoLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = torch.nn.Parameter(20*(2*torch.rand((2, 2), dtype=torch.float32) - 1))\n",
    "        self.b1 = torch.nn.Parameter(20*(2*torch.rand((2,), dtype=torch.float32) - 1))\n",
    "        self.w2 = torch.nn.Parameter(20*(2*torch.rand((2, 1), dtype=torch.float32) - 1))\n",
    "        self.b2 = torch.nn.Parameter(20*(2*torch.rand((1,), dtype=torch.float32) - 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.nn.functional.relu(x@self.w1 + self.b1)\n",
    "        return hidden@self.w2 + self.b2\n",
    "\n",
    "w1_grads = []\n",
    "w2_grads = []\n",
    "for _ in range(100):\n",
    "    model = TwoLayer()\n",
    "    model.to(device)\n",
    "\n",
    "    logits = model(train_x)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_error.backward()\n",
    "\n",
    "    w1_grads.append(model.w1.grad.abs().mean().cpu().tolist())\n",
    "    w2_grads.append(model.w2.grad.abs().mean().cpu().tolist())\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.grad.zero_()\n",
    "\n",
    "(fig, axs) = plt.subplots(1, 2)\n",
    "axs[0].set_title('gradients of $w_1$')\n",
    "axs[0].bar(np.arange(len(w1_grads)), w1_grads)\n",
    "axs[0].grid()\n",
    "axs[1].set_title('gradients of $w_2$')\n",
    "axs[1].bar(np.arange(len(w2_grads)), w2_grads)\n",
    "axs[1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are look more balanced than with sigmoid (although the scale is still different).\n",
    "\n",
    "However, ReLU has a problem known as **the dead ReLU problem**, which is when a lot of your neural units end up always outputting zero, regardless of what the neural network's input is.\n",
    "To prevent this, the **Leaky ReLU** is sometimes used instead which replaces the zero with a fraction of the input such as\n",
    "\n",
    "$$y = \\max\\left(\\frac{1}{10}x, x\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "xs = np.mgrid[-10:10.1:0.01]\n",
    "ys = np.maximum(0.1*xs, xs)\n",
    "ax.plot(xs, ys, color='red', marker='', linestyle='-', linewidth=1)\n",
    "ax.text(-5, 6, '$y = \\max\\\\left(\\\\frac{1}{10}x, x\\\\right)$', color='red', fontdict={'fontsize': 12})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which in PyTorch is defined as\n",
    "\n",
    "    torch.nn.functional.leaky_relu(x, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32, device=device)\n",
    "train_y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32, device=device)\n",
    "\n",
    "class TwoLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(2, 2)\n",
    "        self.layer2 = torch.nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.nn.functional.leaky_relu(self.layer1(x), 0.1)\n",
    "        return self.layer2(hidden) # Sigmoid can still be used for the output (although we only return the logits here).\n",
    "\n",
    "model = TwoLayer()\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "print('step', 'error')\n",
    "train_errors = []\n",
    "for step in range(1, 1000+1):\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if step%100 == 0:\n",
    "        print(step, train_errors[-1])\n",
    "print()\n",
    "\n",
    "print('x0', 'x1', 'y')\n",
    "output = torch.sigmoid(model(train_x))[:, 0].detach().cpu().tolist()\n",
    "for (x, y) in zip(train_x.cpu().tolist(), output):\n",
    "    print(x[0], x[1], y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('step')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output layer can use whatever activation function your want as it will affect all layers equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding overfitting\n",
    "\n",
    "When a model makes very good predictions on the training set but bad predictions on other data, we say that the model has overfitted to the training set, which is bad.\n",
    "Ideally, the model learns something that generalises to other data.\n",
    "\n",
    "Let's see some ways to reduce this effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "\n",
    "The easiest way to avoid overfitting is by using **early stopping**.\n",
    "Keep a small part of the training set separate, called a **validation set**, and don't use it for training.\n",
    "Instead, use it to measure performance after every epoch (such as for measuring the model's accuracy).\n",
    "This will tell you whether the model is just learning the training set by heart or if it's generalising to data outside of the training set.\n",
    "By monitoring the performance on the validation set, you can stop training as soon as the performance starts degrading, hence why it's called early stopping.\n",
    "\n",
    "Rather than stopping exactly on the epoch where performance dips, a number of 'bad' epochs should be allowed in order to allow for some fluctuations.\n",
    "The model parameters are saved every time the validation set performance improves so that the best performing parameters can be loaded at the end.\n",
    "The maximum number of less-than-best epochs to allow is called the **patience**.\n",
    "\n",
    "Performance tip: Avoid having PyTorch preparing to compute gradients when not necessary (such as when using the model outside of training) by putting your module calls in a `torch.no_grad()` context:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = torch.sigmoid(model(train_x))\n",
    "\n",
    "Below is an example of how to use early stopping.\n",
    "Since we're using the XOR task, it doesn't make sense to split the 4 row training set into a smaller training set and a validation set.\n",
    "Instead, we'll be using inputs that are similar to those in the training set as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32, device=device)\n",
    "train_y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32, device=device)\n",
    "\n",
    "val_x = torch.tensor([[-0.1, -0.1], [-0.1, 1.1], [1.1, -0.1], [1.1, 1.1]], dtype=torch.float32, device=device)\n",
    "val_y = np.array([[0], [1], [1], [0]], np.float32)\n",
    "\n",
    "class TwoLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(2, 2)\n",
    "        self.layer2 = torch.nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.sigmoid(self.layer1(x))\n",
    "        return self.layer2(hidden)\n",
    "\n",
    "model = TwoLayer()\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "best_val_acc = 0.0\n",
    "patience = 500\n",
    "num_bad_epochs = 0\n",
    "\n",
    "print('step', 'error', 'val_acc')\n",
    "train_errors = []\n",
    "accs = []\n",
    "for step in range(1, 10000+1):\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = torch.sigmoid(model(val_x))[:, 0].round(decimals=0).cpu().numpy()\n",
    "    val_acc = sklearn.metrics.accuracy_score(output, val_y)\n",
    "    accs.append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        torch.save(model, 'model.pkl')\n",
    "        best_val_acc = val_acc\n",
    "        num_bad_epochs = 0\n",
    "    else:\n",
    "        num_bad_epochs += 1\n",
    "        if num_bad_epochs == patience:\n",
    "            break\n",
    "\n",
    "    if step%100 == 0:\n",
    "        print(step, train_errors[-1], val_acc)\n",
    "print()\n",
    "\n",
    "# Load the best model found (the one that was saved last) after training.\n",
    "best_model = torch.load('model.pkl')\n",
    "print('ended on step:', step)\n",
    "print('best val acc: {:.0%}'.format(best_val_acc))\n",
    "print()\n",
    "\n",
    "print('x0', 'x1', 'y')\n",
    "with torch.no_grad():\n",
    "    output = torch.sigmoid(model(train_x))[:, 0].detach().cpu().tolist()\n",
    "for (x, y) in zip(train_x.cpu().tolist(), output):\n",
    "    print(x[0], x[1], y)\n",
    "\n",
    "(fig, axs) = plt.subplots(1, 2)\n",
    "axs[0].set_xlabel('step')\n",
    "axs[0].set_ylabel('E')\n",
    "axs[0].plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "axs[0].grid()\n",
    "axs[1].set_xlabel('step')\n",
    "axs[1].set_ylabel('acc')\n",
    "axs[1].plot(range(1, len(accs) + 1), accs, color='red', linestyle='-', linewidth=3)\n",
    "axs[1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Anything that makes training more difficult with the intention of improving generalisation is called **regularisation**.\n",
    "Dropout is a form of neural network regularisation that consists of zeroing out a random set of hidden layer neural units, that is, dropping them out.\n",
    "The way it works is that the activations of a hidden layer are multiplied by an array of ones and zeros, called a **mask**, and the result is what is sent to the next layer.\n",
    "The zeroes in the mask array are filled randomly using some probability called the **dropout rate**, and each separate row in the training set is given a new generated mask which is then regenerated after each epoch.\n",
    "\n",
    "Why does this act as regularisation?\n",
    "The hidden layer tends to develop co-adapted groups of neural units that work together to perform a single function as a team.\n",
    "When these groups become large, there will also be less of them because the number of neural units in a layer is fixed.\n",
    "A few complex functions are more likely to overfit than many independent simple functions because complex functions are more likely to latch on to a single accidentally reliable feature which is not reliable outside of the training set.\n",
    "Having many simple functions, on the other hand, makes it more likely that some of them will learn something that will also be found outside of the training set.\n",
    "The fact that a neural unit cannot depend on a group of neural units being present in the previous layer prevents it from forming large co-adapted groups, leaving only small groups being formed.\n",
    "\n",
    "![](dropout.png)\n",
    "\n",
    "This is how you make a mask with a small number of zeros in it in order to multiply it to the hidden layer's activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = torch.rand(3, 4, dtype=torch.float32, device=device) # Make a random hidden layer activation tensor.\n",
    "print('original hidden activations:')\n",
    "print(hidden)\n",
    "print()\n",
    "\n",
    "mask = torch.rand_like(hidden) # Make a tensor with random numbers between 0 and 1 of the same shape as 'hidden'.\n",
    "print('random numbers for mask:')\n",
    "print(mask)\n",
    "print()\n",
    "\n",
    "mask = mask > 0.2 # Replace the numbers with True and False depending on whether they are greater than the dropout rate. Since the random numbers are uniformly distributed, the chance of them being less than the dropout rate is equal to the dropout rate.\n",
    "print('binarised mask with dropout rate 0.2:')\n",
    "print(mask)\n",
    "print()\n",
    "\n",
    "hidden = hidden*mask\n",
    "print('new hidden activations after dropout:')\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you don't want to use dropout when the model is being used outside of training as that would result in random outputs.\n",
    "On the other hand, you can't just remove the mask array and carry on because a neural unit would be trained to expect only a few of the previous layer's neural units being non-zero.\n",
    "Removing the mask would mean that the neural units are suddenly being fed the activations of the entire previous layer, which will overload the neural unit that was trained on less inputs.\n",
    "So if the dropout rate was 0.5 (half of the activations were being replaced by zeros on average), what we do is make them double the value of the activations so that the next layer's neural units learn to work with twice the activation total they see during training.\n",
    "In this way, removing the mask (and the doubling of activations) will make the neural units receive the same total activations they were receiving during training (on average).\n",
    "In general, for a dropout rate of $p$, the activations of a layer with dropout during training should be multiplied by $\\frac{1}{1 - p}$.\n",
    "\n",
    "We will also need larger hidden layers to compensate for the dropped out neural units and a smaller learning rate due to only a fraction of the units being available during training.\n",
    "\n",
    "To know whether to use dropout or not, depending on whether it is being trained or not, PyTorch modules provide the attribute `self.training` which is set to true when `model.train()` is called and to false when `model.eval()` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32, device=device)\n",
    "train_y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32, device=device)\n",
    "\n",
    "class TwoLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.layer1 = torch.nn.Linear(2, 4) # A larger hidden layer.\n",
    "        self.layer2 = torch.nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.sigmoid(self.layer1(x))\n",
    "        if self.training: # Only apply while training.\n",
    "            mask = torch.rand_like(hidden) # Make a tensor with random numbers between 0 and 1 of the same shape as 'hidden'.\n",
    "            mask = mask > self.dropout_rate\n",
    "            hidden = hidden*mask\n",
    "            hidden = hidden*(1/(1 - self.dropout_rate))\n",
    "        return self.layer2(hidden)\n",
    "\n",
    "model = TwoLayer(dropout_rate=0.5)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "print('step', 'error')\n",
    "train_errors = []\n",
    "for step in range(1, 5000+1):\n",
    "    model.train() # Set training mode to use dropout.\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if step%500 == 0:\n",
    "        print(step, train_errors[-1])\n",
    "print()\n",
    "\n",
    "print('x0', 'x1', 'y')\n",
    "model.eval() # End training mode to stop using dropout.\n",
    "with torch.no_grad():\n",
    "    output = torch.sigmoid(model(train_x))[:, 0].detach().cpu().tolist()\n",
    "    for (x, y) in zip(train_x.cpu().tolist(), output):\n",
    "        print(x[0], x[1], y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('step')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides a `torch.nn.functional.dropout` function which performs dropout for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32, device=device)\n",
    "train_y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32, device=device)\n",
    "\n",
    "class TwoLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.layer1 = torch.nn.Linear(2, 4)\n",
    "        self.layer2 = torch.nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.sigmoid(self.layer1(x))\n",
    "        if self.training:\n",
    "            hidden = torch.nn.functional.dropout(hidden, self.dropout_rate)\n",
    "        return self.layer2(hidden)\n",
    "\n",
    "model = TwoLayer(dropout_rate=0.5)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "print('step', 'error')\n",
    "train_errors = []\n",
    "for step in range(1, 5000+1):\n",
    "    model.train()\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if step%500 == 0:\n",
    "        print(step, train_errors[-1])\n",
    "print()\n",
    "\n",
    "print('x0', 'x1', 'y')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = torch.sigmoid(model(train_x))[:, 0].detach().cpu().tolist()\n",
    "    for (x, y) in zip(train_x.cpu().tolist(), output):\n",
    "        print(x[0], x[1], y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('step')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "We've seen many ways to improve the performance of a neural network, but many of these ways introduce numbers that need to be set before training: the learning rate, the hidden layer size, the momentum, the leaky ReLU factor, the dropout rate, and so on.\n",
    "Each one of these numbers that are not taken care of by the optimiser during training are called **hyperparameters**, and they are one of the biggest headaches in deep learning.\n",
    "This is because there is no simple way to find them, other than by trial and error.\n",
    "You try a set of hyperparameter values, train the neural network with them, see how it performs, and try another set of values until you get satisfactory performance.\n",
    "This wouldn't be so bad were it not for the fact that big neural networks take long to train and evaluate.\n",
    "\n",
    "The search can be automated of course, but it will take the vast majority of the time of developing a neural network (provided that you're not making a data set for your neural network, which would take much longer).\n",
    "We need to be smart with what hyperparameter values to try, in order to reduce our **search space** and thus reduce the search time.\n",
    "To evaluate a neural network model, we should use a separate data set called a **development set** (**dev set**).\n",
    "This would be different from both the validation set that is used for early stopping and the **test set** that is used to to evaluate the final model after hyperparameter tuning.\n",
    "The test set should be a representation of the inputs received when you use the model in the real world, which would not be the same ones used during hyperparameter tuning or early stopping.\n",
    "\n",
    "A simple and effective way to search is to use a **random search**.\n",
    "This is when the hyperparameters values to use are sampled randomly from the search space for a set number of times.\n",
    "Be sure not to sample the same combination of hyperparameter values more than once.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32, device=device)\n",
    "train_y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32, device=device)\n",
    "\n",
    "dev_x = torch.tensor([[-0.1, -0.1], [-0.1, 1.1], [1.1, -0.1], [1.1, 1.1]], dtype=torch.float32, device=device)\n",
    "dev_y = np.array([[0], [1], [1], [0]], np.float32)\n",
    "\n",
    "class TwoLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_layer_size, leaky_relu_factor):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(2, hidden_layer_size)\n",
    "        self.layer2 = torch.nn.Linear(hidden_layer_size, 1)\n",
    "        self.leaky_relu_factor = leaky_relu_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.nn.functional.leaky_relu(self.layer1(x), self.leaky_relu_factor)\n",
    "        return self.layer2(hidden)\n",
    "\n",
    "# The search space consisting of 3*3*3 = 27 possible hyperparameter value combinations.\n",
    "hidden_layer_size_set = [1, 2, 4]\n",
    "leaky_relu_factor_set = [0.1, 0.01, 0.001]\n",
    "learning_rate_set = [10.0, 1.0, 0.1]\n",
    "\n",
    "already_generated = set()\n",
    "best_dev_acc = 0.0\n",
    "best_hyperparams = None\n",
    "for i in range(1, 10+1): # There are 27 combinations but we're only going to try a random 10 of them.\n",
    "    while True: # Repeatedly generate random combinations until a new combination is produced.\n",
    "        hidden_layer_size = random.choice(hidden_layer_size_set)\n",
    "        leaky_relu_factor = random.choice(leaky_relu_factor_set)\n",
    "        learning_rate = random.choice(learning_rate_set)\n",
    "        hyperparams = (hidden_layer_size, leaky_relu_factor, learning_rate)\n",
    "        if hyperparams not in already_generated:\n",
    "            already_generated.add(hyperparams)\n",
    "            break\n",
    "    print('Hyperparameter search attempt:', i)\n",
    "    print('hidden_layer_size:', hidden_layer_size)\n",
    "    print('leaky_relu_factor:', leaky_relu_factor)\n",
    "    print('learning_rate:', learning_rate)\n",
    "    \n",
    "    model = TwoLayer(hidden_layer_size, leaky_relu_factor)\n",
    "    model.to(device)\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for step in range(1, 1000+1):\n",
    "        optimiser.zero_grad()\n",
    "        logits = model(train_x)\n",
    "        train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "        train_error.backward()\n",
    "        optimiser.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = torch.sigmoid(model(dev_x))[:, 0].round(decimals=0).detach().cpu().numpy()\n",
    "        dev_acc = sklearn.metrics.accuracy_score(output, dev_y)\n",
    "    print('Dev set accuracy:', dev_acc)\n",
    "    if dev_acc > best_dev_acc:\n",
    "        best_hyperparams = hyperparams\n",
    "        best_dev_acc = dev_acc\n",
    "        torch.save(model, 'best_model.pkl')\n",
    "        print('new best!')\n",
    "    print()\n",
    "\n",
    "(hidden_layer_size, leaky_relu_factor, learning_rate) = best_hyperparams\n",
    "print('Best found:')\n",
    "print('hidden_layer_size:', hidden_layer_size)\n",
    "print('leaky_relu_factor:', leaky_relu_factor)\n",
    "print('learning_rate:', learning_rate)\n",
    "print('Dev set accuracy:', best_dev_acc)\n",
    "\n",
    "best_model = torch.load('best_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Full extras\n",
    "\n",
    "Repeat last topic's example (the movie reviews classification task) but this time try to maximise the test set performance by using all the techniques shown in this topic.\n",
    "All the data preprocessing and validation set extraction has been done for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data_set/sentiment/train.csv')\n",
    "dev_df = pd.read_csv('../data_set/sentiment/dev.csv')\n",
    "test_df = pd.read_csv('../data_set/sentiment/test.csv')\n",
    "\n",
    "train_x = train_df['text']\n",
    "train_y = train_df['class']\n",
    "(train_x, val_x, train_y, val_y) = sklearn.model_selection.train_test_split(train_x, train_y, test_size=0.1, random_state=0)\n",
    "dev_x = dev_df['text']\n",
    "dev_y = dev_df['class']\n",
    "test_x = test_df['text']\n",
    "test_y = test_df['class']\n",
    "categories = ['neg', 'pos'] # neg -> 0, pos -> 1\n",
    "cat2idx = {cat: i for (i, cat) in enumerate(categories)}\n",
    "\n",
    "train_y_idx = train_y.map(cat2idx.get).to_numpy()\n",
    "val_y_idx = val_y.map(cat2idx.get).to_numpy()\n",
    "dev_y_idx = dev_y.map(cat2idx.get).to_numpy()\n",
    "test_y_idx = test_y.map(cat2idx.get).to_numpy()\n",
    "\n",
    "nltk.download('punkt')\n",
    "encoder = sklearn.feature_extraction.text.CountVectorizer(max_features=1000, binary=True, stop_words='english', lowercase=True, tokenizer=nltk.word_tokenize, dtype=np.float32)\n",
    "encoder.fit(train_x)\n",
    "\n",
    "vocabulary = sorted(encoder.vocabulary_.keys(), key=encoder.vocabulary_.get)\n",
    "\n",
    "train_x_bow = encoder.transform(train_x).toarray()\n",
    "val_x_bow = encoder.transform(val_x).toarray()\n",
    "dev_x_bow = encoder.transform(dev_x).toarray()\n",
    "test_x_bow = encoder.transform(test_x).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75ee2b71ad44bf9ef4e9bee896f68ffbc764a6a2c6d1f57c86c48f99ffc25ca8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
