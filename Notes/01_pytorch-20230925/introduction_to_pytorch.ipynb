{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "[PyTorch](https://pytorch.org/) is an opensource Python library based on the Lua library [Torch](http://torch.ch/).\n",
    "It is a very popular numeric library that is designed for deep learning and is developed by [Facebook's AI Research lab](https://ai.facebook.com/research/) (FAIR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The graphics processing unit (GPU)\n",
    "\n",
    "Deep learning can be very intensive on your computer and it could take days to finish processing using only your computer's central processing unit (CPU).\n",
    "To get around this, we usually make use of the graphics processing unit (GPU).\n",
    "A **GPU** is a processor found on your graphics card which can perform many numeric computations in parallel, enabling us to drastically reduce our deep learning processing time.\n",
    "\n",
    "Unfortunately, only NVIDIA's graphic cards can be used to execute code.\n",
    "To be able to execute instructions on the GPU, NVIDIA provides a driver called the **CUDA framework**.\n",
    "You normally use the CUDA driver through C++ libraries, but PyTorch hides that away behind Python code for you.\n",
    "If you have an NVIDIA graphic card, download the CUDA driver for your operating system from [here](https://developer.nvidia.com/cuda-zone).\n",
    "You can check which CUDA version you have by running this command in your command prompt / terminal:\n",
    "\n",
    "    nvcc --version\n",
    "\n",
    "Once you've installed CUDA or determined that you won't be using it, install PyTorch by following the instructions [here](https://pytorch.org/get-started/locally/).\n",
    "Choose the CPU option if you don't have an NVIDIA graphic card.\n",
    "If you have an older graphic card, [older versions of PyTorch](https://pytorch.org/get-started/previous-versions/) support up to CUDA 7.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and NumPy\n",
    "\n",
    "One of the most important concepts to understand in deep learning is the notion of a **tensor**.\n",
    "A tensor is a regularly shaped array of numbers of any number of dimensions.\n",
    "The simplest tensor is the **scalar**, which is just a single number (zero dimensions).\n",
    "A 1D array of numbers is called a **vector** and a 2D grid of numbers is called a **matrix**.\n",
    "All of these are different types of tensors, which can keep on growing in dimensionality (3D, 4D, and so on).\n",
    "The challenge in deep learning is finding a way to represent your inputs and outputs using tensors, such as a sentence being represented as a vector where each number is a different word, or a photo being represented as a matrix where each number is a different pixel.\n",
    "We'll see different ways of representing text as tensors in later topics.\n",
    "\n",
    "NumPy is a very popular numeric library for working with tensors and it is used internally by PyTorch.\n",
    "Let's take a quick look at NumPy before getting into PyTorch.\n",
    "In NumPy, the `np.array` function creates a tensor from nested lists (vectors are a single list, matrices are lists of lists, and so on) and a data type such as `int64` (long integer), `float32` (single precision float), or `bool_` (Boolean) to specify the data type of the numbers.\n",
    "Note that a lot of stuff in PyTorch expects `int64` integers, so we might as well use those all the time.\n",
    "You can get the number of dimensions in a NumPy array using the `ndim` attribute and you can see the size of each dimension using the `shape` attribute (a matrix with 2 rows and 3 columns has a shape of `(2, 3)`).\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar\n",
      "1\n",
      "ndim: 0\n",
      "shape: ()\n",
      "\n",
      "vector\n",
      "[1 2 3 4]\n",
      "ndim: 1\n",
      "shape: (4,)\n",
      "\n",
      "matrix\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "ndim: 2\n",
      "shape: (3, 4)\n",
      "\n",
      "3D tensor\n",
      "[[[ 1  2  3  4]\n",
      "  [ 5  6  7  8]\n",
      "  [ 9 10 11 12]]\n",
      "\n",
      " [[13 14 15 16]\n",
      "  [17 18 19 20]\n",
      "  [21 22 23 24]]]\n",
      "ndim: 3\n",
      "shape: (2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "print('scalar')\n",
    "scalar = np.array(1, dtype=np.int64)\n",
    "print(scalar)\n",
    "print('ndim:', scalar.ndim)\n",
    "print('shape:', scalar.shape)\n",
    "print()\n",
    "\n",
    "print('vector')\n",
    "vector = np.array([1, 2, 3, 4], dtype=np.int64)\n",
    "print(vector)\n",
    "print('ndim:', vector.ndim)\n",
    "print('shape:', vector.shape)\n",
    "print()\n",
    "\n",
    "print('matrix')\n",
    "matrix = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], dtype=np.int64)\n",
    "print(matrix)\n",
    "print('ndim:', matrix.ndim)\n",
    "print('shape:', matrix.shape)\n",
    "print()\n",
    "\n",
    "print('3D tensor')\n",
    "tensor3d = np.array([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]], dtype=np.int64)\n",
    "print(tensor3d)\n",
    "print('ndim:', tensor3d.ndim)\n",
    "print('shape:', tensor3d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With NumPy you can very easily add all the numbers in a vector by 1 or double them, or even add/multiply two vectors together (as long as they have the same shape) without needing to use a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector\n",
      "[1 2 3 4]\n",
      "\n",
      "vector + 1\n",
      "[2 3 4 5]\n",
      "\n",
      "vector * 2\n",
      "[2 4 6 8]\n",
      "\n",
      "vector + vector\n",
      "[2 4 6 8]\n",
      "\n",
      "vector * vector\n",
      "[ 1  4  9 16]\n"
     ]
    }
   ],
   "source": [
    "vector = np.array([1, 2, 3, 4], np.int64)\n",
    "print('vector')\n",
    "print(vector)\n",
    "print()\n",
    "\n",
    "print('vector + 1')\n",
    "print(vector + 1)\n",
    "print()\n",
    "\n",
    "print('vector * 2')\n",
    "print(vector * 2)\n",
    "print()\n",
    "\n",
    "print('vector + vector')\n",
    "print(vector + vector)\n",
    "print()\n",
    "\n",
    "print('vector * vector')\n",
    "print(vector * vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how you do a matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix1\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "\n",
      "matrix2\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "matrix1@matrix2\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "matrix1 = np.array([[1, 2], [3, 4]], np.float32)\n",
    "matrix2 = np.array([[1, 0], [0, 1]], np.float32)\n",
    "print('matrix1')\n",
    "print(matrix1)\n",
    "print()\n",
    "print('matrix2')\n",
    "print(matrix2)\n",
    "print()\n",
    "print('matrix1@matrix2')\n",
    "print(matrix1@matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you cannot multiply a vector to a matrix, only a matrix to a matrix.\n",
    "If you need to multiply a vector to a matrix you'd need to make the vector be a single row matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix1\n",
      "[[1. 2.]]\n",
      "\n",
      "matrix2\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "matrix1@matrix2\n",
      "[[1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "matrix1 = np.array([[1, 2]], np.float32)\n",
    "matrix2 = np.array([[1, 0], [0, 1]], np.float32)\n",
    "print('matrix1')\n",
    "print(matrix1)\n",
    "print()\n",
    "print('matrix2')\n",
    "print(matrix2)\n",
    "print()\n",
    "print('matrix1@matrix2')\n",
    "print(matrix1@matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also manipulate the dimensions of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the matrix:\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "\n",
      "second element of first row:\n",
      "2\n",
      "\n",
      "all elements of first row:\n",
      "[1 2 3 4]\n",
      "\n",
      "all elements of first column:\n",
      "[1 5 9]\n",
      "\n",
      "the matrix with an extra dimension at the end:\n",
      "[[[ 1]\n",
      "  [ 2]\n",
      "  [ 3]\n",
      "  [ 4]]\n",
      "\n",
      " [[ 5]\n",
      "  [ 6]\n",
      "  [ 7]\n",
      "  [ 8]]\n",
      "\n",
      " [[ 9]\n",
      "  [10]\n",
      "  [11]\n",
      "  [12]]]\n",
      "\n",
      "the matrix with an extra dimension at the front:\n",
      "[[[ 1  2  3  4]\n",
      "  [ 5  6  7  8]\n",
      "  [ 9 10 11 12]]]\n",
      "\n",
      "first row but as a single row matrix (add a dimension of size 1 before row data):\n",
      "[[[ 1  2  3  4]]\n",
      "\n",
      " [[ 5  6  7  8]]\n",
      "\n",
      " [[ 9 10 11 12]]]\n",
      "\n",
      "first row but as a single column matrix (add a dimension of size 1 after row data):\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "\n",
      "reshape 3x4 matrix into 4x3:\n",
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "\n",
      "reshape 3x4 matrix into 3x2x2:\n",
      "[[[ 1  2]\n",
      "  [ 3  4]]\n",
      "\n",
      " [[ 5  6]\n",
      "  [ 7  8]]\n",
      "\n",
      " [[ 9 10]\n",
      "  [11 12]]]\n",
      "\n",
      "reshape 3x4 matrix into 12:\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12]\n"
     ]
    }
   ],
   "source": [
    "matrix = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], np.int64)\n",
    "print('the matrix:')\n",
    "print(matrix)\n",
    "print()\n",
    "\n",
    "print('second element of first row:')\n",
    "print(matrix[0, 1])\n",
    "print()\n",
    "\n",
    "print('all elements of first row:')\n",
    "print(matrix[0, :])\n",
    "print()\n",
    "\n",
    "print('all elements of first column:')\n",
    "print(matrix[:, 0])\n",
    "print()\n",
    "\n",
    "print('the matrix with an extra dimension at the end:')\n",
    "print(matrix[:, :, None])\n",
    "print()\n",
    "\n",
    "print('the matrix with an extra dimension at the front:')\n",
    "print(matrix[None, :, :])\n",
    "print()\n",
    "\n",
    "print('first row but as a single row matrix (add a dimension of size 1 before row data):')\n",
    "print(matrix[:, None, :])\n",
    "print()\n",
    "\n",
    "print('first row but as a single column matrix (add a dimension of size 1 after row data):')\n",
    "print(matrix[0, :, None])\n",
    "print()\n",
    "\n",
    "print('reshape 3x4 matrix into 4x3:')\n",
    "print(matrix.reshape([4, 3]))\n",
    "print()\n",
    "\n",
    "print('reshape 3x4 matrix into 3x2x2:')\n",
    "print(matrix.reshape([3, 2, 2]))\n",
    "print()\n",
    "\n",
    "print('reshape 3x4 matrix into 12:')\n",
    "print(matrix.reshape([12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reshaping a tensor, it's important that the number of elements in the tensor do not change.\n",
    "So we can reshape a 3 by 4 tensor into a 3 by 2 by 2 tensor (3×4 = 3×2×2) but not into a 3 by 3 tensor.\n",
    "\n",
    "A dimension of size 1 is called a **singleton dimension**.\n",
    "\n",
    "You can even join tensors together, either by **concatenating** or **stacking**.\n",
    "When you concatenate two matrices, you either join them side-by-side or on top of each other, with the result being a new larger matrix.\n",
    "When you stack two vectors together you create a matrix with the vectors being either the columns or the rows of the new matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix1\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "\n",
      "matrix2\n",
      "[[5. 6.]\n",
      " [7. 8.]]\n",
      "\n",
      "concatenated at dimension 0:\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]\n",
      " [7. 8.]]\n",
      "\n",
      "concatenated at dimension 1:\n",
      "[[1. 2. 5. 6.]\n",
      " [3. 4. 7. 8.]]\n",
      "\n",
      "\n",
      "vector1\n",
      "[1. 2.]\n",
      "\n",
      "vector2\n",
      "[3. 4.]\n",
      "\n",
      "stacked at dimension 0:\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "\n",
      "stacked at dimension 1:\n",
      "[[1. 3.]\n",
      " [2. 4.]]\n"
     ]
    }
   ],
   "source": [
    "matrix1 = np.array([[1, 2], [3, 4]], np.float32)\n",
    "matrix2 = np.array([[5, 6], [7, 8]], np.float32)\n",
    "\n",
    "print('matrix1')\n",
    "print(matrix1)\n",
    "print()\n",
    "print('matrix2')\n",
    "print(matrix2)\n",
    "print()\n",
    "print('concatenated at dimension 0:')\n",
    "print(np.concatenate([matrix1, matrix2], axis=0))\n",
    "print()\n",
    "print('concatenated at dimension 1:')\n",
    "print(np.concatenate([matrix1, matrix2], axis=1))\n",
    "print()\n",
    "print()\n",
    "\n",
    "vector1 = np.array([1, 2], np.float32)\n",
    "vector2 = np.array([3, 4], np.float32)\n",
    "\n",
    "print('vector1')\n",
    "print(vector1)\n",
    "print()\n",
    "print('vector2')\n",
    "print(vector2)\n",
    "print()\n",
    "print('stacked at dimension 0:')\n",
    "print(np.stack([vector1, vector2], axis=0))\n",
    "print()\n",
    "print('stacked at dimension 1:')\n",
    "print(np.stack([vector1, vector2], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch tensors\n",
    "\n",
    "PyTorch works very similarly to NumPy except that it uses `torch.tensor` instead of `np.array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar\n",
      "tensor(1)\n",
      "ndim: 0\n",
      "shape: torch.Size([])\n",
      "\n",
      "vector\n",
      "tensor([1, 2, 3, 4])\n",
      "ndim: 1\n",
      "shape: torch.Size([4])\n",
      "\n",
      "matrix\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "ndim: 2\n",
      "shape: torch.Size([3, 4])\n",
      "\n",
      "3D tensor\n",
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15, 16],\n",
      "         [17, 18, 19, 20],\n",
      "         [21, 22, 23, 24]]])\n",
      "ndim: 3\n",
      "shape: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "print('scalar')\n",
    "scalar = torch.tensor(1, dtype=torch.int64, device='cpu')\n",
    "print(scalar)\n",
    "print('ndim:', scalar.ndim)\n",
    "print('shape:', scalar.shape)\n",
    "print()\n",
    "\n",
    "print('vector')\n",
    "vector = torch.tensor([1, 2, 3, 4], dtype=torch.int64, device='cpu')\n",
    "print(vector)\n",
    "print('ndim:', vector.ndim)\n",
    "print('shape:', vector.shape)\n",
    "print()\n",
    "\n",
    "print('matrix')\n",
    "matrix = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], dtype=torch.int64, device='cpu')\n",
    "print(matrix)\n",
    "print('ndim:', matrix.ndim)\n",
    "print('shape:', matrix.shape)\n",
    "print()\n",
    "\n",
    "print('3D tensor')\n",
    "tensor3d = torch.tensor([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]], dtype=torch.int64, device='cpu')\n",
    "print(tensor3d)\n",
    "print('ndim:', tensor3d.ndim)\n",
    "print('shape:', tensor3d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PyTorch you can also specify the device on which you want to process your tensors, that is, whether the tensor is to be processed on the CPU or on the GPU.\n",
    "To specify GPU, you use it's CUDA name such as 'cuda:0'.\n",
    "The number '0' is there in case you have an expensive graphic card with multiple GPUs.\n",
    "You can get a list of all your available GPUs using the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print('cuda:', i, '-', torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want to know if the computer using your code has CUDA installed, use this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You do not have CUDA installed.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('You have CUDA installed.')\n",
    "else:\n",
    "    print('You do not have CUDA installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You normally shouldn't hard code the specific GPU to use because that depends on which ones are in use.\n",
    "In this case, you should just use `'cuda'` as a device and then it will be set according to the [CUDA_VISIBLE_DEVICES environmental variable](https://stackoverflow.com/questions/39649102/how-do-i-select-which-gpu-to-run-a-job-on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1, dtype=torch.int64, device= device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tensors that are being used in the same operation have to be on the same device.\n",
    "So you cannot add together a CPU vector with a CUDA vector or you'll get an error.\n",
    "You can move a tensor from one device to another using the `to` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(1, dtype=torch.int64, device='cpu')\n",
    "print(scalar.device)\n",
    "if torch.cuda.is_available():\n",
    "    scalar = scalar.to('cuda')\n",
    "    print(scalar.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do all the things we did above with NumPy on PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector\n",
      "tensor([1, 2, 3, 4])\n",
      "\n",
      "vector + 1\n",
      "tensor([2, 3, 4, 5])\n",
      "\n",
      "vector * 2\n",
      "tensor([2, 4, 6, 8])\n",
      "\n",
      "vector + vector\n",
      "tensor([2, 4, 6, 8])\n",
      "\n",
      "vector * vector\n",
      "tensor([ 1,  4,  9, 16])\n",
      "\n",
      "matrix1\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "matrix2\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.]])\n",
      "\n",
      "matrix1@matrix2\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "vector = torch.tensor([1, 2, 3, 4], dtype=torch.int64, device=device)\n",
    "print('vector')\n",
    "print(vector)\n",
    "print()\n",
    "\n",
    "print('vector + 1')\n",
    "print(vector + 1)\n",
    "print()\n",
    "\n",
    "print('vector * 2')\n",
    "print(vector * 2)\n",
    "print()\n",
    "\n",
    "print('vector + vector')\n",
    "print(vector + vector)\n",
    "print()\n",
    "\n",
    "print('vector * vector')\n",
    "print(vector * vector)\n",
    "print()\n",
    "\n",
    "matrix1 = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, device=device)\n",
    "matrix2 = torch.tensor([[1, 0], [0, 1]], dtype=torch.float32, device=device)\n",
    "print('matrix1')\n",
    "print(matrix1)\n",
    "print('matrix2')\n",
    "print(matrix2)\n",
    "print()\n",
    "print('matrix1@matrix2')\n",
    "print(matrix1@matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the matrix:\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "\n",
      "second element of first row:\n",
      "tensor(2)\n",
      "\n",
      "all elements of first row:\n",
      "tensor([1, 2, 3, 4])\n",
      "\n",
      "all elements of first column:\n",
      "tensor([1, 5, 9])\n",
      "\n",
      "the matrix with an extra dimension at the end:\n",
      "tensor([[[ 1],\n",
      "         [ 2],\n",
      "         [ 3],\n",
      "         [ 4]],\n",
      "\n",
      "        [[ 5],\n",
      "         [ 6],\n",
      "         [ 7],\n",
      "         [ 8]],\n",
      "\n",
      "        [[ 9],\n",
      "         [10],\n",
      "         [11],\n",
      "         [12]]])\n",
      "\n",
      "the matrix with an extra dimension at the front:\n",
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12]]])\n",
      "\n",
      "first row but as a single row matrix (add a dimension of size 1 before row data):\n",
      "tensor([[1, 2, 3, 4]])\n",
      "\n",
      "first row but as a single column matrix (add a dimension of size 1 after row data):\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "\n",
      "reshape 3x4 matrix into 4x3:\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "\n",
      "reshape 3x4 matrix into 3x2x2:\n",
      "tensor([[[ 1,  2],\n",
      "         [ 3,  4]],\n",
      "\n",
      "        [[ 5,  6],\n",
      "         [ 7,  8]],\n",
      "\n",
      "        [[ 9, 10],\n",
      "         [11, 12]]])\n",
      "\n",
      "reshape 3x4 matrix into 12:\n",
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], dtype=torch.int64, device=device)\n",
    "print('the matrix:')\n",
    "print(matrix)\n",
    "print()\n",
    "\n",
    "print('second element of first row:')\n",
    "print(matrix[0, 1])\n",
    "print()\n",
    "\n",
    "print('all elements of first row:')\n",
    "print(matrix[0, :])\n",
    "print()\n",
    "\n",
    "print('all elements of first column:')\n",
    "print(matrix[:, 0])\n",
    "print()\n",
    "\n",
    "print('the matrix with an extra dimension at the end:')\n",
    "print(matrix[:, :, None])\n",
    "print()\n",
    "\n",
    "print('the matrix with an extra dimension at the front:')\n",
    "print(matrix[None, :, :])\n",
    "print()\n",
    "\n",
    "print('first row but as a single row matrix (add a dimension of size 1 before row data):')\n",
    "print(matrix[0, None, :])\n",
    "print()\n",
    "\n",
    "print('first row but as a single column matrix (add a dimension of size 1 after row data):')\n",
    "print(matrix[0, :, None])\n",
    "print()\n",
    "\n",
    "print('reshape 3x4 matrix into 4x3:')\n",
    "print(matrix.reshape([4, 3]))\n",
    "print()\n",
    "\n",
    "print('reshape 3x4 matrix into 3x2x2:')\n",
    "print(matrix.reshape([3, 2, 2]))\n",
    "print()\n",
    "\n",
    "print('reshape 3x4 matrix into 12:')\n",
    "print(matrix.reshape([12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix1\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "\n",
      "matrix2\n",
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n",
      "\n",
      "concatenated at dimension 0:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "\n",
      "concatenated at dimension 1:\n",
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n",
      "\n",
      "\n",
      "vector1\n",
      "tensor([1., 2.])\n",
      "\n",
      "vector2\n",
      "tensor([3., 4.])\n",
      "\n",
      "stacked at dimension 0:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "\n",
      "stacked at dimension 1:\n",
      "tensor([[1., 3.],\n",
      "        [2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "matrix1 = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, device=device)\n",
    "matrix2 = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32, device=device)\n",
    "\n",
    "print('matrix1')\n",
    "print(matrix1)\n",
    "print()\n",
    "print('matrix2')\n",
    "print(matrix2)\n",
    "print()\n",
    "print('concatenated at dimension 0:')\n",
    "print(torch.concat([matrix1, matrix2], dim=0))\n",
    "print()\n",
    "print('concatenated at dimension 1:')\n",
    "print(torch.concat([matrix1, matrix2], dim=1))\n",
    "print()\n",
    "print()\n",
    "\n",
    "vector1 = torch.tensor([1, 2], dtype=torch.float32, device=device)\n",
    "vector2 = torch.tensor([3, 4], dtype=torch.float32, device=device)\n",
    "\n",
    "print('vector1')\n",
    "print(vector1)\n",
    "print()\n",
    "print('vector2')\n",
    "print(vector2)\n",
    "print()\n",
    "print('stacked at dimension 0:')\n",
    "print(torch.stack([vector1, vector2], dim=0))\n",
    "print()\n",
    "print('stacked at dimension 1:')\n",
    "print(torch.stack([vector1, vector2], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can convert from PyTorch tensor to NumPy array and to nested lists and vice versa.\n",
    "Note that only a CPU PyTorch tensor can be converted into a NumPy array or list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch to numpy\n",
      "[1 2 3 4]\n",
      "\n",
      "pytorch to list\n",
      "[1, 2, 3, 4]\n",
      "\n",
      "numpy to pytorch\n",
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "pytorch_tensor = torch.tensor([1, 2, 3, 4], dtype=torch.int64, device=device)\n",
    "\n",
    "print('pytorch to numpy')\n",
    "print(pytorch_tensor.cpu().numpy()) # .cpu() is a shorter way to move the tensor to CPU.\n",
    "print()\n",
    "\n",
    "print('pytorch to list')\n",
    "print(pytorch_tensor.cpu().tolist())\n",
    "print()\n",
    "\n",
    "numpy_array = np.array([1, 2, 3, 4], np.int64)\n",
    "\n",
    "print('numpy to pytorch')\n",
    "print(torch.tensor(numpy_array, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a list of all the operations you can do with PyTorch tensors [here](https://pytorch.org/docs/stable/tensors.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch modules\n",
    "\n",
    "In PyTorch you can define **modules**, which are objects that hold a number of tensors and a function that uses them.\n",
    "For example, we can define a quadratic equation module which computes a quadratic equation:\n",
    "\n",
    "$$y = a x^2 + b x + c$$\n",
    "\n",
    "In a module, tensors are used as instance variables and wrapped in a `Parameter` class.\n",
    "To access the tensor object from the parameter object, you can use the `data` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor(1.)\n",
      "b: tensor(2.)\n",
      "c: tensor(3.)\n",
      "\n",
      "0: tensor(3., grad_fn=<AddBackward0>)\n",
      "1: tensor(6., grad_fn=<AddBackward0>)\n",
      "2: tensor(11., grad_fn=<AddBackward0>)\n",
      "3: tensor(18., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Quadratic(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, a, b, c):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.tensor(a, dtype=torch.float32))\n",
    "        self.b = torch.nn.Parameter(torch.tensor(b, dtype=torch.float32))\n",
    "        self.c = torch.nn.Parameter(torch.tensor(c, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.a*x**2 + self.b*x + self.c\n",
    "\n",
    "module = Quadratic(1, 2, 3)\n",
    "print('a:', module.a.data)\n",
    "print('b:', module.b.data)\n",
    "print('c:', module.c.data)\n",
    "print()\n",
    "\n",
    "y = module(0)\n",
    "print('0:', y)\n",
    "\n",
    "y = module(1)\n",
    "print('1:', y)\n",
    "\n",
    "y = module(2)\n",
    "print('2:', y)\n",
    "\n",
    "y = module(3)\n",
    "print('3:', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the bits that say 'grad_fn' in the output for now; that will be explained a bit further down.\n",
    "\n",
    "Notice how calling `module(0)` will call the `forward` method in the class automatically.\n",
    "You are free to define as many parameters as you like in the `forward` method.\n",
    "\n",
    "Among other things, modules can move all their tensors to another device with one function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Documents\\Github\\Deep-Learning-Approaches-to-Natural-Language-Processing\\Notes\\01_pytorch-20230925\\introduction_to_pytorch.ipynb Cell 37\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Github/Deep-Learning-Approaches-to-Natural-Language-Processing/Notes/01_pytorch-20230925/introduction_to_pytorch.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(module\u001b[39m.\u001b[39ma\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Github/Deep-Learning-Approaches-to-Natural-Language-Processing/Notes/01_pytorch-20230925/introduction_to_pytorch.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m module\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Github/Deep-Learning-Approaches-to-Natural-Language-Processing/Notes/01_pytorch-20230925/introduction_to_pytorch.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(module\u001b[39m.\u001b[39ma\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "print(module.a.device)\n",
    "module.to('cuda')\n",
    "print(module.a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a list of all parameters in the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in module.parameters():\n",
    "    print(p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And they can be saved and loaded to/from a file along with their tensor values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = Quadratic(1, 2, 3)\n",
    "\n",
    "torch.save(module, 'module.pkl')\n",
    "print(module.a.data, module.b.data, module.c.data)\n",
    "\n",
    "new_module = torch.load('module.pkl')\n",
    "print(new_module.a.data, new_module.b.data, new_module.c.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the file format is a Python pickle, but saving and loading it using these provided functions allows a module that was saved while on GPU to be loaded on CPU, which would result in an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "One of the coolest things about PyTorch (and other deep learning libraries) is its ability to automatically compute the **gradients** of functions.\n",
    "Gradients are very useful in deep learning, as we'll see in the next topic, so this helps a lot.\n",
    "\n",
    "Let's go through a gentle introduction to gradients.\n",
    "The gradient of a straight line is a measure of the steepness of the line.\n",
    "You can calculate the gradient of a plotted line by taking two points on the line and dividing the vertical difference by the horizontal difference as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.axis('scaled')\n",
    "ax.set_xlim(-1.0, 5.0)\n",
    "ax.set_ylim(-1.0, 2.5)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.plot([-1, 5], [2.5, -0.5], color='red', linestyle='-', linewidth=2)\n",
    "ax.plot([0], [2], linestyle='', marker='o', markersize=5, color='black')\n",
    "ax.plot([4], [0], linestyle='', marker='o', markersize=5, color='black')\n",
    "ax.text(-0.75, 1.75, '$(0, 2)$', color='black', fontsize=15.0)\n",
    "ax.text(4.1, 0.0, '$(4, 0)$', color='black', fontsize=15.0)\n",
    "ax.plot([0, 0, 4], [2, 0, 0], color='black', linestyle=':', linewidth=2)\n",
    "ax.text(0.5, -0.5, '$\\\\frac{\\\\Delta y}{\\\\Delta x} = \\\\frac{0 - 2}{4 - 0} = \\\\frac{-2}{4} = -0.5$', color='black', fontsize=15.0)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line is angled downwards so the vertical difference is negative, making the gradient negative.\n",
    "An upwards angled line would have a positive gradient line.\n",
    "\n",
    "With a curve, there is no single gradient like with a straight line, because the steepness changes as you move along the curve.\n",
    "However, you can calculate the gradient of a curve at a particular point instead, by drawing a straight line that touches the curve at that point (the line is called a tangent) and measuring the gradient of that line.\n",
    "\n",
    "The below curve includes the gradients of three tangets touching three points on the curve: $(-2, 4)$, $(0, 0)$, and $(1.5, 2.25)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(1, 1, figsize=(5, 5))\n",
    "xs = np.mgrid[-3:3.1:0.1]\n",
    "ys = xs**2\n",
    "ax.plot(xs, ys, color='red', linestyle='-', linewidth=1)\n",
    "\n",
    "ax.plot([-2.24, -1.76], [4.97, 3.03], color='black', linestyle='-', linewidth=1)\n",
    "ax.plot([-2.24, -2.24, -1.76], [4.97, 3.03, 3.03], color='black', linestyle=':', linewidth=1)\n",
    "ax.plot([-2], [4], color='black', linestyle='', marker='o', markersize=5)\n",
    "ax.text(-1.75, 3.8, '-4', color='black', fontsize=15.0)\n",
    "\n",
    "ax.plot([-1.00, 1.00], [0.00, 0.00], color='black', linestyle='-', linewidth=1)\n",
    "ax.plot([0], [0], color='black', linestyle='', marker='o', markersize=5)\n",
    "ax.text(-0.15, 0.3, '0', color='black', fontsize=15.0)\n",
    "\n",
    "ax.plot([1.18, 1.82], [1.30, 3.20], color='black', linestyle='-', linewidth=1)\n",
    "ax.plot([1.18, 1.82, 1.82], [1.30, 1.30, 3.20], color='black', linestyle=':', linewidth=1)\n",
    "ax.plot([1.5], [2.25], color='black', linestyle='', marker='o', markersize=5)\n",
    "ax.text(0.8, 2.1, '3', color='black', fontsize=15.0)\n",
    "\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a 3D curve, such as the bowl shown below, the tangent would not be a straight line but a flat plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(1, 1, figsize=(10, 10), subplot_kw={'projection': '3d'})\n",
    "ax.view_init(0, 70)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "(ys, xs) = np.mgrid[-3:3.1:0.1, -3:3.1:0.1]\n",
    "zs = xs**2 + ys**2\n",
    "ax.plot_surface(xs, ys, zs, color='red', linewidth=0, antialiased=True)\n",
    "\n",
    "(ys, xs) = np.mgrid[0:3.1:0.1, 0:3.1:0.1]\n",
    "zs = 4*xs + 4*ys - 8\n",
    "ax.plot_surface(xs, ys, zs, color='black', linewidth=0, antialiased=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A flat plane has two gradients (not one, like in a straight line).\n",
    "It has a gradient in the $x$ direction and another gradient in the $y$ direction.\n",
    "This is because the plane can be made steeper in either of these two directions independently, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, axs) = plt.subplots(1, 2, figsize=(10, 10), subplot_kw={'projection': '3d'})\n",
    "\n",
    "axs[0].set_title('steeper in the $x$ direction')\n",
    "axs[0].set_xlabel('x')\n",
    "axs[0].set_ylabel('y')\n",
    "axs[0].set_zlabel('z')\n",
    "(ys, xs) = np.mgrid[-3:3.1:0.1, -3:3.1:0.1]\n",
    "zs = 10*xs + ys\n",
    "axs[0].plot_surface(xs, ys, zs, color='black', linewidth=0, antialiased=True)\n",
    "\n",
    "axs[1].set_title('steeper in the $y$ direction')\n",
    "axs[1].set_xlabel('x')\n",
    "axs[1].set_ylabel('y')\n",
    "axs[1].set_zlabel('z')\n",
    "(ys, xs) = np.mgrid[-3:3.1:0.1, -3:3.1:0.1]\n",
    "zs = xs + 10*ys\n",
    "axs[1].plot_surface(xs, ys, zs, color='black', linewidth=0, antialiased=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when finding the gradient of a 3D curve, we get two numbers: the gradient with respect to $x$ and the gradient with respect to $y$.\n",
    "In mathematics, these are called the **partial derivatives** of the curve.\n",
    "\n",
    "Now PyTorch can automatically calculate the gradient of a function at a point $(x, y)$ with respect to both $x$ and $y$.\n",
    "This is done as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by creating the tensors that you want to find the gradient with respect of, that is, $x$ and $y$.\n",
    "These will need to be given the additional parameter `requires_grad=True` to tell PyTorch to prepare to calculate the gradient with respect to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, dtype=torch.float32, device=device, requires_grad=True)\n",
    "y = torch.tensor(2.0, dtype=torch.float32, device=device, requires_grad=True)\n",
    "print('x:', x)\n",
    "print('y:', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use these tensors to calculate the equation you want to find the gradient of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x**2 + y**2\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take `z` and apply the `backward` function to it, which will calculate the gradient of the function that produced `z` with respect to all the function's input tensors that had `requires_grad=True`.\n",
    "Note that the word 'backward' comes from an algorithm called the **backpropagation algorithm** which has a **forward pass** and a **backward pass**.\n",
    "Calculating `z` is the forward pass, whilst calling the `backward` function is the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can find the gradients with respect to `x` and `y` using the `grad` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('gradient with respect to x at x=2, y=2:', x.grad)\n",
    "print('gradient with respect to y at x=2, y=2:', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to reset the gradients of the tensors to zero using the `zero_` function in order to make the `grad` attribute zero.\n",
    "If you don't do this, then calculating another gradient of the same tensor will add the new gradient to the previous gradient, which will create chaos if you're not aware of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x.grad:', x.grad)\n",
    "z = x**2 + y**2\n",
    "z.backward()\n",
    "print('x.grad after calling backward again:', x.grad)\n",
    "\n",
    "x.grad.zero_()\n",
    "y.grad.zero_()\n",
    "print('x.grad after calling zero_:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is in its entirety:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, dtype=torch.float32, device=device, requires_grad=True)\n",
    "y = torch.tensor(2.0, dtype=torch.float32, device=device, requires_grad=True)\n",
    "print('inital gradient at x:', x.grad)\n",
    "print('inital gradient at y:', y.grad)\n",
    "print()\n",
    "\n",
    "z = x**2 + y**2\n",
    "z.backward()\n",
    "print(f'gradient z with respect to x at x={x}, y={y}:', x.grad)\n",
    "print(f'gradient z with respect to y at x={x}, y={y}:', y.grad)\n",
    "\n",
    "x.grad.zero_()\n",
    "y.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tensors wrapped in a `Parameter` class are automatically set as `requires_grad=True`.\n",
    "\n",
    "If you want to take a PyTorch value that was used in the computation of a gradient, such as `z` or `x` above, and turn it into a NumPy array, you need to first detach it from PyTorch's gradient measuring process, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('z:', z)\n",
    "print('z.detach().cpu():', z.detach().cpu())\n",
    "print('z.detach().cpu().numpy():', z.detach().cpu().numpy())\n",
    "print()\n",
    "print('x:', x)\n",
    "print('x.detach().cpu():', x.detach().cpu())\n",
    "print('x.detach().cpu().numpy():', x.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that you can only find the gradient at a single point rather than finding the general gradient function.\n",
    "PyTorch does not find the general gradient function and each new gradient will require a new forward pass and backward pass (as described above)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75ee2b71ad44bf9ef4e9bee896f68ffbc764a6a2c6d1f57c86c48f99ffc25ca8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
