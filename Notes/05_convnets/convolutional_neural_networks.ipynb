{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks\n",
    "\n",
    "Up to now we've been processing text such that the neural network was looking at the entire text all at once, either as a bag of words or as a sequence of tokens, such that the number of parameters in the model depends on each token.\n",
    "This is very parameter inefficient, which leads to longer training time and overfitting.\n",
    "A better way is to only look at a small chunk of tokens at once and do something with them before looking at the next chunk of tokens.\n",
    "The number of parameters required in this way depends on the number of tokens in the chunk rather than in the text, which is more reasonable and is closer to how people read.\n",
    "In this topic we'll see how to do this using a mathematical operator called the **convolution operation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The convolution operator\n",
    "\n",
    "The **convolution operator** is a mathematical operator that takes two tensors, the **input tensor** and the smaller **kernel tensor**, and produces a **resultant tensor**.\n",
    "Imagine the kernel tensor sliding over the input tensor.\n",
    "As the kernel slides over the input, the **dot product** (sum of products of corresponding numbers) of the overlapping numbers is appended to the resultant tensor.\n",
    "Below is an example using two vectors:\n",
    "\n",
    "![](simple_conv.png)\n",
    "\n",
    "The width of the kernel, or amount of numbers seen at once, is called the **window size**.\n",
    "Note how the resultant vector is one number shorter than the input vector.\n",
    "Having a smaller resultant tensor is normal in convolution operations.\n",
    "\n",
    "In NumPy, this function is called `correlate` (a signal processing term):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.  8. 11.]\n"
     ]
    }
   ],
   "source": [
    "input_ = np.array([1, 2, 3, 4], np.float32)\n",
    "kernel = np.array([1, 2], np.float32)\n",
    "print(np.correlate(input_, kernel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch you don't slide over a list of scalars like in NumPy, but over a list of vectors (such as token vectors).\n",
    "We'll need to use 3D tensors for the inputs.\n",
    "These three dimensions are\n",
    "\n",
    "* the number of texts in the batch by\n",
    "* the number of token in each text by\n",
    "* the token vector size (the size of the vectors).\n",
    "\n",
    "We'll also need 3D tensors for the kernel.\n",
    "The three dimensions are\n",
    "\n",
    "* the window size by\n",
    "* the vector size in the input tensor by\n",
    "* the vector size in the resultant tensor.\n",
    "\n",
    "The kernel does not perform a dot product to the part of the input it is sliding over but performs a matrix multiplication.\n",
    "This is so that the resultant tensor can have a differently sized vector than the vectors in the input tensor.\n",
    "\n",
    "Also, the vector size is called the number of **channels** (a term from image processing).\n",
    "\n",
    "Here's an illustration showing how this works:\n",
    "\n",
    "![](complex_conv.png)\n",
    "\n",
    "Note how we started with vector sizes of 3 and ended up with vector sizes of 4.\n",
    "Also note that if the number of texts was more than one, the same operation would be computed on each text separately.\n",
    "\n",
    "In PyTorch this is done using the `torch.nn.Conv1d` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1780, -0.7316,  0.4592,  3.3771],\n",
      "         [-2.3428, -1.1487,  0.5295,  6.4880],\n",
      "         [-3.5076, -1.5658,  0.5999,  9.5988]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ = torch.tensor([\n",
    "    [\n",
    "        [1 , 2 , 3 ],\n",
    "        [4 , 5 , 6 ],\n",
    "        [7 , 8 , 9 ],\n",
    "        [10, 11, 12],\n",
    "    ],\n",
    "], dtype=torch.float32, device=device)\n",
    "\n",
    "layer = torch.nn.Conv1d(3, 4, 2) # input vector size, output vector size, window size\n",
    "layer.to(device)\n",
    "\n",
    "print(layer(input_.transpose(1, 2)).transpose(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Conv1d layer is an actual neural layer, called a **convolutional layer**, that replaces the matrix-multiplication of the weight matrix in a linear layer with a convolution operation of a kernel 3D tensor.\n",
    "A bias is also used which is added to the resultant vector.\n",
    "\n",
    "You're surely wondering what those `transpose` functions are doing there.\n",
    "A transpose operation is used to swap two dimensions in a tensor, such as the rows and columns of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "x transposed\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print('x')\n",
    "print(x)\n",
    "print()\n",
    "print('x transposed')\n",
    "print(x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, convolutions in PyTorch are a bit weird in that they require the dimensions of the input and resultant to be in the following orders:\n",
    "\n",
    "Input -\n",
    "\n",
    "* the number of texts in the batch by\n",
    "* the token vector size (the size of the vectors) by\n",
    "* the number of tokens in the text.\n",
    "\n",
    "Resultant -\n",
    "\n",
    "* the number of texts in the batch by\n",
    "* output vector size by\n",
    "* the number of vectors returned.\n",
    "\n",
    "Since this order is not intuitive, it's more readable to perform a transpose operation on an intuitively ordered input tensor and then transpose the resultant tensor into a sensible order.\n",
    "\n",
    "The transpose operation in PyTorch can be made on a tensor of any number of dimensions and you just need to provide the indexes of the dimensions to swap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.],\n",
      "         [ 7.,  8.,  9.],\n",
      "         [10., 11., 12.]],\n",
      "\n",
      "        [[13., 14., 15.],\n",
      "         [16., 17., 18.],\n",
      "         [19., 20., 21.],\n",
      "         [22., 23., 24.]]])\n",
      "\n",
      "input.transpose(1, 2)\n",
      "torch.Size([2, 3, 4])\n",
      "tensor([[[ 1.,  4.,  7., 10.],\n",
      "         [ 2.,  5.,  8., 11.],\n",
      "         [ 3.,  6.,  9., 12.]],\n",
      "\n",
      "        [[13., 16., 19., 22.],\n",
      "         [14., 17., 20., 23.],\n",
      "         [15., 18., 21., 24.]]])\n"
     ]
    }
   ],
   "source": [
    "input_ = torch.tensor([\n",
    "    [\n",
    "        [1 , 2 , 3 ],\n",
    "        [4 , 5 , 6 ],\n",
    "        [7 , 8 , 9 ],\n",
    "        [10, 11, 12],\n",
    "    ],\n",
    "    [\n",
    "        [13, 14, 15],\n",
    "        [16, 17, 18],\n",
    "        [19, 20, 21],\n",
    "        [22, 23, 24],\n",
    "    ],\n",
    "], dtype=torch.float32, device=device)\n",
    "\n",
    "print('input')\n",
    "print(input_.shape)\n",
    "print(input_)\n",
    "print()\n",
    "\n",
    "x = input_.transpose(1, 2)\n",
    "print('input.transpose(1, 2)')\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how all we're doing to the input and resultant tensors is transpose each matrix in the 3D tensor, that is, making the token vectors be columns instead of rows.\n",
    "The order of the texts in the input is not affected at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The convolutional neural network with a vector pooling layer\n",
    "\n",
    "A neural network whose hidden layers mainly consist of convolutional layers is called a **convolutional neural network** (**CNN**).\n",
    "Convolutional neural networks have been extensively used for images, but they are also used for text to a lesser extent.\n",
    "The trouble with convolutional layers is that they return a matrix for each text rather than a vector, which needs to somehow be transformed into a vector before it can be passed into a softmax for classification.\n",
    "In the previous topic, we solved this by flattening the matrix such that all the rows were placed side by side as a single vector, but this forces us to have a fixed number of tokens.\n",
    "\n",
    "A better method than flattening in order to turn a matrix of row vectors into a single vector is to use vector pooling.\n",
    "This is when a bunch of vectors are aggregated into a single vector by either taking the minimum values of each corresponding element in the vectors, called min pooling, or taking the maximum values, called max pooling.\n",
    "Here is an illustration of min pooling:\n",
    "\n",
    "![](pooling.png)\n",
    "\n",
    "Note that the vectors at the top are token (or window) vectors forming a single text and we transformed a bunch of vectors into a single vector representing the entire text.\n",
    "\n",
    "Here is how to do this in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 texts with 3 token vectors\n",
      "tensor([[[ 1.4199, -0.3485, -0.7392, -0.0578],\n",
      "         [ 1.4997,  0.2086,  2.3150,  0.2089],\n",
      "         [ 0.2680,  0.2394, -0.8551, -0.5382]],\n",
      "\n",
      "        [[-0.4834,  1.4276,  0.3755,  0.6827],\n",
      "         [-0.3089, -0.8098,  1.1748,  0.2378],\n",
      "         [-0.7522,  1.8504, -2.2459,  0.1056]]])\n",
      "\n",
      "max pooled\n",
      "tensor([[ 1.4997,  0.2394,  2.3150,  0.2089],\n",
      "        [-0.3089,  1.8504,  1.1748,  0.6827]])\n",
      "\n",
      "min pooled\n",
      "tensor([[ 0.2680, -0.3485, -0.8551, -0.5382],\n",
      "        [-0.7522, -0.8098, -2.2459,  0.1056]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((2, 3, 4), device=device)\n",
    "print('2 texts with 3 token vectors')\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "print('max pooled')\n",
    "(max_values, _) = torch.max(x, dim=1)\n",
    "print(max_values)\n",
    "print()\n",
    "\n",
    "print('min pooled')\n",
    "(min_values, _) = torch.min(x, dim=1)\n",
    "print(min_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the vector representing the entire text will always be represented by a vector having the size of a single token (or window) vector, regardless of how many tokens are in the text.\n",
    "So now, after training, we can pass a sentence with more tokens or less tokens to the model and the neural network will still work.\n",
    "\n",
    "This doesn't mean that can do away with pad tokens because we're still passing a whole batch of texts at once, and if those texts are not all the same length then we need to use pad tokens to make a regularly shaped tensor (we can't have a [jagged array](https://en.wikipedia.org/wiki/Jagged_array) as a tensor).\n",
    "So pad tokens will be in the input and will be affecting the vector that comes out the other end of a pooling function.\n",
    "Now this wasn't a problem for **fixed-size input models**, because a text would always have the same number of pad tokens appended to it.\n",
    "But for **variable-sized input models** this will cause problems because the same text can have any number of pad tokens attached, depending on which other texts are included in its batch.\n",
    "So it's important that we somehow make the model ignore the windows containing pad tokens.\n",
    "\n",
    "<table>\n",
    "    <tr><th>Text</th><td>the dog bit the cat PAD PAD</td></tr>\n",
    "    <tr><th>Windows</th><td>the dog | dog bit | bit the | the cat | cat PAD | PAD PAD</td></tr>\n",
    "    <tr><th>Filtered windows</th><td>the dog | dog bit | bit the | the cat</td></tr>\n",
    "</table>\n",
    "\n",
    "We can do this by substituting the numbers in the window vectors containing pad tokens with infinity when using min pooling, or negative infinity when using max pooling.\n",
    "The window vectors will still be there, but the result of the pool function will not be affected by them.\n",
    "This is a form of masking (like in dropout):\n",
    "\n",
    "<table>\n",
    "    <tr><th>Text</th><td>the dog bit the cat PAD PAD</td></tr>\n",
    "    <tr><th>Windows</th><td>the dog | dog bit | bit the | the cat | cat PAD | PAD PAD</td></tr>\n",
    "    <tr><th>Window vectors</th><td>[[1, -1], [2, -2], [3, -3], [4, -4], [5, -5], [6, -6]]</td></tr>\n",
    "    <tr><th>Masked window vectors</th><td>[[1, -1], [2, -2], [3, -3], [4, -4], [&infin;, &infin;], [&infin;, &infin;]]</td></tr>\n",
    "    <tr><th>Min pool</th><td>[1, -4]</td></tr>\n",
    "</table>\n",
    "\n",
    "You can do this by using `masked_fill`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 text with 3 token vectors\n",
      "tensor([[[-0.7910, -0.2015],\n",
      "         [-1.1737,  0.9525],\n",
      "         [ 0.9410,  0.1220]]])\n",
      "\n",
      "pad_mask\n",
      "tensor([[[False, False],\n",
      "         [False, False],\n",
      "         [ True,  True]]])\n",
      "\n",
      "masked\n",
      "tensor([[[-0.7910, -0.2015],\n",
      "         [-1.1737,  0.9525],\n",
      "         [    inf,     inf]]])\n",
      "\n",
      "min pooled\n",
      "tensor([[-1.1737, -0.2015]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((1, 3, 2), device=device)\n",
    "print('1 text with 3 token vectors')\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "pad_mask = torch.tensor([ # Tells you which values to replace in the input tensor.\n",
    "    [[0, 0], [0, 0], [1, 1]],\n",
    "], dtype=torch.bool, device=device)\n",
    "print('pad_mask')\n",
    "print(pad_mask)\n",
    "print()\n",
    "\n",
    "masked = x.masked_fill(pad_mask, float('inf'))\n",
    "print('masked')\n",
    "print(masked)\n",
    "print()\n",
    "\n",
    "(pooled, _) = torch.min(masked, dim=1)\n",
    "print('min pooled')\n",
    "print(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pad mask is essential here.\n",
    "We need it to have the same shape as the resultant tensor and to have `True` wherever a window would contain even a single pad token.\n",
    "We can produce one if we know the length of the unpadded texts as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded.shape:\n",
      "torch.Size([4, 4, 2])\n",
      "\n",
      "convolved.shape:\n",
      "torch.Size([4, 3, 2])\n",
      "\n",
      "pad_mask:\n",
      "tensor([[ True,  True,  True],\n",
      "        [False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n",
      "\n",
      "3D pad_mask:\n",
      "tensor([[[ True],\n",
      "         [ True],\n",
      "         [ True]],\n",
      "\n",
      "        [[False],\n",
      "         [ True],\n",
      "         [ True]],\n",
      "\n",
      "        [[False],\n",
      "         [False],\n",
      "         [ True]],\n",
      "\n",
      "        [[False],\n",
      "         [False],\n",
      "         [False]]])\n",
      "\n",
      "masked:\n",
      "tensor([[[    inf,     inf],\n",
      "         [    inf,     inf],\n",
      "         [    inf,     inf]],\n",
      "\n",
      "        [[-0.1729,  0.4912],\n",
      "         [    inf,     inf],\n",
      "         [    inf,     inf]],\n",
      "\n",
      "        [[ 2.1480, -0.8487],\n",
      "         [ 1.0317,  0.7237],\n",
      "         [    inf,     inf]],\n",
      "\n",
      "        [[ 0.7158, -0.5111],\n",
      "         [ 0.4219,  0.1313],\n",
      "         [ 0.6741, -0.0310]]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_len = 4\n",
    "window_size = 2\n",
    "text_lens = [\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "]\n",
    "num_texts = len(text_lens)\n",
    "\n",
    "embed_size = 2\n",
    "out_size = 2\n",
    "layer = torch.nn.Conv1d(embed_size, out_size, window_size)\n",
    "layer.to(device)\n",
    "\n",
    "embedded = torch.randn((num_texts, max_len, embed_size), device=device)\n",
    "print('embedded.shape:')\n",
    "print(embedded.shape)\n",
    "print()\n",
    "\n",
    "convolved = layer(embedded.transpose(1, 2)).transpose(1, 2)\n",
    "print('convolved.shape:')\n",
    "print(convolved.shape)\n",
    "print()\n",
    "\n",
    "num_windows = max_len - window_size + 1\n",
    "pad_mask_np = np.ones((num_texts, num_windows), dtype=np.bool_) # Full of the value True.\n",
    "for i in range(num_texts):\n",
    "    pad_mask_np[i, :text_lens[i] - window_size + 1] = False # The first text_len - window_size + 1 windows do not contain a pad token.\n",
    "pad_mask = torch.tensor(pad_mask_np, device=device)\n",
    "\n",
    "print('pad_mask:')\n",
    "print(pad_mask)\n",
    "print()\n",
    "\n",
    "pad_mask = pad_mask[:, :, None]\n",
    "print('3D pad_mask:')\n",
    "print(pad_mask)\n",
    "print()\n",
    "\n",
    "masked = convolved.masked_fill(pad_mask, float('inf'))\n",
    "print('masked:')\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things to notice here:\n",
    "\n",
    "First, the mask needs to be a 3D tensor in order to be used on a 3D tensor.\n",
    "Simply adding a singleton dimension at the end will make it compatible with a 3D tensor of any third dimension size (window vector size).\n",
    "This very useful feature (saves memory) is called **broadcasting**.\n",
    "\n",
    "Second, note that a window size of 2 assumes that the text will contain at least 2 tokens.\n",
    "For a text that contains just 1 token, the first window will have a pad token in it and will be masked as well.\n",
    "This will create problems as the pooling function will end up having nothing but infinities in its input and will thus output infinities, which will get passed to the next layer.\n",
    "In short, the window size is the minimum text size and you should not accept any texts that have less tokens than the window size.\n",
    "\n",
    "Let's solve the toy data set with a the convolutional neural network and min pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_lens: [4, 4, 5, 5]\n",
      "max_len: 5\n",
      "vocab: ['<PAD>', '.', 'I', \"don't\", 'hate', 'it', 'like']\n",
      "\n",
      "train_x_indexed:\n",
      "tensor([[2, 6, 5, 1, 0],\n",
      "        [2, 4, 5, 1, 0],\n",
      "        [2, 3, 4, 5, 1],\n",
      "        [2, 3, 6, 5, 1]])\n"
     ]
    }
   ],
   "source": [
    "train_x = [\n",
    "    'I like it .'.split(' '),\n",
    "    'I hate it .'.split(' '),\n",
    "    'I don\\'t hate it .'.split(' '),\n",
    "    'I don\\'t like it .'.split(' '),\n",
    "]\n",
    "train_y = torch.tensor([\n",
    "    [1],\n",
    "    [0],\n",
    "    [1],\n",
    "    [0],\n",
    "], dtype=torch.float32, device=device)\n",
    "\n",
    "text_lens = [len(text) for text in train_x]\n",
    "print('text_lens:', text_lens)\n",
    "\n",
    "max_len = max(text_lens)\n",
    "print('max_len:', max_len)\n",
    "\n",
    "vocab = ['<PAD>'] + sorted({token for text in train_x for token in text})\n",
    "token2index = {t: i for (i, t) in enumerate(vocab)}\n",
    "pad_index = token2index['<PAD>']\n",
    "print('vocab:', vocab)\n",
    "print()\n",
    "\n",
    "train_x_indexed_np = np.full((len(train_x), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_x)):\n",
    "    for j in range(len(train_x[i])):\n",
    "        train_x_indexed_np[i, j] = token2index[train_x[i][j]]\n",
    "train_x_indexed = torch.tensor(train_x_indexed_np, device=device)\n",
    "print('train_x_indexed:')\n",
    "print(train_x_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch error\n",
      "200 0.4787391722202301\n",
      "400 0.47736459970474243\n",
      "600 0.4770892262458801\n",
      "800 0.47652941942214966\n",
      "1000 0.4749957323074341\n",
      "1200 0.4706624448299408\n",
      "1400 0.4547891318798065\n",
      "1600 0.4024301767349243\n",
      "1800 0.36087045073509216\n",
      "2000 0.3518727123737335\n",
      "\n",
      "text output\n",
      "['I', 'like', 'it', '.'] 0.5099695324897766\n",
      "['I', 'hate', 'it', '.'] 0.000675175862852484\n",
      "['I', \"don't\", 'hate', 'it', '.'] 0.9801546931266785\n",
      "['I', \"don't\", 'like', 'it', '.'] 0.5099695324897766\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, window_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.conv_layer = torch.nn.Conv1d(embedding_size, hidden_size, window_size)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x_indexed, text_lens):\n",
    "        batch_size = x_indexed.shape[0]\n",
    "        \n",
    "        embedded = self.embedding(x_indexed)\n",
    "        embedded_t = embedded.transpose(1, 2)\n",
    "        hidden_t = torch.nn.functional.leaky_relu(self.conv_layer(embedded_t))\n",
    "        hidden = hidden_t.transpose(1, 2)\n",
    "        \n",
    "        num_windows = max_len - self.window_size + 1\n",
    "        pad_mask_np = np.ones((batch_size, num_windows, 1), dtype=np.bool_) # Make a 3D mask immediately.\n",
    "        for i in range(batch_size):\n",
    "            pad_mask_np[i, :text_lens[i] - self.window_size + 1, :] = False\n",
    "        pad_mask = torch.tensor(pad_mask_np, device=device)\n",
    "        masked = hidden.masked_fill(pad_mask, float('inf'))\n",
    "        \n",
    "        (pooled, _) = torch.min(masked, dim=1)\n",
    "        \n",
    "        return self.output_layer(pooled)\n",
    "\n",
    "model = Model(len(vocab), embedding_size=2, window_size=2, hidden_size=3)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 2000+1):\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x_indexed, text_lens)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%200 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('text', 'output')\n",
    "    output = torch.sigmoid(model(train_x_indexed, text_lens))[:, 0].cpu().tolist()\n",
    "    for (text, y) in zip(train_x, output):\n",
    "        print(text, y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the pad token is really being ignored by checking the gradients of all the different tokens in the embedding matrix.\n",
    "Below we're calculating the gradient of the training error with respect to all parameters, extracting the gradients of the embedding matrix, squashing it into a single number per token by taking the absolute sum of each row (each row represents a token), and then checking the result of each row associated with its token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <PAD>: 0.0\n",
      "     .: 0.0011328894179314375\n",
      "     I: 0.0003101474139839411\n",
      " don't: 0.0013983140233904123\n",
      "  hate: 0.0031053246930241585\n",
      "    it: 0.0017865784466266632\n",
      "  like: 0.0011865552514791489\n"
     ]
    }
   ],
   "source": [
    "optimiser.zero_grad()\n",
    "logits = model(train_x_indexed, text_lens)\n",
    "train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "train_error.backward()\n",
    "\n",
    "grads = model.embedding.weight.grad.abs().sum(dim=1).tolist()\n",
    "for (token, grad) in zip(vocab, grads):\n",
    "    print(f'{token: >6s}: {grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient (or sum of absolute gradients in this case) tells you how **sensitive** the error is to each token in the training set, such that the larger the gradient, the more the error will change if the embedding vector of the token changes.\n",
    "A gradient of zero means that the error will not change if that embedding vector is changed, which is what we want for the pad token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs with parallel convolutions\n",
    "\n",
    "There's no reason why only one convolution needs to be applied in the same layer.\n",
    "It is perfectly fine to extract multiple forms of information from the same source and then concatenate the separate vectors together.\n",
    "\n",
    "![](multiple_convs.png)\n",
    "\n",
    "This would require creating separate convolution layers for each branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded:\n",
      "torch.Size([2, 5, 8])\n",
      "\n",
      "branched_embedded:\n",
      "torch.Size([2, 5, 4]) torch.Size([2, 5, 4])\n",
      "\n",
      "branched_hidden:\n",
      "torch.Size([2, 4, 3]) torch.Size([2, 4, 3])\n",
      "\n",
      "hidden:\n",
      "torch.Size([2, 4, 6])\n",
      "\n",
      "pooled:\n",
      "torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "num_texts = 2\n",
    "max_len = 5\n",
    "embedding_size = 8\n",
    "num_branches = 2\n",
    "window_size = 2\n",
    "hidden_size = 6\n",
    "assert embedding_size%num_branches == 0 # Embedding size must be divisible by number of branches.\n",
    "assert hidden_size%num_branches == 0 # Conv layer output size must be divisible by number of branches.\n",
    "\n",
    "# Create a list of conv layers, each of which takes in and outputs a part of the whole vector.\n",
    "conv_layers = [\n",
    "    torch.nn.Conv1d(embedding_size//num_branches, hidden_size//num_branches, window_size)\n",
    "    for _ in range(num_branches)\n",
    "]\n",
    "for layer in conv_layers:\n",
    "    layer.to(device)\n",
    "\n",
    "embedded = torch.randn((num_texts, max_len, embedding_size), device=device)\n",
    "print('embedded:')\n",
    "print(embedded.shape)\n",
    "print()\n",
    "\n",
    "# Break up the embedded tensor into branches.\n",
    "branched_embedded = [\n",
    "    embedded[:, :, embedding_size//num_branches*i:embedding_size//num_branches*(i+1)]\n",
    "    for i in range(num_branches)\n",
    "]\n",
    "print('branched_embedded:')\n",
    "print(*[branch.shape for branch in branched_embedded])\n",
    "print()\n",
    "\n",
    "# Perform the convolutional process on each branch.\n",
    "branched_embedded_t = [\n",
    "    branched_embedded[i].transpose(1, 2)\n",
    "    for i in range(num_branches)\n",
    "]\n",
    "branched_hidden_t = [\n",
    "    torch.nn.functional.leaky_relu(conv_layers[i](branched_embedded_t[i]))\n",
    "    for i in range(num_branches)\n",
    "]\n",
    "branched_hidden = [\n",
    "    branched_hidden_t[i].transpose(1, 2)\n",
    "    for i in range(num_branches)\n",
    "]\n",
    "print('branched_hidden:')\n",
    "print(*[branch.shape for branch in branched_hidden])\n",
    "print()\n",
    "\n",
    "# Concatenate the branches back into one whole.\n",
    "hidden = torch.cat(branched_hidden, dim=2)\n",
    "print('hidden:')\n",
    "print(hidden.shape)\n",
    "print()\n",
    "\n",
    "(pooled, _) = torch.min(hidden, dim=1)\n",
    "print('pooled:')\n",
    "print(pooled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a multibranch convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_lens: [4, 4, 5, 5]\n",
      "max_len: 5\n",
      "vocab: ['<PAD>', '.', 'I', \"don't\", 'hate', 'it', 'like']\n",
      "\n",
      "train_x_indexed:\n",
      "tensor([[2, 6, 5, 1, 0],\n",
      "        [2, 4, 5, 1, 0],\n",
      "        [2, 3, 4, 5, 1],\n",
      "        [2, 3, 6, 5, 1]])\n"
     ]
    }
   ],
   "source": [
    "train_x = [\n",
    "    'I like it .'.split(' '),\n",
    "    'I hate it .'.split(' '),\n",
    "    'I don\\'t hate it .'.split(' '),\n",
    "    'I don\\'t like it .'.split(' '),\n",
    "]\n",
    "train_y = torch.tensor([\n",
    "    [1],\n",
    "    [0],\n",
    "    [1],\n",
    "    [0],\n",
    "], dtype=torch.float32, device=device)\n",
    "\n",
    "text_lens = [len(text) for text in train_x]\n",
    "print('text_lens:', text_lens)\n",
    "\n",
    "max_len = max(text_lens)\n",
    "print('max_len:', max_len)\n",
    "\n",
    "vocab = ['<PAD>'] + sorted({token for text in train_x for token in text})\n",
    "token2index = {t: i for (i, t) in enumerate(vocab)}\n",
    "pad_index = token2index['<PAD>']\n",
    "print('vocab:', vocab)\n",
    "print()\n",
    "\n",
    "train_x_indexed_np = np.full((len(train_x), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_x)):\n",
    "    for j in range(len(train_x[i])):\n",
    "        train_x_indexed_np[i, j] = token2index[train_x[i][j]]\n",
    "train_x_indexed = torch.tensor(train_x_indexed_np, device=device)\n",
    "print('train_x_indexed:')\n",
    "print(train_x_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch error\n",
      "200 0.47681424021720886\n",
      "400 0.4753340184688568\n",
      "600 0.47193825244903564\n",
      "800 0.46259039640426636\n",
      "1000 0.45813748240470886\n",
      "1200 0.01376353483647108\n",
      "1400 0.0037368021439760923\n",
      "1600 0.002135838381946087\n",
      "1800 0.001490132068283856\n",
      "2000 0.0011424344265833497\n",
      "\n",
      "text output\n",
      "['I', 'like', 'it', '.'] 0.9999173879623413\n",
      "['I', 'hate', 'it', '.'] 2.0143513857419906e-17\n",
      "['I', \"don't\", 'hate', 'it', '.'] 0.9999371767044067\n",
      "['I', \"don't\", 'like', 'it', '.'] 0.004409343004226685\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, window_size, num_branches, hidden_size):\n",
    "        super().__init__()\n",
    "        assert embedding_size%num_branches == 0\n",
    "        assert hidden_size%num_branches == 0\n",
    "        self.embedding_size = embedding_size\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.conv_layers = torch.nn.ModuleList([ # Lists of layers need to be put in a ModuleList object to be part of the module.\n",
    "            torch.nn.Conv1d(embedding_size//num_branches, hidden_size//num_branches, window_size)\n",
    "            for _ in range(num_branches)\n",
    "        ])\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x_indexed, lens):\n",
    "        num_branches = len(self.conv_layers)\n",
    "        batch_size = x_indexed.shape[0]\n",
    "        \n",
    "        embedded = self.embedding_layer(x_indexed)\n",
    "\n",
    "        branched_embedded = [\n",
    "            embedded[:, :, self.embedding_size//num_branches*i:self.embedding_size//num_branches*(i+1)]\n",
    "            for i in range(num_branches)\n",
    "        ]\n",
    "\n",
    "        branched_embedded_t = [\n",
    "            branched_embedded[i].transpose(1, 2)\n",
    "            for i in range(num_branches)\n",
    "        ]\n",
    "        branched_hidden_t = [\n",
    "            torch.nn.functional.leaky_relu(self.conv_layers[i](branched_embedded_t[i]))\n",
    "            for i in range(num_branches)\n",
    "        ]\n",
    "        branched_hidden = [\n",
    "            branched_hidden_t[i].transpose(1, 2)\n",
    "            for i in range(num_branches)\n",
    "        ]\n",
    "        \n",
    "        hidden = torch.cat(branched_hidden, dim=2)\n",
    "        \n",
    "        num_windows = max_len - self.window_size + 1\n",
    "        pad_mask_np = np.ones((batch_size, num_windows, 1), dtype=np.bool_)\n",
    "        for i in range(batch_size):\n",
    "            pad_mask_np[i, :lens[i] - self.window_size + 1, :] = False\n",
    "        pad_mask = torch.tensor(pad_mask_np, device=device)\n",
    "        masked = hidden.masked_fill(pad_mask, float('inf'))\n",
    "        \n",
    "        pooled = torch.min(masked, dim=1)[0]\n",
    "        \n",
    "        return self.output_layer(pooled)\n",
    "\n",
    "model = Model(len(vocab), embedding_size=4, window_size=2, num_branches=2, hidden_size=4)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 2000+1):\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x_indexed, text_lens)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%200 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('text', 'output')\n",
    "    output = torch.sigmoid(model(train_x_indexed, text_lens))[:, 0].cpu().tolist()\n",
    "    for (text, y) in zip(train_x, output):\n",
    "        print(text, y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Using convolutional layers\n",
    "\n",
    "Rewrite the movie reviews classification program using a CNN.\n",
    "Use 2 parallel convolutional layers with a window size of 2, and a min or max pooling layer after that.\n",
    "Preprocessing has ben done for you.\n",
    "Don't forget to calculate the test set accuracy after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "min_freq = 3\n",
    "\n",
    "train_df = pd.read_csv('../data_set/sentiment/train.csv')\n",
    "test_df = pd.read_csv('../data_set/sentiment/test.csv')\n",
    "\n",
    "train_x = train_df['text']\n",
    "train_y = train_df['class']\n",
    "test_x = test_df['text']\n",
    "test_y = test_df['class']\n",
    "categories = ['neg', 'pos']\n",
    "cat2idx = {cat: i for (i, cat) in enumerate(categories)}\n",
    "\n",
    "train_y_indexed = torch.tensor(\n",
    "    train_y.map(cat2idx.get).to_numpy()[:, None],\n",
    "    dtype=torch.float32, device=device\n",
    ")\n",
    "test_y_indexed = test_y.map(cat2idx.get).to_numpy()[:, None]\n",
    "\n",
    "nltk.download('punkt')\n",
    "train_x_tokens = [nltk.word_tokenize(text) for text in train_x]\n",
    "test_x_tokens = [nltk.word_tokenize(text) for text in test_x]\n",
    "train_lens = [len(text) for text in train_x_tokens]\n",
    "test_lens = [len(text) for text in test_x_tokens]\n",
    "max_len = max(max(train_lens), max(test_lens))\n",
    "\n",
    "frequencies = collections.Counter(token for text in train_x_tokens for token in text)\n",
    "vocabulary = sorted(frequencies.keys(), key=frequencies.get, reverse=True)\n",
    "while frequencies[vocabulary[-1]] < min_freq:\n",
    "    vocabulary.pop()\n",
    "vocab = ['<PAD>', '<UNK>'] + vocabulary\n",
    "token2index = {token: i for (i, token) in enumerate(vocab)}\n",
    "pad_index = token2index['<PAD>']\n",
    "unk_index = token2index['<UNK>']\n",
    "\n",
    "train_x_indexed_np = np.full((len(train_x_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_x_tokens)):\n",
    "    for j in range(len(train_x_tokens[i])):\n",
    "        train_x_indexed_np[i, j] = token2index.get(train_x_tokens[i][j], unk_index)\n",
    "train_x_indexed = torch.tensor(train_x_indexed_np, device=device)\n",
    "\n",
    "test_x_indexed_np = np.full((len(test_x_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(test_x_tokens)):\n",
    "    for j in range(len(test_x_tokens[i])):\n",
    "        test_x_indexed_np[i, j] = token2index.get(test_x_tokens[i][j], unk_index)\n",
    "test_x_indexed = torch.tensor(test_x_indexed_np, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch error\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Documents\\Github\\Deep-Learning-Approaches-to-Natural-Language-Processing\\Notes\\05_convnets\\convolutional_neural_networks.ipynb Cell 32\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Github/Deep-Learning-Approaches-to-Natural-Language-Processing/Notes/05_convnets/convolutional_neural_networks.ipynb#X43sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m optimiser\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Github/Deep-Learning-Approaches-to-Natural-Language-Processing/Notes/05_convnets/convolutional_neural_networks.ipynb#X43sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m logits \u001b[39m=\u001b[39m model(train_x_indexed, train_lens)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Github/Deep-Learning-Approaches-to-Natural-Language-Processing/Notes/05_convnets/convolutional_neural_networks.ipynb#X43sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m train_error \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(logits, test_y_indexed)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Github/Deep-Learning-Approaches-to-Natural-Language-Processing/Notes/05_convnets/convolutional_neural_networks.ipynb#X43sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m train_errors\u001b[39m.\u001b[39mappend(train_error\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mtolist())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/Github/Deep-Learning-Approaches-to-Natural-Language-Processing/Notes/05_convnets/convolutional_neural_networks.ipynb#X43sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m train_error\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\functional.py:3162\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3159\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3160\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m-> 3162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39;49msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[0;32m   3163\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[0;32m   3165\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, window_size, num_branches, hidden_size):\n",
    "        super().__init__()\n",
    "        assert embedding_size%num_branches == 0\n",
    "        assert hidden_size%num_branches == 0\n",
    "        self.embedding_size = embedding_size\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.conv_layers = torch.nn.ModuleList([ # Lists of layers need to be put in a ModuleList object to be part of the module.\n",
    "            torch.nn.Conv1d(embedding_size//num_branches, hidden_size//num_branches, window_size)\n",
    "            for _ in range(num_branches)\n",
    "        ])\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x_indexed, lens):\n",
    "        num_branches = len(self.conv_layers)\n",
    "        batch_size = x_indexed.shape[0]\n",
    "        \n",
    "        embedded = self.embedding_layer(x_indexed)\n",
    "\n",
    "        branched_embedded = [\n",
    "            embedded[:, :, self.embedding_size//num_branches*i:self.embedding_size//num_branches*(i+1)]\n",
    "            for i in range(num_branches)\n",
    "        ]\n",
    "\n",
    "        branched_embedded_t = [\n",
    "            branched_embedded[i].transpose(1, 2)\n",
    "            for i in range(num_branches)\n",
    "        ]\n",
    "        branched_hidden_t = [\n",
    "            torch.nn.functional.leaky_relu(self.conv_layers[i](branched_embedded_t[i]))\n",
    "            for i in range(num_branches)\n",
    "        ]\n",
    "        branched_hidden = [\n",
    "            branched_hidden_t[i].transpose(1, 2)\n",
    "            for i in range(num_branches)\n",
    "        ]\n",
    "        \n",
    "        hidden = torch.cat(branched_hidden, dim=2)\n",
    "        \n",
    "        num_windows = max_len - self.window_size + 1\n",
    "        pad_mask_np = np.ones((batch_size, num_windows, 1), dtype=np.bool_)\n",
    "        for i in range(batch_size):\n",
    "            pad_mask_np[i, :lens[i] - self.window_size + 1, :] = False\n",
    "        pad_mask = torch.tensor(pad_mask_np, device=device)\n",
    "        masked = hidden.masked_fill(pad_mask, float('inf'))\n",
    "        \n",
    "        pooled = torch.min(masked, dim=1)[0]\n",
    "        \n",
    "        return self.output_layer(pooled)\n",
    "\n",
    "model = Model(len(vocab), embedding_size=4, window_size=2, num_branches=2, hidden_size=4)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 2000+1):\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x_indexed, train_lens)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, test_y_indexed)\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%200 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_x_indexed, test_lens)[:,0].round(decimal=0).cpu().tolist()\n",
    "    accuracy = sklearn.metrics.accuracy_score(outputs, test_y_indexed)\n",
    "    print(f'accuracy: {accuracy:.3%}')\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75ee2b71ad44bf9ef4e9bee896f68ffbc764a6a2c6d1f57c86c48f99ffc25ca8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
