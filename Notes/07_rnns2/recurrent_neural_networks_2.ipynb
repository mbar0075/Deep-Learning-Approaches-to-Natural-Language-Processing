{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks 2\n",
    "\n",
    "Last time we saw how to use an RNN to turn a sequence of tokens into a single vector, regardless of the number of tokens.\n",
    "This was useful for classifying a whole text but sometimes we want to classify each token of the text instead, such as for part-of-speech tagging (identify nouns, verbs, etc.).\n",
    "The problem with classifying tokens is that you can't just look at each token in isolation as you'll need to see how that token is used, that is, its context.\n",
    "Traditionally, this was solved by taking a fixed number of tokens around each token as context (for example, two to the left and two to the right) and passing the chunk of tokens as input, similarly to how the CNN works.\n",
    "In this topic we'll see how to use the entire text as a context to each token by using RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the intermediate states\n",
    "\n",
    "Refer back to this diagram we saw last time:\n",
    "\n",
    "![](rnn_chain.png)\n",
    "\n",
    "Up to now we've used the final state (state<sub>3</sub>) for representing the entire text (input<sub>1</sub>, input<sub>2</sub>, and input<sub>3</sub>), but we know that the intermediate states also have a useful meaning: state<sub>2</sub> represents input<sub>1</sub> and input<sub>2</sub> whilst state<sub>1</sub> represents input<sub>1</sub>.\n",
    "\n",
    "To introduce the use of these intermediate states, we'll try classifying the sentiment of a text (using the toy data set) but using prefixes of the text as input rather than the full text only.\n",
    "So we'll train the model to learn that \"I\", \"I like\", \"I like it\", and \"I like it .\" should all be predicted as having a positive sentiment.\n",
    "This will be done by classifying each intermediate state produced by the RNN when consuming the text rather than only the final state as we've done up to now.\n",
    "To do this, we'll need to learn some new techniques that will help us.\n",
    "You may be wondering when happens when a prefix is found in both positive and negative texts; wait and see.\n",
    "\n",
    "Let's start from how to collect the intermediate states from an RNN.\n",
    "What you do is, for every time step in the `for` loop, collect each matrix of states (each row being from a different text in the batch) into a list, and then join them all together in a 3D tensor after the loop ends.\n",
    "If you think this is very inefficient, it's actually what PyTorch suggests in their [example code](https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html) (see bottom of web page).\n",
    "Note that we will not be masking out the pad tokens just yet because that's something that we'll do later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 4\n",
    "state_size = 5\n",
    "\n",
    "x_indexed = torch.tensor([\n",
    "    [1, 0, 0],\n",
    "    [1, 2, 0],\n",
    "], dtype=torch.int64, device=device)\n",
    "pad_index = 0\n",
    "print('x_indexed:')\n",
    "print(x_indexed)\n",
    "print()\n",
    "\n",
    "batch_size = x_indexed.shape[0]\n",
    "time_steps = x_indexed.shape[1]\n",
    "\n",
    "embedding = torch.nn.Embedding(3, embedding_size)\n",
    "embedding.to(device)\n",
    "embedded = embedding(x_indexed)\n",
    "print('embedded:')\n",
    "print(embedded)\n",
    "print()\n",
    "\n",
    "rnn = torch.nn.LSTMCell(embedding_size, state_size)\n",
    "rnn.to(device)\n",
    "s0 = torch.zeros((state_size,), dtype=torch.float32, device=device)\n",
    "c0 = torch.zeros((state_size,), dtype=torch.float32, device=device)\n",
    "\n",
    "state = s0[None, :].tile((batch_size, 1))\n",
    "c = c0[None, :].tile((batch_size, 1))\n",
    "interm_states_list = []\n",
    "for t in range(time_steps):\n",
    "    (state, c) = rnn(embedded[:, t, :], (state, c))\n",
    "    interm_states_list.append(state)\n",
    "\n",
    "for (t, state) in enumerate(interm_states_list, start=1):\n",
    "    print('state at time', t)\n",
    "    print(state)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To join all of these states into a 3D tensor (number of texts by numbers of state vectors by state vector size) we need to use the `stack` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interm_states = torch.stack(interm_states_list, dim=1)\n",
    "print(interm_states.shape)\n",
    "print(interm_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next problem we have is how to apply the output layer on each one of these states.\n",
    "You might think that you cannot apply a linear layer on a 3D tensor; thankfully this is not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs = 1\n",
    "\n",
    "layer = torch.nn.Linear(state_size, num_outputs)\n",
    "layer.to(device)\n",
    "\n",
    "logits = layer(interm_states)\n",
    "print('logits for 2 texts by 4 state vectors each')\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the matrix multiplication operator (`@`) let's you multiply a 3D tensor by a matrix and it will automatically multiply each vector in the 3D tensor by the weight matrix and preserve the shape of the resulting tensor.\n",
    "\n",
    "Finally we need to think about how to compute the error.\n",
    "We can calculate the cross-entropy of all the logits in the 3D tensor by comparing it with a 3D tensor of target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = torch.tensor([\n",
    "    [[1], [1], [0]],\n",
    "    [[0], [1], [0]],\n",
    "], dtype=torch.float32, device=device)\n",
    "print('train_y')\n",
    "print(train_y)\n",
    "print()\n",
    "\n",
    "error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "print('error')\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is ignoring the fact that we'll have pad tokens in our inputs, which should be ignored.\n",
    "To do this we'll need to do several things.\n",
    "\n",
    "First, we need to calculate the individual cross entropies of all logits.\n",
    "The cross entropy function, by default, calculates the mean of the cross entropy of each logit value.\n",
    "You can make it give you individual cross entropies using the parameter `reduction='none'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_errors = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y, reduction='none')\n",
    "print(token_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to mask out all the errors that are from pad tokens such that they are replaced by zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mask = x_indexed == pad_index\n",
    "\n",
    "print('pad_mask:')\n",
    "print(pad_mask)\n",
    "print()\n",
    "\n",
    "print('masked errors')\n",
    "token_errors = token_errors.masked_fill(pad_mask[:, :, None], 0.0)\n",
    "print(token_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Finally we need to calculate the average error of the unmasked errors.\n",
    "You can't just use the `torch.mean` function because that will include the zeroed out errors as well.\n",
    "Instead, we'll compute it ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = token_errors.sum()/(~pad_mask).sum() # Total error divided by the total number of non-pad tokens.\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `~` is for complementing/negating a mask so that what's true becomes false and what's false becomes true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('pad_mask:')\n",
    "print(pad_mask)\n",
    "print()\n",
    "print('~pad_mask:')\n",
    "print(~pad_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put it all together to make a neural network that tries to classify the sentiment of each prefix of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [\n",
    "    'I like it .'.split(' '),\n",
    "    'I hate it .'.split(' '),\n",
    "    'I don\\'t hate it .'.split(' '),\n",
    "    'I don\\'t like it .'.split(' '),\n",
    "]\n",
    "train_y = torch.tensor([\n",
    "    [1],\n",
    "    [0],\n",
    "    [1],\n",
    "    [0],\n",
    "], dtype=torch.float32, device=device)\n",
    "\n",
    "max_len = max(len(text) for text in train_x)\n",
    "print('max_len:', max_len)\n",
    "\n",
    "vocab = ['<PAD>'] + sorted({token for text in train_x for token in text})\n",
    "token2index = {t: i for (i, t) in enumerate(vocab)}\n",
    "pad_index = token2index['<PAD>']\n",
    "print('vocab:', vocab)\n",
    "print()\n",
    "\n",
    "train_x_indexed_np = np.full((len(train_x), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_x)):\n",
    "    for j in range(len(train_x[i])):\n",
    "        train_x_indexed_np[i, j] = token2index[train_x[i][j]]\n",
    "train_x_indexed = torch.tensor(train_x_indexed_np, device=device)\n",
    "\n",
    "# The target value of each text will be replicated for each prefix (number of prefixes == number of tokens).\n",
    "train_y_seq = train_y[:, None, :].tile((1, max_len, 1))\n",
    "print('train_y_seq:')\n",
    "print(train_y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, state_size):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_s0 = torch.nn.Parameter(torch.zeros((state_size,), dtype=torch.float32))\n",
    "        self.rnn_c0 = torch.nn.Parameter(torch.zeros((state_size,), dtype=torch.float32))\n",
    "        self.rnn_cell = torch.nn.LSTMCell(embedding_size, state_size)\n",
    "        self.output_layer = torch.nn.Linear(state_size, 1)\n",
    "\n",
    "    def forward(self, x_indexed):\n",
    "        batch_size = x_indexed.shape[0]\n",
    "        time_steps = x_indexed.shape[1]\n",
    "\n",
    "        embedded = self.embedding(x_indexed)\n",
    "        state = self.rnn_s0[None, :].tile((batch_size, 1))\n",
    "        c = self.rnn_s0[None, :].tile((batch_size, 1))\n",
    "        interm_states_list = []\n",
    "        for t in range(time_steps):\n",
    "            # No need to mask anything here, because we'll be masking the output at the end.\n",
    "            (state, c) = self.rnn_cell(embedded[:, t, :], (state, c))\n",
    "            interm_states_list.append(state)\n",
    "        interm_states = torch.stack(interm_states_list, dim=1)\n",
    "        return self.output_layer(interm_states)\n",
    "\n",
    "model = Model(len(vocab), embedding_size=2, state_size=2)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 1000+1):\n",
    "    pad_mask = train_x_indexed == pad_index\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x_indexed)\n",
    "    train_token_errors = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y_seq, reduction='none')\n",
    "    train_token_errors = train_token_errors.masked_fill(pad_mask[:, :, None], 0.0)\n",
    "    train_error = train_token_errors.sum()/(~pad_mask).sum()\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('text', 'output')\n",
    "    output = torch.sigmoid(model(train_x_indexed))[:, :, 0].cpu().tolist()\n",
    "    for (text, y) in zip(train_x, output):\n",
    "        print(text + ['<PAD>']*(max_len - len(text)), y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above outputs, we're seeing the sentiment prediction for each prefix starting from the first token.\n",
    "So we're seeing how the neural network has learned to classify \"I\", \"I like\", \"I like it\", \"I like it .\", and \"I like it . \\<PAD>\" for example.\n",
    "We can see how the first prefix (\"I\") is always close to 0.5, because it is equally likely to be in a positive or a negative text.\n",
    "As soon as there is a prefix that is unique to a text category, the neural network gives that prefix and all the following prefixes the correct output.\n",
    "\n",
    "How did the neural network give an output of 0.5 for the ambiguous prefixes if it wasn't trained to do so?\n",
    "The number comes from the fact that it is what gives the smallest training error when you have to give both an output of 0 and an output of 1 for the same input (it's the closest single output to both desired outputs).\n",
    "If a particular prefix was in 2 training set texts with a target output of 1 and 1 text with a target of 0, its prediction would instead settle on $\\frac{2}{2 + 1} = \\frac{2}{3} \\approx 0.67$.\n",
    "In general, if a prefix is in $a$ training set texts with a target output of 1 and $b$ texts with a target of 0, a sigmoid output will settle somewhere close to $\\frac{a}{a+b}$.\n",
    "Remember this for the next topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-directional RNNs\n",
    "\n",
    "Intermediates are not only useful for making predictions about prefixes (although that is a big use for them, as we'll see in the next topic).\n",
    "You can also use them to make **bi-directional RNNs**.\n",
    "For tasks that require information about the full context of a token rather than just its preceding tokens (prefix), such as for part-of-speech tagging, the intermediate states of a single RNN are not enough as that will only tell us what is on one side of a token.\n",
    "We need to have information about both the tokens before the token being tagged as well as the tokens that come after.\n",
    "For this purpose we need two RNNs: one that goes forward in the text and one that goes backward, hence bi-directional.\n",
    "We call the forward RNN 'fw' and the backward RNN 'bw' in short.\n",
    "\n",
    "![](bidirectional_rnn.png)\n",
    "\n",
    "By concatenating the two sets of intermediate states, as shown at the bottom of the diagram, each token will be represented by information about all the tokens in front and behind it.\n",
    "\n",
    "Unfortunately we can't just run the `for` loop backwards on the sequences for the bw RNN because that would mean having it start with the pad tokens which means that the pad tokens would influence the representation.\n",
    "So for the bw RNN we'll need to use the masking technique we used in the previous topic so that the initial state remains as the current state until the first non-pad token is encountered.\n",
    "\n",
    "Also, we'll need to reverse the bw intermediate states before concatenating them to the fw intermediate states (see the above diagram).\n",
    "Since the list of intermediate states is a normal Python list (before being stacked into a single tensor), we can reverse the list easily as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(['a', 'b', 'c', 'd'][::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reversing the list it can then be stacked into a single 3D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, state_size, pad_index):\n",
    "        super().__init__()\n",
    "        self.pad_index = pad_index\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_fw_s0 = torch.nn.Parameter(torch.zeros((state_size,), dtype=torch.float32))\n",
    "        self.rnn_fw_c0 = torch.nn.Parameter(torch.zeros((state_size,), dtype=torch.float32))\n",
    "        self.rnn_fw_cell = torch.nn.LSTMCell(embedding_size, state_size)\n",
    "        self.rnn_bw_s0 = torch.nn.Parameter(torch.zeros((state_size,), dtype=torch.float32))\n",
    "        self.rnn_bw_c0 = torch.nn.Parameter(torch.zeros((state_size,), dtype=torch.float32))\n",
    "        self.rnn_bw_cell = torch.nn.LSTMCell(embedding_size, state_size)\n",
    "        self.output_layer = torch.nn.Linear(2*state_size, 1) # The input to this layer will be the fw and bw states concatenated together.\n",
    "\n",
    "    def forward(self, x_indexed):\n",
    "        batch_size = x_indexed.shape[0]\n",
    "        time_steps = x_indexed.shape[1]\n",
    "        non_pad_mask = x_indexed != pad_index\n",
    "\n",
    "        embedded = self.embedding(x_indexed)\n",
    "        \n",
    "        state = self.rnn_fw_s0[None, :].tile((batch_size, 1))\n",
    "        c = self.rnn_fw_c0[None, :].tile((batch_size, 1))\n",
    "        interm_states_list = []\n",
    "        for t in range(time_steps):\n",
    "            (state, c) = self.rnn_fw_cell(embedded[:, t, :], (state, c))\n",
    "            interm_states_list.append(state)\n",
    "        interm_states_fw = torch.stack(interm_states_list, dim=1)\n",
    "\n",
    "        state = self.rnn_bw_s0[None, :].tile((batch_size, 1))\n",
    "        c = self.rnn_bw_c0[None, :].tile((batch_size, 1))\n",
    "        interm_states_list = []\n",
    "        for t in reversed(range(time_steps)): # Go backwards from time_steps-1 to 0.\n",
    "            (new_state, new_c) = self.rnn_bw_cell(embedded[:, t, :], (state, c))\n",
    "            state = torch.where(non_pad_mask[:, t, None], new_state, state)\n",
    "            c = torch.where(non_pad_mask[:, t, None], new_c, c) # We also need to mask the cell state now because it also needs to remain as the initial state while we're on a pad token.\n",
    "            interm_states_list.append(state)\n",
    "        interm_states_bw = torch.stack(interm_states_list[::-1], dim=1) # Reverse the bw intermediate states.\n",
    "\n",
    "        interm_states = torch.concat((interm_states_fw, interm_states_bw), dim=2)\n",
    "\n",
    "        return self.output_layer(interm_states)\n",
    "\n",
    "model = Model(len(vocab), embedding_size=2, state_size=2, pad_index=pad_index)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 1000+1):\n",
    "    pad_mask = train_x_indexed == pad_index\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x_indexed)\n",
    "    train_token_errors = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y_seq, reduction='none')\n",
    "    train_token_errors = train_token_errors.masked_fill(pad_mask[:, :, None], 0.0)\n",
    "    train_error = train_token_errors.sum()/(~pad_mask).sum()\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('text', 'output')\n",
    "    output = torch.sigmoid(model(train_x_indexed))[:, :, 0].cpu().tolist()\n",
    "    for (text, y) in zip(train_x, output):\n",
    "        print(text + ['<PAD>']*(max_len - len(text)), y)\n",
    "        \n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are even the previously ambiguous prefixes getting classified correctly now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Vote-based classification\n",
    "\n",
    "Rewrite the movie reviews classification program as follows:\n",
    "\n",
    "* Use a bi-directional RNN to classify each token in the texts as shown above.\n",
    "* *When measuring the accuracy on the test set*, convert all the individual token classifications into a single classification by taking the average of the token-level predictions (this is a form of classifier voting or **ensembling** meant to improve the performance).\n",
    "    Do not do this while training!\n",
    "\n",
    "Preprocessing has been done for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 3\n",
    "\n",
    "train_df = pd.read_csv('../data_set/sentiment/train.csv')\n",
    "test_df = pd.read_csv('../data_set/sentiment/test.csv')\n",
    "\n",
    "train_x = train_df['text']\n",
    "train_y = train_df['class']\n",
    "test_x = test_df['text']\n",
    "test_y = test_df['class']\n",
    "categories = ['neg', 'pos']\n",
    "cat2idx = {cat: i for (i, cat) in enumerate(categories)}\n",
    "\n",
    "train_y_indexed = torch.tensor(\n",
    "    train_y.map(cat2idx.get).to_numpy()[:, None],\n",
    "    dtype=torch.float32, device=device\n",
    ")\n",
    "test_y_indexed = test_y.map(cat2idx.get).to_numpy()[:, None]\n",
    "\n",
    "nltk.download('punkt')\n",
    "train_x_tokens = [nltk.word_tokenize(text) for text in train_x]\n",
    "test_x_tokens = [nltk.word_tokenize(text) for text in test_x]\n",
    "max_len = max(max(len(text) for text in train_x_tokens), max(len(text) for text in test_x_tokens))\n",
    "\n",
    "frequencies = collections.Counter(token for text in train_x_tokens for token in text)\n",
    "vocabulary = sorted(frequencies.keys(), key=frequencies.get, reverse=True)\n",
    "while frequencies[vocabulary[-1]] < min_freq:\n",
    "    vocabulary.pop()\n",
    "vocab = ['<PAD>', '<UNK>'] + vocabulary\n",
    "token2index = {token: i for (i, token) in enumerate(vocab)}\n",
    "pad_index = token2index['<PAD>']\n",
    "unk_index = token2index['<UNK>']\n",
    "\n",
    "train_x_indexed_np = np.full((len(train_x_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_x_tokens)):\n",
    "    for j in range(len(train_x_tokens[i])):\n",
    "        train_x_indexed_np[i, j] = token2index.get(train_x_tokens[i][j], unk_index)\n",
    "train_x_indexed = torch.tensor(train_x_indexed_np, device=device)\n",
    "\n",
    "test_x_indexed_np = np.full((len(test_x_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(test_x_tokens)):\n",
    "    for j in range(len(test_x_tokens[i])):\n",
    "        test_x_indexed_np[i, j] = token2index.get(test_x_tokens[i][j], unk_index)\n",
    "test_x_indexed = torch.tensor(test_x_indexed_np, device=device)\n",
    "\n",
    "train_y_indexed_seq = train_y_indexed[:, None, :].tile([1, max_len, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75ee2b71ad44bf9ef4e9bee896f68ffbc764a6a2c6d1f57c86c48f99ffc25ca8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
