{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "\n",
    "Up to now we've represented sentences as bag of words vectors, which is fine if you don't care about the sentence structure and only care about the words used, but when you do care about sentence structure, you need to use a different approach.\n",
    "\n",
    "Note that a word is a linguistic object whilst in NLP we talk about **tokens** instead.\n",
    "A text is broken up into a sequence of tokens, which are strings in the text that are not further decomposed.\n",
    "They can be of any complexity you choose such as whole words, parts of words, characters, or even bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot vectors\n",
    "\n",
    "Let's start by looking at just one token before thinking about entire sentences.\n",
    "In the context of inputs to neural networks, how can you refer to a single element out of a finite set of elements?\n",
    "Remember that all inputs to a neural network must be numeric.\n",
    "\n",
    "One way would be by using a single index number; for example 0 would refer to the first element in the set, 1 would refer to the second, and so on.\n",
    "The problem would this approach is that:\n",
    "\n",
    "1. Neural networks don't work well with large and varied inputs and you should keep your inputs close to zero.\n",
    "1. It's implying to the neural network that the first element is more similar to the second than it is to the third, since the numbers are closer.\n",
    "\n",
    "A better way is to do something similar to the bag-of-words vector, but with just one word, that is, a vector of 0s with a single 1 somewhere depending on which token is used.\n",
    "This is called a one-hot vector.\n",
    "So the first token in a vocabulary of 3 is represented as `[1, 0, 0]` whilst the third token in a vocabulary of 5 is represented as `[0, 0, 1, 0, 0]`.\n",
    "The nice thing about one-hot vectors is that you can easily generate them using an identity matrix, that is, a square matrix consisting of zeros except for the diagonal which consists of ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_matrix = np.eye(4)\n",
    "print(identity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given vocabulary size, which would correspond to the length of identity matrix's side, you can get any one-hot vector for any index from the identity matrix by picking a row from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(identity_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even a sequence of one-hot vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hots = identity_matrix[[2, 2, 0]]\n",
    "print(one_hots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last part is done to get a sentence of tokens: by representing each token in the sentence as separate vectors that are put together in a matrix.\n",
    "The simplest way to feed such a sentence into a neural network is to **flatten** the matrix of one-hot vectors into a single vector.\n",
    "This can be done by **reshaping** the 3x4-matrix into a 12-vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = one_hots.shape[0]\n",
    "vec_size = one_hots.shape[1]\n",
    "print(one_hots.reshape([num_tokens*vec_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's consider a fixed vocabulary consisting of `['cat', 'dog', 'bit', 'scratched', 'the']`.\n",
    "The sentence 'the dog bit the cat' would be represented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['cat', 'dog', 'bit', 'scratched', 'the']\n",
    "identity_matrix = np.eye(len(vocab))\n",
    "tokens = 'the dog bit the cat'.split(' ')\n",
    "\n",
    "print('tokens:', tokens)\n",
    "\n",
    "indexes = [vocab.index(token) for token in tokens]\n",
    "print('indexes:', indexes)\n",
    "print()\n",
    "\n",
    "one_hots = identity_matrix[indexes]\n",
    "print('one-hots:')\n",
    "print(one_hots)\n",
    "print()\n",
    "\n",
    "print('flattened one-hots:')\n",
    "num_tokens = one_hots.shape[0]\n",
    "print(one_hots.reshape([num_tokens*len(vocab)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already shown in the previous exercises, the vocabulary is chosen from the tokens used in the training set.\n",
    "Either the $v$ most frequent tokens in the training set are used, where $v$ is a chosen vocabulary size, or all the tokens that appear at least $f$ times in the training set, where $f$ is a chosen minimum frequency.\n",
    "When not using bags of words, you don't ignore stop words because those are important for understanding the structure of a sentence.\n",
    "The reason why we use the most frequent tokens in the training set as a vocabulary is because, the more times a neural network sees a token being used in the training set, the more likely it will learn how it's used.\n",
    "A token that is only used once will not allow the neural network to understand the kinds of contexts it typically appears in, so it might as well be ignored.\n",
    "\n",
    "But then, what do we do with the tokens that are not in the vocabulary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special tokens\n",
    "\n",
    "Your vocabulary will not consist of just strings extracted from the text but will also include made-up tokens that exist only for our convenience.\n",
    "One example of this is the **unknown token**, also known as the **out-of-vocabulary token** (**OOV**), used in place of any token that is not in the vocabulary.\n",
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['<UNK>', 'cat', 'dog', 'bit', 'scratched', 'the']\n",
    "identity_matrix = np.eye(len(vocab))\n",
    "tokens = 'the handsome dog bit the beautiful cat'.split(' ')\n",
    "\n",
    "print('tokens:', tokens)\n",
    "\n",
    "cleaned_tokens = [token if token in vocab else '<UNK>' for token in tokens]\n",
    "\n",
    "print('cleaned tokens:', cleaned_tokens)\n",
    "\n",
    "indexes = [vocab.index(token) for token in cleaned_tokens]\n",
    "print('indexes:', indexes)\n",
    "print()\n",
    "\n",
    "one_hots = identity_matrix[indexes]\n",
    "print('one-hots:')\n",
    "print(one_hots)\n",
    "print()\n",
    "\n",
    "print('flattened one-hots:')\n",
    "num_tokens = one_hots.shape[0]\n",
    "print(one_hots.reshape([num_tokens*len(vocab)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the unknown token to be useful, it must be used in the training set as well, which means that you can't have all the tokens in the training set be in the vocabulary, or else your neural network will not learn how to use it.\n",
    "\n",
    "Another special token you'll typically need is the **pad token**, a token that means 'no token'.\n",
    "The reason you'll need this is because your neural network is going to expect a certain number of tokens as input.\n",
    "The flattened vector representing the text needs to be a certain size, and that size is the number of tokens times the one-hot vector size.\n",
    "Changing the number of tokens will change the flattened vector size, and a neural network can only have a fixed input vector size.\n",
    "This is because the input vector is going to be multiplied with the weight matrix of the first layer, which is only compatible with a certain size of vector.\n",
    "So if the number of tokens in your text is going to be shorter than this size, then you'll need to fill the missing tokens with something (there's nothing to do about having too many tokens other than drop some of them).\n",
    "So we use the pad token to add extra tokens to the end of the sentence (or the beginning, just be consistent) in order to make it have the expected number of tokens, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_len = 10\n",
    "vocab = ['<PAD>', '<UNK>', 'cat', 'dog', 'bit', 'scratched', 'the']\n",
    "identity_matrix = np.eye(len(vocab))\n",
    "tokens = 'the handsome dog bit the beautiful cat'.split(' ')\n",
    "\n",
    "print('tokens:', tokens)\n",
    "\n",
    "cleaned_tokens = [token if token in vocab else '<UNK>' for token in tokens]\n",
    "\n",
    "print('cleaned tokens:', cleaned_tokens)\n",
    "\n",
    "print('expected number of tokens:', expected_len)\n",
    "\n",
    "padded_tokens = cleaned_tokens + ['<PAD>']*(expected_len - len(cleaned_tokens))\n",
    "\n",
    "print('padded tokens:', padded_tokens)\n",
    "\n",
    "indexes = [vocab.index(token) for token in padded_tokens]\n",
    "print('indexes:', indexes)\n",
    "print()\n",
    "\n",
    "one_hots = identity_matrix[indexes]\n",
    "print('one-hots:')\n",
    "print(one_hots)\n",
    "print()\n",
    "\n",
    "print('flattened one-hots:')\n",
    "num_tokens = one_hots.shape[0]\n",
    "print(one_hots.reshape([num_tokens*len(vocab)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can have everything we need to process a text of tokens with a neural network!\n",
    "Although we still need to be able to accept a batch of texts rather than a single text at a time.\n",
    "Let's see an example in PyTorch.\n",
    "\n",
    "Let's try making a toy neural network perform sentiment analysis (recognise if a sentence is speaking positively about something).\n",
    "We'll use a very simple set of sentences constructed in such a way that it is impossible to predict the answer by just looking for a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [\n",
    "    'I like it .'.split(' '),\n",
    "    'I hate it .'.split(' '),\n",
    "    'I don\\'t hate it .'.split(' '),\n",
    "    'I don\\'t like it .'.split(' '),\n",
    "]\n",
    "train_y = torch.tensor([\n",
    "    [1],\n",
    "    [0],\n",
    "    [1],\n",
    "    [0],\n",
    "], dtype=torch.float32, device=device)\n",
    "\n",
    "max_len = max(len(text) for text in train_x)\n",
    "print('max_len:', max_len)\n",
    "\n",
    "vocab = ['<PAD>'] + sorted({token for text in train_x for token in text})\n",
    "print('vocab:', vocab)\n",
    "print()\n",
    "\n",
    "identity_matrix = np.eye(len(vocab))\n",
    "\n",
    "train_x_padded = [text + ['<PAD>']*(max_len - len(text)) for text in train_x]\n",
    "print('train_x_padded:')\n",
    "print(train_x_padded)\n",
    "print()\n",
    "\n",
    "train_x_indexed = np.array([[vocab.index(token) for token in text] for text in train_x_padded], np.int64)\n",
    "print('train_x_indexed:')\n",
    "print(train_x_indexed)\n",
    "print()\n",
    "\n",
    "train_x_one_hots = torch.tensor(identity_matrix[train_x_indexed], dtype=torch.float32, device=device)\n",
    "print('train_x_one_hots:')\n",
    "print(train_x_one_hots)\n",
    "print()\n",
    "\n",
    "print('train_x_flattened:')\n",
    "train_x_flattened = torch.flatten(train_x_one_hots, 1, 2) # The first dimension Flatten from the second dimension to the third dimension of the 3D tensor.\n",
    "print(train_x_flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the use of batches of texts rather than a single text makes `train_x_one_hots` a three-dimensional tensor.\n",
    "It's filled with the one-hot vectors (1D) of 5 tokens (2D) of 4 texts (3D), as shown below:\n",
    "\n",
    "![](3d_tensor_tokens.png)\n",
    "\n",
    "The 3D tensor was then flattened into a 2D tensor by merging all the one-hot vectors of the same text together into a single vector.\n",
    "This 2D tensor is what the neural network will see as an input.\n",
    "We used PyTorch's `flatten` instead of `reshape` because it is easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, expected_len, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(vocab_size*expected_len, hidden_size)\n",
    "        self.layer2 = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.nn.functional.leaky_relu(self.layer1(x), 0.1)\n",
    "        return self.layer2(hidden)\n",
    "\n",
    "model = Model(len(vocab), max_len, hidden_size=2)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "print('epoch', 'error')\n",
    "errors = []\n",
    "for epoch in range(1, 100+1):\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x_flattened)\n",
    "    error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    errors.append(error.detach().cpu().tolist())\n",
    "    error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch, errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('text', 'prediction')\n",
    "    output = torch.sigmoid(model(train_x_flattened))[:, 0].cpu().numpy()\n",
    "    for (text, y) in zip(train_x, output):\n",
    "        print(text, y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(errors) + 1), errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The embedding matrix\n",
    "\n",
    "One-hot vectors, while usable, are a huge waste of space.\n",
    "If your vocabulary is a thousand tokens long, you're keeping 1000 numbers in memory just to specify a single token.\n",
    "Keep in mind that vocabularies with tens of thousands of tokens are normal nowadays.\n",
    "Can we do better?\n",
    "\n",
    "It turns out that we don't really need one-hot vectors at all.\n",
    "All we need is a way to represent tokens as vectors.\n",
    "One-hot vectors are called **sparse vectors**, because they are big and contain very little information.\n",
    "Alternatively we can create **dense vectors**, which are small vectors that contain a lot of information.\n",
    "These dense token vectors, referred to as **word embeddings**, will consist of a lot of random-looking small numbers that can be both positive and negative.\n",
    "They are not directly interpretable like one-hot vectors, but they work really well.\n",
    "\n",
    "All we need to do is replace the first layer with a single matrix called an **embedding matrix** and the act of turning tokens into vectors is called embedding the tokens into vector space.\n",
    "The embedding matrix will have a number of rows equal to the vocabulary size and a number of columns equal to the hidden layer size.\n",
    "\n",
    "Let's see how this is done in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4\n",
    "hidden_size = 2\n",
    "\n",
    "embedding_matrix = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "embedding_matrix.to(device)\n",
    "print('embedding_matrix:')\n",
    "print(embedding_matrix.weight.data)\n",
    "print()\n",
    "\n",
    "indexed_tokens = torch.tensor([[0, 1, 2], [0, 3, 3]], dtype=torch.int64, device=device)\n",
    "print('indexed_tokens (two texts of three tokens each):')\n",
    "print(indexed_tokens)\n",
    "print()\n",
    "\n",
    "embedded = embedding_matrix(indexed_tokens)\n",
    "print('embedded tokens:')\n",
    "print(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a 3D tensor, just like with one-hot vectors, but the vector size can now be much smaller (just 2 numbers here).\n",
    "\n",
    "`torch.nn.Embedding` will now replace `torch.nn.Linear` as the first layer.\n",
    "Its job is to replace token indexes with token vectors.\n",
    "The numbers in this matrix are optimised in the same way as an other parameter in the neural network.\n",
    "Note that what is being optimised are the rows in the matrix that correspond to tokens used in the training set.\n",
    "In fact, if a token in the vocabulary is never used in the training set, the row corresponding to that token will not be changed at all during training.\n",
    "\n",
    "We can now flatten the resulting 3D tensor into a 2D one where each text is a flat vector, no need for any other operations in the first layer.\n",
    "We don't need to use an activation function for this layer because the token vectors are all learned independently from each other and are free to move into any non-linear configuration they need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = torch.flatten(embedded, 1, 2)\n",
    "print('flattened:')\n",
    "print(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the resulting flattened vectors are now much smaller than the flattened one-hot vectors, we have significantly reduced the size of our model, which will make training faster.\n",
    "\n",
    "Let's use this embedding matrix in the previous toy neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [\n",
    "    'I like it .'.split(' '),\n",
    "    'I hate it .'.split(' '),\n",
    "    'I don\\'t hate it .'.split(' '),\n",
    "    'I don\\'t like it .'.split(' '),\n",
    "]\n",
    "train_y = torch.tensor([\n",
    "    [1],\n",
    "    [0],\n",
    "    [1],\n",
    "    [0],\n",
    "], dtype=torch.float32, device=device)\n",
    "\n",
    "max_len = max(len(text) for text in train_x)\n",
    "\n",
    "vocab = ['<PAD>'] + sorted({token for text in train_x for token in text})\n",
    "\n",
    "train_x_padded = [text + ['<PAD>']*(max_len - len(text)) for text in train_x]\n",
    "\n",
    "train_x_indexed = torch.tensor([[vocab.index(token) for token in text] for text in train_x_padded], dtype=torch.int64, device=device)\n",
    "print('train_x_indexed:')\n",
    "print(train_x_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, max_len, embed_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Embedding(vocab_size, embed_size) # Using Embedding instead of Linear.\n",
    "        self.layer2 = torch.nn.Linear(max_len*embed_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.layer1(x)\n",
    "        flattened = torch.flatten(hidden, 1, 2)\n",
    "        return self.layer2(flattened)\n",
    "\n",
    "model = Model(len(vocab), max_len, embed_size=2)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 100+1):\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x_indexed)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('text', 'output')\n",
    "    output = torch.sigmoid(model(train_x_indexed))[:, 0].cpu().tolist()\n",
    "    for (text, y) in zip(train_x, output):\n",
    "        print(text, y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(errors) + 1), errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we're processing sentences more efficiently.\n",
    "Of course this method still isn't ideal because it assumes that there is a maximum length to a sentence and the model parameters must still grow with the sentence length (the weight matrix of the output layer needs to grow with the number of tokens).\n",
    "Ideally there would be a way to process a sentence by looking at a few tokens at a time instead of having the entire sentence processed at once, just like we do when reading.\n",
    "We'll look at better ways to process sentences with neural networks in the following topics.\n",
    "\n",
    "Finally, here is a more efficient way to convert tokens into indexes (which we'll refer to as **indexifying** the tokens).\n",
    "It is faster because it makes use of a dictionary to look up the index of tokens and also avoids inserting pad tokens.\n",
    "We also avoid putting everything immediately in the PyTorch tensor in order to allow for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [\n",
    "    'I like it .'.split(' '),\n",
    "    'I hate it .'.split(' '),\n",
    "    'I don\\'t hate it .'.split(' '),\n",
    "    'I don\\'t like it .'.split(' '),\n",
    "]\n",
    "\n",
    "max_len = max(len(text) for text in train_x)\n",
    "\n",
    "vocab = ['<PAD>', '<UNK>'] + sorted({token for text in train_x for token in text})\n",
    "token2index = {t: i for (i, t) in enumerate(vocab)} # Use a dictionary to find the index of tokens.\n",
    "pad_index = token2index['<PAD>']\n",
    "unk_index = token2index['<UNK>']\n",
    "\n",
    "train_x_indexed_np = np.full((len(train_x), max_len), pad_index, np.int64) # Make the indexes tensor full of pad indexes.\n",
    "for i in range(len(train_x)):\n",
    "    for j in range(len(train_x[i])):\n",
    "        train_x_indexed_np[i, j] = token2index.get(train_x[i][j], unk_index) # Use 'get' to get the token if it's in the vocabulary and return a default value if it isn't (the unknown token).\n",
    "train_x_indexed = torch.tensor(train_x_indexed_np, device=device)\n",
    "print('train_x_indexed:')\n",
    "print(train_x_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Using embeddings\n",
    "\n",
    "Rewrite the movie reviews classification program from the previous topic using full texts.\n",
    "Preprocessing has been done for you.\n",
    "Don't forget to calculate the test set accuracy after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 3 # The minimum number of times a token must occur in the training set for it to be included in the vocabulary.\n",
    "\n",
    "train_df = pd.read_csv('../data_set/sentiment/train.csv')\n",
    "test_df = pd.read_csv('../data_set/sentiment/test.csv')\n",
    "\n",
    "train_x = train_df['text']\n",
    "train_y = train_df['class']\n",
    "test_x = test_df['text']\n",
    "test_y = test_df['class']\n",
    "categories = ['neg', 'pos'] # neg -> 0, pos -> 1\n",
    "cat2idx = {cat: i for (i, cat) in enumerate(categories)}\n",
    "\n",
    "train_y_indexed = torch.tensor(\n",
    "    train_y.map(cat2idx.get).to_numpy()[:, None], # Make the binary labels be a single column matrix.\n",
    "    dtype=torch.float32, device=device\n",
    ")\n",
    "test_y_indexed = test_y.map(cat2idx.get).to_numpy()[:, None]\n",
    "\n",
    "nltk.download('punkt')\n",
    "train_x_tokens = [nltk.word_tokenize(text) for text in train_x]\n",
    "test_x_tokens = [nltk.word_tokenize(text) for text in test_x]\n",
    "max_len = max(max(len(text) for text in train_x_tokens), max(len(text) for text in test_x_tokens)) # Get the maximum length from both the training set and testing set.\n",
    "\n",
    "print('First train_x_tokens:')\n",
    "print(train_x_tokens[0])\n",
    "print()\n",
    "\n",
    "frequencies = collections.Counter(token for text in train_x_tokens for token in text)\n",
    "vocabulary = sorted(frequencies.keys(), key=frequencies.get, reverse=True)\n",
    "while frequencies[vocabulary[-1]] < min_freq:\n",
    "    vocabulary.pop()\n",
    "vocab = ['<PAD>', '<UNK>'] + vocabulary\n",
    "token2index = {token: i for (i, token) in enumerate(vocab)}\n",
    "pad_index = token2index['<PAD>']\n",
    "unk_index = token2index['<UNK>']\n",
    "\n",
    "print('First 10 vocabulary items:')\n",
    "print(vocab[:10])\n",
    "print()\n",
    "\n",
    "train_x_indexed_np = np.full((len(train_x_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_x_tokens)):\n",
    "    for j in range(len(train_x_tokens[i])):\n",
    "        train_x_indexed_np[i, j] = token2index.get(train_x_tokens[i][j], unk_index)\n",
    "train_x_indexed = torch.tensor(train_x_indexed_np, device=device)\n",
    "\n",
    "test_x_indexed_np = np.full((len(test_x_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(test_x_tokens)):\n",
    "    for j in range(len(test_x_tokens[i])):\n",
    "        test_x_indexed_np[i, j] = token2index.get(test_x_tokens[i][j], unk_index)\n",
    "test_x_indexed = torch.tensor(test_x_indexed_np, device=device)\n",
    "\n",
    "print('First train_x_indexed:')\n",
    "print(train_x_indexed[0])\n",
    "print()\n",
    "\n",
    "print('First train_y_indexed:')\n",
    "print(train_y_indexed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75ee2b71ad44bf9ef4e9bee896f68ffbc764a6a2c6d1f57c86c48f99ffc25ca8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
