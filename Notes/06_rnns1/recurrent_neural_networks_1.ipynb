{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks 1\n",
    "\n",
    "Pooling operations allow us to reduce a sequence of vectors into a single vector, which then allows the neural network to work with any length sequence.\n",
    "However, this does not preserve a lot of information about the sequence and it requires using multiple convolution operations in parallel.\n",
    "In order to compact as much information about the sequence as possible in a single vector, we'll need to use a special type of neural architecture called a **recurrent neural network** (**RNN**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A neural network with a memory\n",
    "\n",
    "Our neural networks up to now did not have a memory as they only operate on what they are currently seeing, not on what they saw earlier.\n",
    "The simplest way to add a memory, called a **state**, is to make the neural network give an output that will be used as input the next time it is used.\n",
    "This is the basic building block of the **simple recurrent neural network**:\n",
    "\n",
    "![](rnn_cell.png)\n",
    "\n",
    "The above diagram is illustrating a single layer neural network (in red), called an **RNN cell**, taking in a state vector and an input vector in order to produce a new state vector.\n",
    "The $t$ is used to indicate time, where $t+1$ happens one step after $t$.\n",
    "An example of an input vector is a token vector.\n",
    "$s_{t+1}$ is going to be a vector of the same size as $s_t$ and it is a function of both the previous state and the new input.\n",
    "The state and input vectors are usually just concatenated together before they are fed to the single layer neural network.\n",
    "\n",
    "We can create a chain of these cells where the state vector that comes out of one cell is fed into the next cell like this:\n",
    "\n",
    "![](rnn_chain.png)\n",
    "\n",
    "This chain is consuming three separate inputs in three time steps.\n",
    "Each one of the red layers is using the exact same parameters (same weight matrix and bias vector), so the neural network size does not change as more inputs are consumed because it's the same one used multiple times.\n",
    "state<sub>3</sub> is called the **final state** and it is a vector that is influenced by all three inputs, so it should be storing information summarising the three inputs.\n",
    "state<sub>2</sub> is a vector that is only influenced by the first two inputs, so it is storing information about a prefix of the sequence.\n",
    "state<sub>0</sub> is called the **initial state** and it is normally a fixed vector (not influenced by the input).\n",
    "You can either use an all-zeros vector or let the optimiser find a suitable vector during training.\n",
    "Since the memory isn't growing with the number of inputs, there will come a point where it must forget some inputs in order to remember new ones, so the size of the state vector will determine what the maximum number of inputs remembered will be.\n",
    "\n",
    "We can now do something like classify the sequence of inputs by passing the final state into a normal neural layer with softmax for example.\n",
    "So how do we do this in PyTorch?\n",
    "It's actually just a matter of using a `for` loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.tensor([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "    [1],\n",
    "], dtype=torch.float32, device=device)\n",
    "\n",
    "w = torch.tensor([[-15], [15]], dtype=torch.float32, device=device)\n",
    "b = torch.tensor([-10], dtype=torch.float32, device=device)\n",
    "\n",
    "state = torch.tensor([0], dtype=torch.float32, device=device)\n",
    "for t in range(input_.shape[0]):\n",
    "    state_input = torch.concat((state, input_[t, :]), dim=0)\n",
    "    state = torch.sigmoid(state_input@w + b)\n",
    "    print(f'state {t+1}:', state.round(decimals=0)[0].detach().cpu().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above recurrent neural network produces a parity bit, that is, tells you if the number of '1' bits seen in the input up to that time step is even or odd.\n",
    "\n",
    "Mathematically, the RNN cell is defined as:\n",
    "\n",
    "$$s_{t+1} = f(\\text{conc}(s_t, x_{t+1}) W + b)$$\n",
    "\n",
    "where $s_t$ is the state at time step $t$, $f$ is the activation function such as sigmoid, *conc* means concatenate two vectors into one, $x_t$ is the input at time step $t$, and $W$ and $b$ are the weights and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNNs in practice\n",
    "\n",
    "In order to apply RNNs on a training set, we need to first learn about a few useful techniques in PyTorch.\n",
    "\n",
    "First, since we'll be working with batches of inputs rather than one vector at a time, we will need to have an initial RNN state for each item in the batch, such that there will be a matrix with an initial state vector in each row.\n",
    "Most of the time, we'll be replicating the same initial state vector for each row.\n",
    "In PyTorch, replication is done using the `tile` function.\n",
    "`tile` takes a tensor and repeats each of its dimensions a number of times (for example, repeat the columns of a matrix 2 times and the rows 3 times).\n",
    "\n",
    "We'll first turn the initial state vector into a single row matrix and then repeat the row for as many times as there are batch items, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "initial_state = torch.tensor(\n",
    "    [1, 2, 3],\n",
    "    dtype=torch.float32, device=device\n",
    ")\n",
    "print('initial_state:')\n",
    "print(initial_state.shape)\n",
    "print(initial_state)\n",
    "print()\n",
    "\n",
    "matrix = initial_state[None, :]\n",
    "print('initial_state matrix:')\n",
    "print(matrix.shape)\n",
    "print(matrix)\n",
    "print()\n",
    "\n",
    "tiled = matrix.tile([batch_size, 1]) # Repeat the row batch_size times and the columns 1 time (leave as-is).\n",
    "print('tiled:')\n",
    "print(tiled.shape)\n",
    "print(tiled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we need to be able to ignore the pad tokens.\n",
    "Just like in CNNs, we'll need to use a mask to ignore some of the inputs.\n",
    "What we want to do is, during each time step in the `for` loop, choose whether to apply the RNN cell on a text or leave the state as-is because there is no input to process, like this:\n",
    "\n",
    "<table>\n",
    "    <tr><th>token</th><td style=\"text-align: center;\">I</td><td style=\"text-align: center;\">like</td><td style=\"text-align: center;\">it</td><td style=\"text-align: center;\">.</td><td style=\"text-align: center;\">PAD</td><td style=\"text-align: center;\">PAD</td></tr>\n",
    "    <tr><th>state</th><td><pre>[1, -1]</pre></td><td><pre>[2, -2]</pre></td><td><pre>[3, -3]</pre></td><td><pre>[4, -4]</pre></td><td><pre>[4, -4]</pre></td><td><pre>[4, -4]</pre></td></tr>\n",
    "</table>\n",
    "\n",
    "Note how the state stops changing after the last actual token in the text.\n",
    "What we want is to have a final state that is only influenced by the actual tokens and not by the pad tokens.\n",
    "\n",
    "This can be done using the `torch.where` function which chooses items from two tensors based on a mask, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_state = torch.tensor([\n",
    "    [1 , 2 ],\n",
    "    [3 , 4 ],\n",
    "    [5 , 6 ],\n",
    "], dtype=torch.float32, device=device)\n",
    "print('current state')\n",
    "print(curr_state)\n",
    "print()\n",
    "\n",
    "new_state = torch.tensor([\n",
    "    [10, 20],\n",
    "    [30, 40],\n",
    "    [50, 60],\n",
    "], dtype=torch.float32, device=device)\n",
    "print('new state')\n",
    "print(new_state)\n",
    "print()\n",
    "\n",
    "mask = torch.tensor([\n",
    "    [0, 0],\n",
    "    [1, 1],\n",
    "    [0, 0],\n",
    "], dtype=torch.bool, device=device)\n",
    "print('mask')\n",
    "print(mask)\n",
    "print()\n",
    "\n",
    "state = torch.where(mask, curr_state, new_state)\n",
    "print('next state')\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between `masked_fill` and `where` is that `masked_fill` always replaces masked values using the same value whereas `where` replaces using the corresponding value in another tensor.\n",
    "\n",
    "The mask we'll be using is produced much more simply than in CNNs because we're not looking at windows but at individual tokens now.\n",
    "The mask will just indicate where in the input are there non-pad tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed = torch.tensor([\n",
    "    [1, 0, 0],\n",
    "    [1, 2, 0],\n",
    "    [1, 2, 3],\n",
    "], dtype=torch.int64, device=device)\n",
    "pad_index = 0\n",
    "print('indexed:')\n",
    "print(indexed)\n",
    "print()\n",
    "\n",
    "non_pad_mask = indexed != pad_index\n",
    "print('non_pad_mask:')\n",
    "print(non_pad_mask)\n",
    "print()\n",
    "\n",
    "print('mask used at every time step:')\n",
    "print()\n",
    "for t in range(3):\n",
    "    mask = non_pad_mask[:, t, None] # The mask needs to be applied to a matrix of state vectors so we need to add a singleton dimension.\n",
    "    print(f'time step {t}:')\n",
    "    print(mask)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our simple RNN implementation in the toy sentiment analysis task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [\n",
    "    'I like it .'.split(' '),\n",
    "    'I hate it .'.split(' '),\n",
    "    'I don\\'t hate it .'.split(' '),\n",
    "    'I don\\'t like it .'.split(' '),\n",
    "]\n",
    "train_y = torch.tensor([\n",
    "    [1],\n",
    "    [0],\n",
    "    [1],\n",
    "    [0],\n",
    "], dtype=torch.float32, device=device)\n",
    "\n",
    "max_len = max(len(text) for text in train_x)\n",
    "print('max_len:', max_len)\n",
    "\n",
    "vocab = ['<PAD>'] + sorted({token for text in train_x for token in text})\n",
    "token2index = {t: i for (i, t) in enumerate(vocab)}\n",
    "pad_index = token2index['<PAD>']\n",
    "print('vocab:', vocab)\n",
    "print()\n",
    "\n",
    "train_x_indexed_np = np.full([len(train_x), max_len], pad_index, np.int64)\n",
    "for i in range(len(train_x)):\n",
    "    for j in range(len(train_x[i])):\n",
    "        train_x_indexed_np[i, j] = token2index[train_x[i][j]]\n",
    "train_x_indexed = torch.tensor(train_x_indexed_np, device=device)\n",
    "print('train_x_indexed:')\n",
    "print(train_x_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, state_size, pad_index):\n",
    "        super().__init__()\n",
    "        self.pad_index = pad_index\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_s0 = torch.nn.Parameter(torch.zeros((state_size,), dtype=torch.float32)) # Initial state starts as zeros and is then optimised.\n",
    "        self.rnn_cell = torch.nn.Linear(state_size + embedding_size, state_size)\n",
    "        self.output_layer = torch.nn.Linear(state_size, 1)\n",
    "        \n",
    "    def forward(self, x_indexed):\n",
    "        batch_size = x_indexed.shape[0]\n",
    "        time_steps = x_indexed.shape[1]\n",
    "\n",
    "        non_pad_mask = x_indexed != self.pad_index\n",
    "        \n",
    "        embedded = self.embedding(x_indexed)\n",
    "        state = self.rnn_s0[None, :].tile([batch_size, 1])\n",
    "        for t in range(time_steps):\n",
    "            state_input = torch.concat([state, embedded[:, t, :]], dim=1)\n",
    "            new_state = torch.nn.functional.leaky_relu(self.rnn_cell(state_input))\n",
    "            state = torch.where(non_pad_mask[:, t, None], new_state, state)\n",
    "        return self.output_layer(state)\n",
    "\n",
    "model = Model(len(vocab), embedding_size=2, state_size=3, pad_index=pad_index)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 2000+1):\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x_indexed)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%200 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('text', 'output')\n",
    "    output = torch.sigmoid(model(train_x_indexed)).round(decimals=0)[:, 0].cpu().tolist()\n",
    "    for (text, y) in zip(train_x, output):\n",
    "        print(text, y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with the CNN, we can check whether the pad token is having any influence by checking the gradients of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser.zero_grad()\n",
    "logits = model(train_x_indexed)\n",
    "train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "train_error.backward()\n",
    "\n",
    "grads = model.embedding.weight.grad.abs().sum(dim=1).tolist()\n",
    "for (token, grad) in zip(vocab, grads):\n",
    "    print(f'{token: >6s}: {grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstable gradients and the LSTM\n",
    "\n",
    "Remember how, in the two layer network, the gradients of the first layer were much smaller than the gradients of the second layer?\n",
    "Well the same thing happens with time steps in an RNN.\n",
    "As the sequence gets longer, the gradients will either vanish or explode.\n",
    "Let's see an example.\n",
    "\n",
    "We'll generate several random input sequences with single number vectors and use a randomly generated weights matrices for RNNs with a single number state vector.\n",
    "The RNN will then go through the input sequence and the number in the final state will be used to calculate the gradient with respect to each input item in the input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Parameter(2*torch.rand((2, 1), dtype=torch.float32) - 1)\n",
    "        self.b = torch.nn.Parameter(2*torch.rand((1,), dtype=torch.float32) - 1)\n",
    "        self.s0 = torch.nn.Parameter(torch.tensor([0], dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        state = self.s0\n",
    "        for t in range(x.shape[0]):\n",
    "            state_input = torch.concat((state, x[t, :]), dim=0)\n",
    "            state = torch.nn.functional.leaky_relu(state_input@self.w + self.b)\n",
    "        return state\n",
    "\n",
    "(fig, axs) = plt.subplots(1, 8, figsize=(20, 2))\n",
    "axs[0].set_ylabel('gradient')\n",
    "for i in range(8):\n",
    "    model = RNN()\n",
    "    model.to(device)\n",
    "        \n",
    "    input_seq = torch.randn((10, 1), device=device, requires_grad=True)\n",
    "    state = model(input_seq)[0]\n",
    "    state.backward()\n",
    "    grads = input_seq.grad.abs()[:, 0].cpu().numpy()\n",
    "    \n",
    "    axs[i].bar(np.arange(input_seq.shape[0]), grads)\n",
    "    axs[i].set_xlabel('time step')\n",
    "    axs[i].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In almost all of the cases, the gradient with respect to the last item is much larger than the gradient with respect to the first item, meaning that the RNN will ignore inputs close to the beginning of the sequence during training.\n",
    "To fix this, in 1997, Hochreiter and Schmidhuber developed the **Long Short-Term Memory network** (**LSTM**).\n",
    "The main change from the simple RNN is to add the current state to the new state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN2(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Parameter(2*torch.rand((2, 1), dtype=torch.float32) - 1)\n",
    "        self.b = torch.nn.Parameter(2*torch.rand((1,), dtype=torch.float32) - 1)\n",
    "        self.s0 = torch.nn.Parameter(torch.tensor([0], dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        state = self.s0\n",
    "        for t in range(x.shape[0]):\n",
    "            state_input = torch.concat((state, x[t, :]), dim=0)\n",
    "            state = torch.nn.functional.leaky_relu(state_input@self.w + self.b) + state\n",
    "        return state\n",
    "\n",
    "(fig, axs) = plt.subplots(1, 8, figsize=(20, 2))\n",
    "axs[0].set_ylabel('gradient')\n",
    "for i in range(8):\n",
    "    model = RNN2()\n",
    "    model.to(device)\n",
    "        \n",
    "    input_seq = torch.randn((10, 1), device=device, requires_grad=True)\n",
    "    state = model(input_seq)[0]\n",
    "    state.backward()\n",
    "    grads = input_seq.grad.abs()[:, 0].cpu().numpy()\n",
    "    \n",
    "    axs[i].bar(np.arange(input_seq.shape[0]), grads)\n",
    "    axs[i].set_xlabel('time step')\n",
    "    axs[i].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the simplified version; in reality the LSTM is a little more complex.\n",
    "\n",
    "For example, the LSTM actually uses a **hyperbolic tangent** (**tanh**) instead of leaky ReLU.\n",
    "Hyperbolic tangent is a sigmoid function which is stretched so that its range is between 1 and -1 instead of 1 and 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(1, 1)\n",
    "xs = np.linspace(-10, 10, 100)\n",
    "ys = np.tanh(xs)\n",
    "ax.plot(xs, ys, linestyle='-', linewidth=2, marker='', color='red')\n",
    "ax.text(-7.0, 0.0, '$y = \\\\tanh(x)$', color='red', fontsize=16)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM also uses **gates** which are fractions between 1 and 0 that are multiplied to activation values in order to either leave the activation values unchanged (gate open) or replace them with a zero (gate closed).\n",
    "The gate value is produced by a layer in the LSTM which is defined like this:\n",
    "\n",
    "$$g_{t+1} = \\text{sig}(\\text{conc}(s_t, x_{t+1}) W + b)$$\n",
    "\n",
    "where $g_t$ is the gate at time step $t$ and $\\sig$ means sigmoid.\n",
    "The sigmoid is what makes the gate a fraction between zero and one.\n",
    "The gate is actually a vector of fractions rather than a single number so that it can be multiplied by a vector of activation values.\n",
    "When multiplied by another vector, the gate will act as a **soft mask** that zeros out some values in the vector whilst leaving other values as-is but in a continuous way rather than a hard yes or no.\n",
    "Note that this factor is generated based on the new input vector and the current state vector such that the LSTM can learn to ignore different values at different time steps.\n",
    "\n",
    "The LSTM is mathematically defined as follows:\n",
    "\n",
    "$$c_{t+1} = g^i_t \\times \\tanh(\\text{conc}(s_t, x_{t+1}) W + b) + g^f_t \\times c_t$$\n",
    "$$s_{t+1} = g^o_t \\times \\tanh(c_{t+1})$$\n",
    "\n",
    "There's quite a few things to unpack here.\n",
    "First of all, $g^i$, $g^f$, and $g^o$ are all different gates with their own parameters.\n",
    "They are the **input gate**, used to control which parts of the input vector gets to be processed, the **forget gate**, used to forget some part of the state, and the **output gate**, used to control which parts of the state get to go out of the LSTM.\n",
    "There's also $c_t$, which is sort of a second RNN state.\n",
    "The $s_t$ state is called the **hidden state** whilst the $c_t$ is called the **cell state**.\n",
    "Usually it is the hidden state that is used to represent the sequence.\n",
    "Both states need to be initialised for time step zero.\n",
    "\n",
    "Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, state_size):\n",
    "        super().__init__()\n",
    "        self.linear_gi = torch.nn.Linear(state_size + input_size, state_size)\n",
    "        self.linear_gf = torch.nn.Linear(state_size + input_size, state_size)\n",
    "        self.linear_go = torch.nn.Linear(state_size + input_size, state_size)\n",
    "        self.linear_c = torch.nn.Linear(state_size + input_size, state_size)\n",
    "\n",
    "    def forward(self, x, c, state):\n",
    "        state_input = torch.concat((state, x), dim=1)\n",
    "        gi = torch.sigmoid(self.linear_gi(state_input))\n",
    "        gf = torch.sigmoid(self.linear_gf(state_input))\n",
    "        go = torch.sigmoid(self.linear_go(state_input))\n",
    "        c = gi*torch.tanh(self.linear_c(state_input)) + gf*c\n",
    "        state = go*torch.tanh(c)\n",
    "        return (c, state)\n",
    "\n",
    "input_size = 1\n",
    "state_size = 1\n",
    "time_steps = 5\n",
    "batch_size = 3\n",
    "\n",
    "lstm = LSTM(input_size=input_size, state_size=state_size)\n",
    "lstm.to(device)\n",
    "\n",
    "input_seq = torch.randn((batch_size, time_steps, input_size), dtype=torch.float32, device=device)\n",
    "\n",
    "c = torch.zeros([batch_size, state_size], dtype=torch.float32, device=device)\n",
    "state = torch.zeros([batch_size, state_size], dtype=torch.float32, device=device)\n",
    "for t in range(time_steps):\n",
    "    (c, state) = lstm(input_seq[:, t, :], c, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course there's no need to write down all this code as it's already available in PyTorch as\n",
    "\n",
    "    torch.nn.LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "state_size = 1\n",
    "time_steps = 5\n",
    "batch_size = 3\n",
    "\n",
    "lstm = torch.nn.LSTMCell(input_size, state_size)\n",
    "lstm.to(device)\n",
    "\n",
    "input_seq = torch.randn((batch_size, time_steps, input_size), dtype=torch.float32, device=device)\n",
    "\n",
    "c = torch.zeros([batch_size, state_size], dtype=torch.float32, device=device)\n",
    "state = torch.zeros([batch_size, state_size], dtype=torch.float32, device=device)\n",
    "for t in range(time_steps):\n",
    "    (c, state) = lstm(input_seq[:, t, :], (c, state)) # States must be provided as a tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this LSTM in the sentiment analysis toy task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, state_size, pad_index):\n",
    "        super().__init__()\n",
    "        self.pad_index = pad_index\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_s0 = torch.nn.Parameter(torch.zeros((state_size,), dtype=torch.float32))\n",
    "        self.rnn_c0 = torch.nn.Parameter(torch.zeros((state_size,), dtype=torch.float32))\n",
    "        self.rnn_cell = torch.nn.LSTMCell(embedding_size, state_size)\n",
    "        self.output_layer = torch.nn.Linear(state_size, 1)\n",
    "        \n",
    "    def forward(self, x_indexed):\n",
    "        batch_size = x_indexed.shape[0]\n",
    "        time_steps = x_indexed.shape[1]\n",
    "\n",
    "        non_pad_mask = x_indexed != self.pad_index\n",
    "        \n",
    "        embedded = self.embedding(x_indexed)\n",
    "        state = self.rnn_s0[None, :].tile((batch_size, 1))\n",
    "        c = self.rnn_c0[None, :].tile((batch_size, 1))\n",
    "        for t in range(time_steps):\n",
    "            (new_state, c) = self.rnn_cell(embedded[:, t, :], (state, c))\n",
    "            state = torch.where(non_pad_mask[:, t, None], new_state, state) # If we're not outputting the c state then we don't need to mask it.\n",
    "        return self.output_layer(state)\n",
    "\n",
    "model = Model(len(vocab), embedding_size=2, state_size=3, pad_index=pad_index)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 1000+1):\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x_indexed)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_errors.append(train_error.detach().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('text', 'output')\n",
    "    output = torch.sigmoid(model(train_x_indexed))[:, 0].cpu().tolist()\n",
    "    for (text, y) in zip(train_x, output):\n",
    "        print(text, y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Using the LSTM\n",
    "\n",
    "Rewrite the movie reviews classification program using an LSTM.\n",
    "Preprocessing has been done for you.\n",
    "Don't forget to calculate the test set accuracy after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 3\n",
    "\n",
    "train_df = pd.read_csv('../data_set/sentiment/train.csv')\n",
    "test_df = pd.read_csv('../data_set/sentiment/test.csv')\n",
    "\n",
    "train_x = train_df['text']\n",
    "train_y = train_df['class']\n",
    "test_x = test_df['text']\n",
    "test_y = test_df['class']\n",
    "categories = ['neg', 'pos']\n",
    "cat2idx = {cat: i for (i, cat) in enumerate(categories)}\n",
    "\n",
    "train_y_indexed = torch.tensor(\n",
    "    train_y.map(cat2idx.get).to_numpy()[:, None],\n",
    "    dtype=torch.float32, device=device\n",
    ")\n",
    "test_y_indexed = test_y.map(cat2idx.get).to_numpy()[:, None]\n",
    "\n",
    "nltk.download('punkt')\n",
    "train_x_tokens = [nltk.word_tokenize(text) for text in train_x]\n",
    "test_x_tokens = [nltk.word_tokenize(text) for text in test_x]\n",
    "max_len = max(max(len(text) for text in train_x_tokens), max(len(text) for text in test_x_tokens))\n",
    "\n",
    "frequencies = collections.Counter(token for text in train_x_tokens for token in text)\n",
    "vocabulary = sorted(frequencies.keys(), key=frequencies.get, reverse=True)\n",
    "while frequencies[vocabulary[-1]] < min_freq:\n",
    "    vocabulary.pop()\n",
    "vocab = ['<PAD>', '<UNK>'] + vocabulary\n",
    "token2index = {token: i for (i, token) in enumerate(vocab)}\n",
    "pad_index = token2index['<PAD>']\n",
    "unk_index = token2index['<UNK>']\n",
    "\n",
    "train_x_indexed_np = np.full((len(train_x_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_x_tokens)):\n",
    "    for j in range(len(train_x_tokens[i])):\n",
    "        train_x_indexed_np[i, j] = token2index.get(train_x_tokens[i][j], unk_index)\n",
    "train_x_indexed = torch.tensor(train_x_indexed_np, device=device)\n",
    "\n",
    "test_x_indexed_np = np.full((len(test_x_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(test_x_tokens)):\n",
    "    for j in range(len(test_x_tokens[i])):\n",
    "        test_x_indexed_np[i, j] = token2index.get(test_x_tokens[i][j], unk_index)\n",
    "test_x_indexed = torch.tensor(test_x_indexed_np, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75ee2b71ad44bf9ef4e9bee896f68ffbc764a6a2c6d1f57c86c48f99ffc25ca8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
