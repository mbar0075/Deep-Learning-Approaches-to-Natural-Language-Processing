{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "In 2017, another paper that revolutionised deep learning for NLP was published: [Attention is All you Need](https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) by Vaswani and others.\n",
    "In it, the **transformer** was described, which was a new way to encode words-in-context instead of the bi-directional RNN.\n",
    "\n",
    "The problem with RNNs is that they are inherently sequential.\n",
    "You can't get the next state unless you have the previous one.\n",
    "So regardless of how big your GPU is, if you want to encode a sequence with $n$ items then you will need to go through $n$ sequential time steps.\n",
    "\n",
    "The transformer solves this problem by doing something similar to taking the average vector of the embedding vectors.\n",
    "Adding all the embedding vectors together can be done in parallel, so that would solve the RNN's problem.\n",
    "Unfortunately it also loses word order information, which is a big deal.\n",
    "We'll get to how word order information is preserved in a bit, but for now we should focus on this part of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries, keys, and values\n",
    "\n",
    "Transformers do the following:\n",
    "\n",
    "* Take each token vector ($t_i$) and use three separate neural layers to transform each token vector into three separate vectors: a query vector ($q_i$), a key vector ($k_i$), and a value vector ($v_i$).\n",
    "* Combine each $q_i$ with each $k_j$ (including the key from the same token as $q_i$) to produce an attention value $a_{ij}$.\n",
    "* Multiply $a_{ij}$ by $v_j$ (the value vector that came from the same token as the key vector) and take their sum to produce a vector for every token.\n",
    "\n",
    "Here is a diagram illustrating this architecture:\n",
    "\n",
    "![](encoder_full.png)\n",
    "\n",
    "Here is the same diagram but focusing only on the first token's context vector:\n",
    "\n",
    "![](encoder_focused.png)\n",
    "\n",
    "Note how we're comparing each token $t_i$ to every other token $t_j$, including $t_i$ itself, to determine how important $t_j$ is for understanding $t_i$.\n",
    "This importance is used to make a weighted average of all the tokens in the text in order to represent the meaning of $t_i$.\n",
    "So we compare the query vector of $t_i$ to the key vector of $t_j$ to see if we should return the value vector of $t_j$, similar to how a Python dictionary works, except that a weighted average of all the values in the dictionary is returned instead of just one.\n",
    "\n",
    "This process is done for each token which, again, happens all in parallel.\n",
    "The advantage of having separate query and key vectors is that you can compare a token to itself without comparing the tokens's vector to itself (so the model can control how similar the token should be to itself).\n",
    "\n",
    "As explained in the previous topic, the attention maker is the dot product of the two vectors (the key and the query in this case).\n",
    "Vaswani adds a modification to avoid the dot product from getting too big due to having large query and key vectors: the dot product is divided by the square root of the vector size.\n",
    "\n",
    "Furthermore, there is not just one word-in-context vector per token coming out at the other end.\n",
    "Just like in convolutional neural networks, the embeddings are split to be processed by several branches in parallel, each of which having its own query-key-value vectors and attention values.\n",
    "This is called **multihead attention** in transformers.\n",
    "\n",
    "![](encoder_focused_branched.png)\n",
    "\n",
    "Let's show how all of this is done in PyTorch.\n",
    "First, create the token embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_indexed = torch.tensor([\n",
    "    [1, 2, 0],\n",
    "    [1, 0, 0],\n",
    "], dtype=torch.int64, device=device)\n",
    "\n",
    "batch_size = x_indexed.shape[0]\n",
    "time_steps = x_indexed.shape[1]\n",
    "pad_index = 0\n",
    "vocab_size = 3\n",
    "num_branches = 2\n",
    "embedding_size = 8\n",
    "\n",
    "embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "embedding.to(device)\n",
    "\n",
    "embedded = embedding(x_indexed)\n",
    "print('embedded:')\n",
    "print(embedded.shape)\n",
    "print(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, split the token vectors into branches by adding another dimension (this is much faster than working with lists like in CNNs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branched_embedded = embedded.reshape(\n",
    "    (batch_size, time_steps, num_branches, embedding_size//num_branches)\n",
    ")\n",
    "branched_embedded = branched_embedded.transpose(1, 2) # Put the branches dimension after the batch size.\n",
    "print('branched_embedded:')\n",
    "print(branched_embedded.shape)\n",
    "print(branched_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the queries, keys, and values from these branched token vectors.\n",
    "Note that we can just pass in that 4D tensor through a linear layer and everything will be as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_layer = torch.nn.Linear(embedding_size//num_branches, embedding_size)\n",
    "key_layer = torch.nn.Linear(embedding_size//num_branches, embedding_size)\n",
    "value_layer = torch.nn.Linear(embedding_size//num_branches, embedding_size)\n",
    "query_layer.to(device)\n",
    "key_layer.to(device)\n",
    "value_layer.to(device)\n",
    "\n",
    "q = query_layer(branched_embedded)\n",
    "k = key_layer(branched_embedded)\n",
    "v = value_layer(branched_embedded)\n",
    "\n",
    "print('q:')\n",
    "print(q.shape)\n",
    "print()\n",
    "print('k:')\n",
    "print(k.shape)\n",
    "print()\n",
    "print('v:')\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we produce the attention logits by taking the dot product of every query to every key (remember that this can be made by just making a matrix multiplication):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_logits = q@k.transpose(2, 3)\n",
    "\n",
    "# Divide the logits by the square root of the vector size.\n",
    "sqrt_dim = np.sqrt(embedding_size)\n",
    "attn_logits = attn_logits/sqrt_dim\n",
    "\n",
    "print('attn_logits:')\n",
    "print(attn_logits.shape) # Dimensions are: [batch, branch, query, key]\n",
    "print(attn_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, mask out the pad tokens by replacing their logits with negative infinity which will result in an attention of 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mask = x_indexed == pad_index\n",
    "attn_logits = attn_logits.masked_fill(pad_mask[:, None, None, :], float('-inf')) # Add a singleton dimension for the branch dimension and the query dimension (queries corresponding to pad tokens also need to be ignored but that can be done later).\n",
    "print('attn_logits:')\n",
    "print(attn_logits.shape)\n",
    "print(attn_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert these logits into attention values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = torch.softmax(attn_logits, dim=3)\n",
    "print('attention:')\n",
    "print(attention.shape)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the attention to the values which, again, is just a matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branched_attended_values = attention@v\n",
    "print('branched_attended_values:')\n",
    "print(branched_attended_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then rejoin the branches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attended_values = branched_attended_values.transpose(1, 2).reshape((batch_size, time_steps, num_branches*embedding_size))\n",
    "print('attended_values:')\n",
    "print(attended_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally transform these vectors with another layer to squash them to the value size and those are your word-in-context vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_in_context_layer = torch.nn.Linear(num_branches*embedding_size, embedding_size)\n",
    "word_in_context_layer.to(device)\n",
    "\n",
    "word_in_context = torch.nn.functional.leaky_relu(word_in_context_layer(attended_values))\n",
    "print('word_in_context:')\n",
    "print(word_in_context.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some other things missing from Vaswani's implementation such as residual connections and layer normalisation, but we don't need to get into those.\n",
    "Also Vaswani's transformer was actually a sequence to sequence model, which we'll get to later in this topic.\n",
    "\n",
    "Let's use a transformer on the toy data set in order to re-implement the text classification at every token task we previously implemented using a bi-directional RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [\n",
    "    'I like it .'.split(' '),\n",
    "    'I hate it .'.split(' '),\n",
    "    'I don\\'t hate it .'.split(' '),\n",
    "    'I don\\'t like it .'.split(' '),\n",
    "]\n",
    "train_y = torch.tensor([\n",
    "    [1],\n",
    "    [0],\n",
    "    [1],\n",
    "    [0],\n",
    "], dtype=torch.float32, device=device)\n",
    "\n",
    "max_len = max(len(text) for text in train_x)\n",
    "print('max_len:', max_len)\n",
    "\n",
    "vocab = ['<PAD>'] + sorted({token for text in train_x for token in text})\n",
    "token2index = {t: i for (i, t) in enumerate(vocab)}\n",
    "pad_index = token2index['<PAD>']\n",
    "print('vocab:', vocab)\n",
    "print()\n",
    "\n",
    "train_x_indexed_np = np.full((len(train_x), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_x)):\n",
    "    for j in range(len(train_x[i])):\n",
    "        train_x_indexed_np[i, j] = token2index[train_x[i][j]]\n",
    "train_x_indexed = torch.tensor(train_x_indexed_np, device=device)\n",
    "\n",
    "train_y_seq = train_y[:, None, :].tile((1, max_len, 1))\n",
    "print('train_y_seq:')\n",
    "print(train_y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, num_branches, pad_index):\n",
    "        super().__init__()\n",
    "        self.pad_index = pad_index\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_branches = num_branches\n",
    "        self.sqrt_dim = np.sqrt(embedding_size)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.query_layer = torch.nn.Linear(embedding_size//num_branches, embedding_size)\n",
    "        self.key_layer = torch.nn.Linear(embedding_size//num_branches, embedding_size)\n",
    "        self.value_layer = torch.nn.Linear(embedding_size//num_branches, embedding_size)\n",
    "        self.word_in_context_layer = torch.nn.Linear(num_branches*embedding_size, embedding_size)\n",
    "        self.output_layer = torch.nn.Linear(embedding_size, 1)\n",
    "\n",
    "    def forward(self, x_indexed):\n",
    "        batch_size = x_indexed.shape[0]\n",
    "        time_steps = x_indexed.shape[1]\n",
    "        pad_mask = x_indexed == self.pad_index\n",
    "\n",
    "        embedded = self.embedding(x_indexed)\n",
    "        \n",
    "        branched_embedded = embedded.reshape(\n",
    "            (batch_size, time_steps, self.num_branches, self.embedding_size//self.num_branches)\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        q = self.query_layer(branched_embedded)\n",
    "        k = self.key_layer(branched_embedded)\n",
    "        v = self.value_layer(branched_embedded)\n",
    "\n",
    "        attn_logits = q@k.transpose(2, 3)\n",
    "        attn_logits = attn_logits/self.sqrt_dim\n",
    "        attn_logits = attn_logits.masked_fill(pad_mask[:, None, None, :], float('-inf'))\n",
    "        attention = torch.softmax(attn_logits, dim=3)\n",
    "        branched_attended_values = attention@v\n",
    "\n",
    "        attended_values = branched_attended_values.transpose(1, 2).reshape(\n",
    "            (batch_size, time_steps, self.num_branches*self.embedding_size)\n",
    "        )\n",
    "        word_in_context = torch.nn.functional.leaky_relu(self.word_in_context_layer(attended_values))\n",
    "\n",
    "        return self.output_layer(word_in_context)\n",
    "\n",
    "model = Model(len(vocab), embedding_size=4, num_branches=2, pad_index=pad_index)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 1000+1):\n",
    "    batch_size = train_y_seq.shape[0]\n",
    "    time_steps = train_y_seq.shape[1]\n",
    "    pad_mask = train_x_indexed == pad_index\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x_indexed)\n",
    "    train_token_errors = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y_seq, reduction='none')\n",
    "    train_token_errors = torch.masked_fill(train_token_errors, pad_mask[:, :, None], 0.0)\n",
    "    train_error = train_token_errors.sum()/(~pad_mask).sum()\n",
    "    train_errors.append(train_error.detach().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('text', 'output')\n",
    "    output = torch.sigmoid(model(train_x_indexed))[:, :, 0].cpu().tolist()\n",
    "    for (text, y) in zip(train_x, output):\n",
    "        print(text + ['<PAD>']*(max_len - len(text)), y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully there is a built-in module for multihead attention:\n",
    "\n",
    "    torch.nn.MultiheadAttention\n",
    "\n",
    "This is how you use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, num_branches, pad_index):\n",
    "        super().__init__()\n",
    "        self.pad_index = pad_index\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        self.multihead_attention_layer = torch.nn.MultiheadAttention(embedding_size, num_branches, batch_first=True) # batch_first is used to say that the token vectors will be provided with the batch as the first dimension rather than the second.\n",
    "        \n",
    "        self.output_layer = torch.nn.Linear(embedding_size, 1)\n",
    "\n",
    "    def forward(self, x_indexed):\n",
    "        batch_size = x_indexed.shape[0]\n",
    "        time_steps = x_indexed.shape[1]\n",
    "        pad_mask = x_indexed == self.pad_index\n",
    "\n",
    "        embedded = self.embedding(x_indexed)\n",
    "        \n",
    "        # The __forward__ function of the multihead attention module returns two things: the words in context and the attention values; the attention values are set to None unless need_weights is set to True.\n",
    "        (word_in_context, _) = self.multihead_attention_layer(query=embedded, key=embedded, value=embedded, key_padding_mask=pad_mask, need_weights=False)\n",
    "        \n",
    "        return self.output_layer(word_in_context)\n",
    "\n",
    "model = Model(len(vocab), embedding_size=4, num_branches=2, pad_index=pad_index)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 1000+1):\n",
    "    batch_size = train_y_seq.shape[0]\n",
    "    time_steps = train_y_seq.shape[1]\n",
    "    pad_mask = train_x_indexed == pad_index\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x_indexed)\n",
    "    train_token_errors = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y_seq, reduction='none')\n",
    "    train_token_errors = torch.masked_fill(train_token_errors, pad_mask[:, :, None], 0.0)\n",
    "    train_error = train_token_errors.sum()/(~pad_mask).sum()\n",
    "    train_errors.append(train_error.detach().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('text', 'output')\n",
    "    output = torch.sigmoid(model(train_x_indexed))[:, :, 0].cpu().tolist()\n",
    "    for (text, y) in zip(train_x, output):\n",
    "        print(text + ['<PAD>']*(max_len - len(text)), y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that the multihead attention module expects you to pass the embedded tokens 3 times, once for the query, once for the key, and once for the value.\n",
    "Later on we'll see a situation where these would not be the same tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token order information\n",
    "\n",
    "As-is, when calculating the attention, there is no way to take into account where a particular query and key pair are situated in the text, which means that the neural network won't see anything different if you shuffle the order of the tokens.\n",
    "To solve this problem, the embedding vectors are modified to include positional information by adding to each embedding vector a positional vector.\n",
    "A positional vector is like an embedding vector but for positions instead of tokens.\n",
    "There are many ways to do this, but the simplest is by using a **positioning matrix**.\n",
    "\n",
    "A positioning matrix would have a row vector for every token position, just like an embedding matrix has a row vector for every token in the vocabulary.\n",
    "The problem with this is that, just like you must have a fixed number of tokens in your vocabulary, you also must have a fixed number of positions, which means that you have a maximum text length you can process.\n",
    "This is a common issue in transformers.\n",
    "\n",
    "We can get a sequence of position indexes for every token in a batch by using `torch.arange`, which gives a tensor of increasing numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "time_steps = 5\n",
    "position_indexes = torch.arange(time_steps)[None, :].tile((batch_size, 1))\n",
    "print(position_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These indexes can be passed into an embedding layer to convert them into vectors which can then be either concatenated to the token vectors or simply added together, the second being the most common method.\n",
    "\n",
    "Let's use it on the toy data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, max_len, embedding_size, num_branches, pad_index):\n",
    "        super().__init__()\n",
    "        self.pad_index = pad_index\n",
    "        self.register_buffer('positions', torch.arange(max_len)) # This creates a positions instance variable that is not a parameter but still moves to the requested device when the module's 'to' method is used.\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.positioning = torch.nn.Embedding(max_len, embedding_size)\n",
    "        self.multihead_attention_layer = torch.nn.MultiheadAttention(embedding_size, num_branches, batch_first=True)\n",
    "        self.output_layer = torch.nn.Linear(embedding_size, 1)\n",
    "\n",
    "    def forward(self, x_indexed):\n",
    "        batch_size = x_indexed.shape[0]\n",
    "        time_steps = x_indexed.shape[1]\n",
    "        pad_mask = x_indexed == self.pad_index\n",
    "        \n",
    "        embedded = self.embedding(x_indexed)\n",
    "        positions = self.positions[None, :time_steps].tile((batch_size, 1)) # Trim the positions sequence to the number of time steps used.\n",
    "        positioned = self.positioning(positions)\n",
    "        embedded = embedded + positioned # Position vectors are added to the embedding vectors.\n",
    "        \n",
    "        (word_in_context, _) = self.multihead_attention_layer(query=embedded, key=embedded, value=embedded, key_padding_mask=pad_mask, need_weights=False)\n",
    "        \n",
    "        return self.output_layer(word_in_context)\n",
    "\n",
    "model = Model(len(vocab), max_len=max_len, embedding_size=4, num_branches=2, pad_index=pad_index)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 1000+1):\n",
    "    batch_size = train_y_seq.shape[0]\n",
    "    time_steps = train_y_seq.shape[1]\n",
    "    pad_mask = train_x_indexed == pad_index\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x_indexed)\n",
    "    train_token_errors = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y_seq, reduction='none')\n",
    "    train_token_errors = torch.masked_fill(train_token_errors, pad_mask[:, :, None], 0.0)\n",
    "    train_error = train_token_errors.sum()/(~pad_mask).sum()\n",
    "    train_errors.append(train_error.detach().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('text', 'output')\n",
    "    output = torch.sigmoid(model(train_x_indexed))[:, :, 0].cpu().tolist()\n",
    "    for (text, y) in zip(train_x, output):\n",
    "        print(text + ['<PAD>']*(max_len - len(text)), y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text representation\n",
    "\n",
    "We've seen how to represent each word-in-context, but how do you represent a whole text with one vector?\n",
    "A lot of papers to do so by just taking the average of the word-in-context vectors.\n",
    "The average of a batch of vector sequences that need to be masked is found as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mask = torch.tensor([\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 1],\n",
    "], dtype=torch.bool, device=device)\n",
    "print('pad_mask')\n",
    "print(pad_mask)\n",
    "print()\n",
    "\n",
    "batch_size = pad_mask.shape[0]\n",
    "time_step = pad_mask.shape[1]\n",
    "embedding_size = 4\n",
    "\n",
    "word_in_context = torch.randn((batch_size, time_step, embedding_size), dtype=torch.float32, device=device)\n",
    "print('word_in_context')\n",
    "print(word_in_context)\n",
    "print()\n",
    "\n",
    "word_in_context = word_in_context.masked_fill(pad_mask[:, :, None], 0.0) # Zero out the vectors corresponding to pad tokens.\n",
    "text_vecs = word_in_context.sum(dim=1)/(~pad_mask).sum(dim=1)[:, None] # Divide the each sum of the vector sequences by the number of tokens in each sequence.\n",
    "print('text_vecs')\n",
    "print(text_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how it would be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, max_len, embedding_size, num_branches, pad_index):\n",
    "        super().__init__()\n",
    "        self.pad_index = pad_index\n",
    "        self.register_buffer('positions', torch.arange(max_len))\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.positioning = torch.nn.Embedding(max_len, embedding_size)\n",
    "        self.multihead_attention_layer = torch.nn.MultiheadAttention(embedding_size, num_branches, batch_first=True)\n",
    "        self.output_layer = torch.nn.Linear(embedding_size, 1)\n",
    "\n",
    "    def forward(self, x_indexed):\n",
    "        batch_size = x_indexed.shape[0]\n",
    "        time_steps = x_indexed.shape[1]\n",
    "        pad_mask = x_indexed == self.pad_index\n",
    "        \n",
    "        embedded = self.embedding(x_indexed)\n",
    "        positions = self.positions[None, :time_steps].tile((batch_size, 1))\n",
    "        positioned = self.positioning(positions)\n",
    "        embedded = embedded + positioned\n",
    "        \n",
    "        (word_in_context, _) = self.multihead_attention_layer(query=embedded, key=embedded, value=embedded, key_padding_mask=pad_mask, need_weights=False)\n",
    "        text_vecs = word_in_context.sum(dim=1)/(~pad_mask).sum(dim=1)[:, None]\n",
    "        \n",
    "        return self.output_layer(text_vecs)\n",
    "\n",
    "model = Model(len(vocab), max_len=max_len, embedding_size=4, num_branches=2, pad_index=pad_index)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 1000+1):\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x_indexed)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_errors.append(train_error.detach().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('text', 'output')\n",
    "    output = torch.sigmoid(model(train_x_indexed))[:, 0].cpu().tolist()\n",
    "    for (text, y) in zip(train_x, output):\n",
    "        print(text, y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to get a vector for the entire text is to add another special token, usually called a **class token**, which is always added to the beginning of the text.\n",
    "The vector produced at this token is then used to represent the whole text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [\n",
    "    '<CLS> I like it .'.split(' '),\n",
    "    '<CLS> I hate it .'.split(' '),\n",
    "    '<CLS> I don\\'t hate it .'.split(' '),\n",
    "    '<CLS> I don\\'t like it .'.split(' '),\n",
    "]\n",
    "train_y = torch.tensor([\n",
    "    [1],\n",
    "    [0],\n",
    "    [1],\n",
    "    [0],\n",
    "], dtype=torch.float32, device=device)\n",
    "\n",
    "max_len = max(len(text) for text in train_x)\n",
    "print('max_len:', max_len)\n",
    "\n",
    "vocab = ['<PAD>'] + sorted({token for text in train_x for token in text})\n",
    "token2index = {t: i for (i, t) in enumerate(vocab)}\n",
    "pad_index = token2index['<PAD>']\n",
    "print('vocab:', vocab)\n",
    "print()\n",
    "\n",
    "train_x_indexed_np = np.full((len(train_x), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_x)):\n",
    "    for j in range(len(train_x[i])):\n",
    "        train_x_indexed_np[i, j] = token2index[train_x[i][j]]\n",
    "train_x_indexed = torch.tensor(train_x_indexed_np, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, max_len, embedding_size, num_branches, pad_index):\n",
    "        super().__init__()\n",
    "        self.pad_index = pad_index\n",
    "        self.register_buffer('positions', torch.arange(max_len))\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.positioning = torch.nn.Embedding(max_len, embedding_size)\n",
    "        self.multihead_attention_layer = torch.nn.MultiheadAttention(embedding_size, num_branches, batch_first=True)\n",
    "        self.output_layer = torch.nn.Linear(embedding_size, 1)\n",
    "\n",
    "    def forward(self, x_indexed):\n",
    "        batch_size = x_indexed.shape[0]\n",
    "        time_steps = x_indexed.shape[1]\n",
    "        pad_mask = x_indexed == self.pad_index\n",
    "        \n",
    "        embedded = self.embedding(x_indexed)\n",
    "        positions = self.positions[None, :time_steps].tile((batch_size, 1))\n",
    "        positioned = self.positioning(positions)\n",
    "        embedded = embedded + positioned\n",
    "        \n",
    "        (word_in_context, _) = self.multihead_attention_layer(query=embedded, key=embedded, value=embedded, key_padding_mask=pad_mask, need_weights=False)\n",
    "        text_vecs = word_in_context[:, 0, :] # Take the vector of the first token, which should be the CLS token.\n",
    "        \n",
    "        return self.output_layer(text_vecs)\n",
    "\n",
    "model = Model(len(vocab), max_len=max_len, embedding_size=4, num_branches=2, pad_index=pad_index)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 1000+1):\n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_x_indexed)\n",
    "    train_error = torch.nn.functional.binary_cross_entropy_with_logits(logits, train_y)\n",
    "    train_errors.append(train_error.detach().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('text', 'output')\n",
    "    output = torch.sigmoid(model(train_x_indexed))[:, 0].cpu().tolist()\n",
    "    for (text, y) in zip(train_x, output):\n",
    "        print(text, y)\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modelling / text generation\n",
    "\n",
    "Transformers are also very popular for generating text.\n",
    "Just like with an RNN, you predict the next token for every prefix.\n",
    "Unlike an RNN, a transformer encodes the left and right context of each token, so it doesn't make prefix vectors.\n",
    "To make each word context vector only use its prefix as context we need to mask the tokens to the right for each token.\n",
    "\n",
    "![](decoder.png)\n",
    "\n",
    "This is done using a triangular mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones((4, 4), dtype=torch.bool, device=device)\n",
    "print(torch.triu(ones, diagonal=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A triangular mask can be used to make each query ignore the keys of the tokens to its right by making those keys have an attention of zero, thus be ignored when computing the word-in-context vector.\n",
    "\n",
    "This automatically handles pad tokens as well since any query that attends to a pad token key will also be itself a pad token and so will be masked when calculating the train error.\n",
    "Remember that both the keys and the queries are just different representations of the same tokens.\n",
    "\n",
    "Let's implement a language modelling task on the toy data set using our fully-coded transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_tokens = [\n",
    "    'I like it .'.split(' '),\n",
    "    'I hate it .'.split(' '),\n",
    "    'I don\\'t hate it .'.split(' '),\n",
    "    'I don\\'t like it .'.split(' '),\n",
    "]\n",
    "\n",
    "max_len = max(len(text) for text in train_text_tokens) + 1\n",
    "print('max_len:', max_len)\n",
    "\n",
    "vocab = ['<PAD>', '<EDGE>'] + sorted({token for text in train_text_tokens for token in text})\n",
    "token2index = {t: i for (i, t) in enumerate(vocab)}\n",
    "pad_index = token2index['<PAD>']\n",
    "edge_index = token2index['<EDGE>']\n",
    "print('vocab:', vocab)\n",
    "\n",
    "train_text_x_indexed_np = np.full((len(train_text_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_text_tokens)):\n",
    "    train_text_x_indexed_np[i, 0] = edge_index\n",
    "    for j in range(len(train_text_tokens[i])):\n",
    "        train_text_x_indexed_np[i, j + 1] = token2index[train_text_tokens[i][j]]\n",
    "train_text_x_indexed = torch.tensor(train_text_x_indexed_np, device=device)\n",
    "\n",
    "train_text_y_indexed_np = np.full((len(train_text_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_text_tokens)):\n",
    "    for j in range(len(train_text_tokens[i])):\n",
    "        train_text_y_indexed_np[i, j] = token2index[train_text_tokens[i][j]]\n",
    "    train_text_y_indexed_np[i, len(train_text_tokens[i])] = edge_index\n",
    "train_text_y_indexed = torch.tensor(train_text_y_indexed_np, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, max_len, embedding_size, num_branches):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_branches = num_branches\n",
    "        self.embedding_size = embedding_size\n",
    "        self.sqrt_dim = np.sqrt(embedding_size)\n",
    "        self.register_buffer('positions', torch.arange(max_len))\n",
    "        self.register_buffer('tri_mask', torch.triu(torch.ones((max_len, max_len), dtype=torch.bool), diagonal=1)) # Make the tri mask part of the module.\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.positioning = torch.nn.Embedding(max_len, embedding_size)\n",
    "        self.query_layer = torch.nn.Linear(embedding_size//num_branches, embedding_size)\n",
    "        self.key_layer = torch.nn.Linear(embedding_size//num_branches, embedding_size)\n",
    "        self.value_layer = torch.nn.Linear(embedding_size//num_branches, embedding_size)\n",
    "        self.word_in_context_layer = torch.nn.Linear(num_branches*embedding_size, embedding_size)\n",
    "        self.output_layer = torch.nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, x_indexed):\n",
    "        batch_size = x_indexed.shape[0]\n",
    "        time_steps = x_indexed.shape[1]\n",
    "        \n",
    "        embedded = self.embedding(x_indexed)\n",
    "        positions = self.positions[None, :time_steps].tile((batch_size, 1))\n",
    "        positioned = self.positioning(positions)\n",
    "        embedded = embedded + positioned\n",
    "        \n",
    "        branched_embedded = embedded.reshape(\n",
    "            (batch_size, time_steps, self.num_branches, self.embedding_size//self.num_branches)\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        q = self.query_layer(branched_embedded)\n",
    "        k = self.key_layer(branched_embedded)\n",
    "        v = self.value_layer(branched_embedded)\n",
    "\n",
    "        attn_logits = q@k.transpose(2, 3)\n",
    "        attn_logits = attn_logits/self.sqrt_dim\n",
    "        attn_logits = attn_logits.masked_fill(self.tri_mask[None, None, :time_steps, :time_steps], float('-inf')) # Use the tri mask instead of the pad mask.\n",
    "        attention = torch.softmax(attn_logits, dim=3)\n",
    "        branched_attended_values = attention@v\n",
    "\n",
    "        attended_values = branched_attended_values.transpose(1, 2).reshape(\n",
    "            (batch_size, time_steps, self.num_branches*self.embedding_size)\n",
    "        )\n",
    "        word_in_context = torch.nn.functional.leaky_relu(self.word_in_context_layer(attended_values))\n",
    "\n",
    "        return self.output_layer(word_in_context)\n",
    "\n",
    "model = Model(len(vocab), max_len, embedding_size=4, num_branches=2)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 1000+1):\n",
    "    batch_size = train_y_seq.shape[0]\n",
    "    time_steps = train_y_seq.shape[1]\n",
    "    pad_mask = train_x_indexed == pad_index\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_text_x_indexed)\n",
    "    train_token_errors = torch.nn.functional.cross_entropy(logits.transpose(1, 2), train_text_y_indexed, reduction='none')\n",
    "    train_token_errors = train_token_errors.masked_fill(pad_mask[:, :], 0.0)\n",
    "    train_error = train_token_errors.sum()/(~pad_mask).sum()\n",
    "    train_errors.append(train_error.detach().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = sorted({tuple(text[:i]) for text in train_text_tokens for i in range(len(text) + 1)}) # Prefix must be a tuple so we can put it in a set.\n",
    "prefix_max_len = max(len(prefix) for prefix in prefixes) + 1\n",
    "prefix_text_x_indexed_np = np.full((len(prefixes), prefix_max_len), pad_index, np.int64)\n",
    "for i in range(len(prefixes)):\n",
    "    prefix_text_x_indexed_np[i, 0] = edge_index\n",
    "    for j in range(len(prefixes[i])):\n",
    "        prefix_text_x_indexed_np[i, j+1] = token2index[prefixes[i][j]]\n",
    "prefix_text_x_indexed = torch.tensor(prefix_text_x_indexed_np, device=device)\n",
    "with torch.no_grad():\n",
    "    output = torch.softmax(model(prefix_text_x_indexed), dim=2).cpu().tolist()\n",
    "\n",
    "for (prefix, y) in zip(prefixes, output):\n",
    "    last_token_y = y[len(prefix)]\n",
    "    top_preds = sorted(zip(last_token_y, vocab), reverse=True)[:5]\n",
    "    print(['<EDGE>'] + list(prefix))\n",
    "    for (x, token) in top_preds:\n",
    "        print(f'   {token:6s}: {x:.8f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how we use Pytorch's `MultiheadAttention` to do the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, max_len, embedding_size, num_branches):\n",
    "        super().__init__()\n",
    "        self.register_buffer('positions', torch.arange(max_len))\n",
    "        self.register_buffer('tri_mask', torch.triu(torch.ones((max_len, max_len), dtype=torch.bool), diagonal=1))\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.positioning = torch.nn.Embedding(max_len, embedding_size)\n",
    "        self.multihead_attention_layer = torch.nn.MultiheadAttention(embedding_size, num_branches, batch_first=True)\n",
    "        self.output_layer = torch.nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, x_indexed):\n",
    "        batch_size = x_indexed.shape[0]\n",
    "        time_steps = x_indexed.shape[1]\n",
    "        \n",
    "        embedded = self.embedding(x_indexed)\n",
    "        positions = self.positions[None, :time_steps].tile((batch_size, 1))\n",
    "        positioned = self.positioning(positions)\n",
    "        embedded = embedded + positioned\n",
    "        \n",
    "        # We now specify the tri mask to the attn_mask parameter and set the is_causal parameter to true (causal refers to a language model that predicts the next token given the previous ones).\n",
    "        (word_in_context, _) = self.multihead_attention_layer(query=embedded, key=embedded, value=embedded, attn_mask=self.tri_mask[:time_steps, :time_steps], need_weights=False, is_causal=True)\n",
    "        \n",
    "        return self.output_layer(word_in_context)\n",
    "\n",
    "model = Model(len(vocab), max_len, embedding_size=4, num_branches=2)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 1000+1):\n",
    "    batch_size = train_y_seq.shape[0]\n",
    "    time_steps = train_y_seq.shape[1]\n",
    "    pad_mask = train_x_indexed == pad_index\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_text_x_indexed)\n",
    "    train_token_errors = torch.nn.functional.cross_entropy(logits.transpose(1, 2), train_text_y_indexed, reduction='none')\n",
    "    train_token_errors = train_token_errors.masked_fill(pad_mask[:, :], 0.0)\n",
    "    train_error = train_token_errors.sum()/(~pad_mask).sum()\n",
    "    train_errors.append(train_error.detach().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = sorted({tuple(text[:i]) for text in train_text_tokens for i in range(len(text) + 1)}) # Prefix must be a tuple so we can put it in a set.\n",
    "prefix_max_len = max(len(prefix) for prefix in prefixes) + 1\n",
    "prefix_text_x_indexed_np = np.full((len(prefixes), prefix_max_len), pad_index, np.int64)\n",
    "for i in range(len(prefixes)):\n",
    "    prefix_text_x_indexed_np[i, 0] = edge_index\n",
    "    for j in range(len(prefixes[i])):\n",
    "        prefix_text_x_indexed_np[i, j+1] = token2index[prefixes[i][j]]\n",
    "prefix_text_x_indexed = torch.tensor(prefix_text_x_indexed_np, device=device)\n",
    "with torch.no_grad():\n",
    "    output = torch.softmax(model(prefix_text_x_indexed), dim=2).cpu().tolist()\n",
    "\n",
    "for (prefix, y) in zip(prefixes, output):\n",
    "    last_token_y = y[len(prefix)]\n",
    "    top_preds = sorted(zip(last_token_y, vocab), reverse=True)[:5]\n",
    "    print(['<EDGE>'] + list(prefix))\n",
    "    for (x, token) in top_preds:\n",
    "        print(f'   {token:6s}: {x:.8f}')\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq\n",
    "\n",
    "So how do you perform seq2seq with a transformer?\n",
    "According to Vaswani's paper, you use 3 transformers.\n",
    "The first transformer acts as a language model and is used to produce prefix vectors from the target tokens.\n",
    "These prefix vectors will be used to make query vectors.\n",
    "The second transformer acts as a word-in-context model and is used to produce token vectors from the source tokens.\n",
    "These token vectors will be used to make key and value vectors.\n",
    "The query vectors are then combined with key and value vectors and passed through the third transformer.\n",
    "This aligns the target prefixes to the source tokens (in context) to produce a translation.\n",
    "\n",
    "Here is a diagram showing what happens in the third transformer (the first two are as shown in the previous diagrams):\n",
    "\n",
    "![](seq2seq.png)\n",
    "\n",
    "Note that there is no need to use a triangular mask in the third transformer because the prefix vectors are already not influenced by the target tokens that come after them.\n",
    "\n",
    "Now let's use it on the toy data set for sentiment translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src_tokens = [\n",
    "    'I like it .'.split(' '),\n",
    "    'I hate it .'.split(' '),\n",
    "    'I don\\'t hate it .'.split(' '),\n",
    "    'I don\\'t like it .'.split(' '),\n",
    "]\n",
    "\n",
    "train_trg_tokens = [\n",
    "    'I don\\'t like it .'.split(' '),\n",
    "    'I don\\'t hate it .'.split(' '),\n",
    "    'I hate it .'.split(' '),\n",
    "    'I like it .'.split(' '),\n",
    "]\n",
    "\n",
    "src_max_len = max(len(text) for text in train_src_tokens)\n",
    "print('src_max_len:', src_max_len)\n",
    "\n",
    "src_vocab = ['<PAD>'] + sorted({token for text in train_src_tokens for token in text})\n",
    "src_token2index = {t: i for (i, t) in enumerate(src_vocab)}\n",
    "src_pad_index = src_token2index['<PAD>']\n",
    "print('src_vocab:', src_vocab)\n",
    "print()\n",
    "\n",
    "train_src_indexed_np = np.full([len(train_src_tokens), src_max_len], src_pad_index, np.int64)\n",
    "for i in range(len(train_src_tokens)):\n",
    "    for j in range(len(train_src_tokens[i])):\n",
    "        train_src_indexed_np[i, j] = src_token2index[train_src_tokens[i][j]]\n",
    "train_src_indexed = torch.tensor(train_src_indexed_np, device=device)\n",
    "\n",
    "trg_max_len = max(len(text) + 1 for text in train_trg_tokens)\n",
    "print('trg_max_len:', trg_max_len)\n",
    "\n",
    "trg_vocab = ['<PAD>', '<EDGE>'] + sorted({token for text in train_trg_tokens for token in text})\n",
    "trg_token2index = {t: i for (i, t) in enumerate(trg_vocab)}\n",
    "trg_pad_index = trg_token2index['<PAD>']\n",
    "trg_edge_index = trg_token2index['<EDGE>']\n",
    "print('trg_vocab:', trg_vocab)\n",
    "print()\n",
    "\n",
    "train_trg_x_indexed_np = np.full((len(train_trg_tokens), trg_max_len), trg_pad_index, np.int64)\n",
    "for i in range(len(train_trg_tokens)):\n",
    "    train_trg_x_indexed_np[i, 0] = trg_edge_index\n",
    "    for j in range(len(train_trg_tokens[i])):\n",
    "        train_trg_x_indexed_np[i, j + 1] = trg_token2index[train_trg_tokens[i][j]]\n",
    "train_trg_x_indexed = torch.tensor(train_trg_x_indexed_np, device=device)\n",
    "\n",
    "train_trg_y_indexed_np = np.full((len(train_trg_tokens), trg_max_len), trg_pad_index, np.int64)\n",
    "for i in range(len(train_trg_tokens)):\n",
    "    for j in range(len(train_trg_tokens[i])):\n",
    "        train_trg_y_indexed_np[i, j] = trg_token2index[train_trg_tokens[i][j]]\n",
    "    train_trg_y_indexed_np[i, len(train_trg_tokens[i])] = trg_edge_index # Add the edge token at the end.\n",
    "train_trg_y_indexed = torch.tensor(train_trg_y_indexed_np, device=device)\n",
    "\n",
    "max_len = max(src_max_len, trg_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, max_len, embedding_size, num_branches, src_pad_index):\n",
    "        super().__init__()\n",
    "        self.src_pad_index = src_pad_index\n",
    "        self.register_buffer('positions', torch.arange(max_len))\n",
    "        self.register_buffer('tri_mask', torch.triu(torch.ones((max_len, max_len), dtype=torch.bool), diagonal=1))\n",
    "        self.positioning = torch.nn.Embedding(max_len, embedding_size)\n",
    "\n",
    "        # Prefixes transformer\n",
    "        self.trg_embedding = torch.nn.Embedding(trg_vocab_size, embedding_size)\n",
    "        self.trg_multihead_attention_layer = torch.nn.MultiheadAttention(embedding_size, num_branches, batch_first=True)\n",
    "\n",
    "        # Sources transformer\n",
    "        self.src_embedding = torch.nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.src_multihead_attention_layer = torch.nn.MultiheadAttention(embedding_size, num_branches, batch_first=True)\n",
    "\n",
    "        # Combining transformer\n",
    "        self.multihead_attention_layer = torch.nn.MultiheadAttention(embedding_size, num_branches, batch_first=True)\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(embedding_size, trg_vocab_size)\n",
    "\n",
    "    def forward(self, src_indexed, trg_x_indexed):\n",
    "        batch_size = src_indexed.shape[0]\n",
    "        src_time_steps = src_indexed.shape[1]\n",
    "        trg_time_steps = trg_x_indexed.shape[1]\n",
    "\n",
    "        #############\n",
    "        # Prefixes\n",
    "        #############\n",
    "        \n",
    "        embedded = self.trg_embedding(trg_x_indexed)\n",
    "        positions = self.positions[None, :trg_time_steps].tile((batch_size, 1))\n",
    "        positioned = self.positioning(positions)\n",
    "        embedded = embedded + positioned\n",
    "\n",
    "        (prefix_vecs, _) = self.trg_multihead_attention_layer(query=embedded, key=embedded, value=embedded, attn_mask=self.tri_mask[:trg_time_steps, :trg_time_steps], need_weights=False, is_causal=True)\n",
    "\n",
    "        #############\n",
    "        # Sources\n",
    "        #############\n",
    "\n",
    "        src_pad_mask = src_indexed == self.src_pad_index\n",
    "\n",
    "        embedded = self.src_embedding(src_indexed)\n",
    "        positions = self.positions[None, :src_time_steps].tile((batch_size, 1))\n",
    "        positioned = self.positioning(positions)\n",
    "        embedded = embedded + positioned\n",
    "\n",
    "        (word_in_context, _) = self.src_multihead_attention_layer(query=embedded, key=embedded, value=embedded, key_padding_mask=src_pad_mask, need_weights=False)\n",
    "\n",
    "        #############\n",
    "        # Combining\n",
    "        #############\n",
    "\n",
    "        (final_prefix_vecs, _) = self.multihead_attention_layer(query=prefix_vecs, key=word_in_context, value=word_in_context, key_padding_mask=src_pad_mask, need_weights=False)\n",
    "        \n",
    "        return self.output_layer(final_prefix_vecs)\n",
    "\n",
    "\n",
    "model = Model(len(src_vocab), len(trg_vocab), max_len, embedding_size=4, num_branches=2, src_pad_index=src_pad_index)\n",
    "model.to(device)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print('epoch', 'error')\n",
    "train_errors = []\n",
    "for epoch in range(1, 1000+1):\n",
    "    batch_size = train_trg_x_indexed.shape[0]\n",
    "    trg_time_steps = train_trg_x_indexed.shape[1]\n",
    "    trg_pad_mask = train_trg_y_indexed == trg_pad_index\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    logits = model(train_src_indexed, train_trg_x_indexed)\n",
    "    train_token_errors = torch.nn.functional.cross_entropy(logits.transpose(1, 2), train_trg_y_indexed, reduction='none')\n",
    "    train_token_errors = torch.masked_fill(train_token_errors, trg_pad_mask, 0.0)\n",
    "    train_error = train_token_errors.sum()/(~trg_pad_mask).sum()\n",
    "    train_errors.append(train_error.detach().cpu().tolist())\n",
    "    train_error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, train_errors[-1])\n",
    "print()\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('$E$')\n",
    "ax.plot(range(1, len(train_errors) + 1), train_errors, color='blue', linestyle='-', linewidth=3)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_generate(model, src_token2index, src_pad_index, trg_vocab, trg_edge_index, src_tokens, max_len, beam_size):\n",
    "    src_indexed = torch.tensor(\n",
    "        [[src_token2index[token] for token in src_tokens]],\n",
    "        dtype=torch.int64, device=device\n",
    "    ).tile((beam_size, 1))\n",
    "    beam_prefixes_indexed = torch.tensor([[trg_edge_index]], dtype=torch.int64, device=device)\n",
    "    beam_prefixes_probs = np.array([1.0], np.float32)\n",
    "    \n",
    "    best_full_prefix_indexed = None\n",
    "    best_full_prefix_prob = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            outputs = torch.softmax(model(src_indexed[:len(beam_prefixes_indexed), :], beam_prefixes_indexed), dim=2)\n",
    "            token_probs = outputs[:, -1, :]\n",
    "            new_prefixes_probs = beam_prefixes_probs[:, None]*token_probs.cpu().numpy()\n",
    "\n",
    "            new_partial_prefixes = []\n",
    "            for (prefix, probs_group) in zip(beam_prefixes_indexed.cpu().tolist(), new_prefixes_probs.tolist()):\n",
    "                for (next_token_index, prefix_prob) in enumerate(probs_group):\n",
    "                    if next_token_index == trg_edge_index:\n",
    "                        if best_full_prefix_prob is None or prefix_prob > best_full_prefix_prob:\n",
    "                            best_full_prefix_indexed = prefix + [next_token_index]\n",
    "                            best_full_prefix_prob = prefix_prob\n",
    "                    else:\n",
    "                        new_partial_prefixes.append((prefix_prob, prefix + [next_token_index]))\n",
    "            \n",
    "            new_partial_prefixes.sort(reverse=True)\n",
    "            (best_partial_prefix_prob, _) = new_partial_prefixes[0]\n",
    "            if best_full_prefix_prob > best_partial_prefix_prob:\n",
    "                text = [trg_vocab[index] for index in best_full_prefix_indexed]\n",
    "                return (text, best_full_prefix_prob)\n",
    "            \n",
    "            new_beam = new_partial_prefixes[:beam_size]\n",
    "            beam_prefixes_indexed = torch.tensor([prefix for (prob, prefix) in new_beam], dtype=torch.int64, device=device)\n",
    "            beam_prefixes_probs = np.array([prob for (prob, prefix) in new_beam], np.float32)\n",
    "\n",
    "    text = [trg_vocab[index] for index in beam_prefixes_indexed[0, :].cpu().tolist()]\n",
    "    return (text, beam_prefixes_probs[0])\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for src_text in train_src_tokens:\n",
    "        print(src_text)\n",
    "        (trg_text, prob) = beam_generate(model, src_token2index, src_pad_index, trg_vocab, trg_edge_index, src_text, max_len=10, beam_size=3)\n",
    "        print(trg_text, prob)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Autoencoding sentences again\n",
    "\n",
    "Redo the sentence autoencoder exercise from last topic but this time use transformers.\n",
    "Use a class token to represent the original sentence as a single token vector.\n",
    "Note that, given that you will have only one source token, you don't technically need to use a transformer for combining the prefixes with the source tokens, although you still can if you want.\n",
    "If you do use a transformer, make sure that you don't use a pad mask in it because there's just one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 3\n",
    "\n",
    "train_df = pd.read_csv('../data_set/sentiment/train.csv')\n",
    "test_df = pd.read_csv('../data_set/sentiment/test.csv')\n",
    "\n",
    "train_text = train_df['text']\n",
    "test_text = test_df['text'][0]\n",
    "\n",
    "nltk.download('punkt')\n",
    "train_text_tokens = [nltk.word_tokenize(text) for text in train_text]\n",
    "test_text_tokens = nltk.word_tokenize(test_text)\n",
    "max_len = max(len(text) for text in train_text_tokens) + 1 # Both src and trg have same max len because src includes the CLS token and trg includes the EDGE token.\n",
    "\n",
    "frequencies = collections.Counter(token for text in train_text_tokens for token in text)\n",
    "vocabulary = sorted(frequencies.keys(), key=frequencies.get, reverse=True)\n",
    "while frequencies[vocabulary[-1]] < min_freq:\n",
    "    vocabulary.pop()\n",
    "vocab = ['<PAD>', '<EDGE>', '<UNK>', '<CLS>'] + vocabulary\n",
    "token2index = {token: i for (i, token) in enumerate(vocab)}\n",
    "pad_index = token2index['<PAD>']\n",
    "edge_index = token2index['<EDGE>']\n",
    "unk_index = token2index['<UNK>']\n",
    "cls_index = token2index['<CLS>']\n",
    "\n",
    "train_src_indexed_np = np.full((len(train_text_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_text_tokens)):\n",
    "    train_src_indexed_np[i, 0] = cls_index\n",
    "    for j in range(len(train_text_tokens[i])):\n",
    "        train_src_indexed_np[i, j + 1] = token2index.get(train_text_tokens[i][j], unk_index)\n",
    "train_src_indexed = torch.tensor(train_src_indexed_np, device=device)\n",
    "\n",
    "train_trg_x_indexed_np = np.full((len(train_text_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_text_tokens)):\n",
    "    train_trg_x_indexed_np[i, 0] = edge_index\n",
    "    for j in range(len(train_text_tokens[i])):\n",
    "        train_trg_x_indexed_np[i, j + 1] = token2index.get(train_text_tokens[i][j], unk_index)\n",
    "train_trg_x_indexed = torch.tensor(train_trg_x_indexed_np, device=device)\n",
    "\n",
    "train_trg_y_indexed_np = np.full((len(train_text_tokens), max_len), pad_index, np.int64)\n",
    "for i in range(len(train_text_tokens)):\n",
    "    for j in range(len(train_text_tokens[i])):\n",
    "        train_trg_y_indexed_np[i, j] = token2index.get(train_text_tokens[i][j], unk_index)\n",
    "    train_trg_y_indexed_np[i, len(train_text_tokens[i])] = edge_index\n",
    "train_trg_y_indexed = torch.tensor(train_trg_y_indexed_np, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_generate(model, token2index, pad_index, edge_index, unk_index, cls_index, vocab, src_tokens, max_len, beam_size):\n",
    "    src_indexed = torch.tensor(\n",
    "        [[cls_index] + [token2index.get(token, unk_index) for token in src_tokens]],\n",
    "        dtype=torch.int64, device=device\n",
    "    ).tile((beam_size, 1))\n",
    "    beam_prefixes_indexed = torch.tensor([[edge_index]], dtype=torch.int64, device=device)\n",
    "    beam_prefixes_probs = np.array([1.0], np.float32)\n",
    "    \n",
    "    best_full_prefix_indexed = None\n",
    "    best_full_prefix_prob = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            outputs = torch.softmax(model(src_indexed[:len(beam_prefixes_indexed), :], beam_prefixes_indexed), dim=2)\n",
    "            token_probs = outputs[:, -1, :]\n",
    "            new_prefixes_probs = beam_prefixes_probs[:, None]*token_probs.cpu().numpy()\n",
    "\n",
    "            new_partial_prefixes = []\n",
    "            for (prefix, probs_group) in zip(beam_prefixes_indexed.cpu().tolist(), new_prefixes_probs.tolist()):\n",
    "                for (next_token_index, prefix_prob) in enumerate(probs_group):\n",
    "                    if next_token_index == edge_index:\n",
    "                        if best_full_prefix_prob is None or prefix_prob > best_full_prefix_prob:\n",
    "                            best_full_prefix_indexed = prefix + [next_token_index]\n",
    "                            best_full_prefix_prob = prefix_prob\n",
    "                    else:\n",
    "                        new_partial_prefixes.append((prefix_prob, prefix + [next_token_index]))\n",
    "            \n",
    "            new_partial_prefixes.sort(reverse=True)\n",
    "            (best_partial_prefix_prob, _) = new_partial_prefixes[0]\n",
    "            if best_full_prefix_prob > best_partial_prefix_prob:\n",
    "                text = [vocab[index] for index in best_full_prefix_indexed]\n",
    "                return (text, best_full_prefix_prob)\n",
    "            \n",
    "            new_beam = new_partial_prefixes[:beam_size]\n",
    "            beam_prefixes_indexed = torch.tensor([prefix for (prob, prefix) in new_beam], dtype=torch.int64, device=device)\n",
    "            beam_prefixes_probs = np.array([prob for (prob, prefix) in new_beam], np.float32)\n",
    "\n",
    "    text = [vocab[index] for index in beam_prefixes_indexed[0, :].cpu().tolist()]\n",
    "    return (text, beam_prefixes_probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75ee2b71ad44bf9ef4e9bee896f68ffbc764a6a2c6d1f57c86c48f99ffc25ca8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
